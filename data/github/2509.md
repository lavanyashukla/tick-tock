TITLE:
[Q] Sweep three errors (forward () missing, _global_run_stack, shutdown)

LABEL:
stale

STATE:
closed

BODY:
[System]
OS: Window
IDE: Visual Studio Code
DL Framework: Pytorch

When I run the below code, I have some error messages.

```python
import os
from albumentations.pytorch import transforms
import numpy as np

import torch
from torch._C import dtype
import torch.nn as nn
from torch.utils.data import DataLoader

from model import UNet
from alb_data_load import Dataset

import time

#from torchvision import transforms
import albumentations as A


import copy
from torchvision.utils import save_image

import wandb


sweep_config = {
                'method': 'random',
                'metric': {'goal': 'minimize', 'name': 'loss'},
                'parameters': {
                    'learning_rate': {
                        'distribution': 'uniform',
                        'max': 0.1,
                        'min': 0},
                    'optimizer': {
                        'values': ['adam', 'sgd']}
                }
 }

sweep_id = wandb.sweep(sweep_config, project="test")

config_defaults = {
        'learning_rate': 1e-3,
        'optimizer': 'adam'
    }
    


data_dir = 'data'
batch_size= 2

transform_train = A.Compose([
    A.HorizontalFlip(),
    A.VerticalFlip(), 
    A.Normalize(mean=0.5, std=0.5),
    transforms.ToTensorV2(transpose_mask=True)
    ])


transform_val = A.Compose([
    A.HorizontalFlip(), 
    A.Normalize(mean=0.5, std=0.5),
    transforms.ToTensorV2(transpose_mask=True)
    ])


dataset_train = Dataset(data_dir=os.path.join(data_dir, 'train'), transform=transform_train)
loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=False, num_workers=0)

dataset_val = Dataset(data_dir=os.path.join(data_dir, 'val'), transform=transform_val)
loader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=False, num_workers=0)


# 그밖에 부수적인 variables 설정하기
num_data_train = len(dataset_train)
num_data_val = len(dataset_val)

num_batch_train = np.ceil(num_data_train / batch_size)
num_batch_val = np.ceil(num_data_val / batch_size)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

wandb.init(project='test', entity='douner89', config=config_defaults)
config = wandb.config

## 네트워크 생성하기
net = UNet().to(device)

## 손실함수 정의하기
fn_loss = nn.BCEWithLogitsLoss().to(device)

## Optimizer 설정하기
optim = torch.optim.Adam(net.parameters(), lr=1e-3)

if config.optimizer=='sgd':
    optim = torch.optim.SGD(net.parameters(),lr=config.learning_rate, momentum=0.9)
elif config.optimizer=='adam':
    optim = torch.optim.Adam(net.parameters(),lr=config.learning_rate)

## 네트워크 학습시키기
st_epoch = 0
num_epoch = 10

wandb.watch(net, fn_loss, log="all", log_freq=10) 

# TRAIN MODE
def train_model(net, fn_loss, optim, num_epoch):
    since = time.time()

    best_model_wts = copy.deepcopy(net.state_dict())
    best_loss = 100


    for epoch in range(st_epoch + 1, num_epoch + 1):
        net.train()
        loss_arr = []
        batch_order=0

        for batch, data in enumerate(loader_train, 1):
            batch_order=batch_order+1
            data['label'] = data['label']/255.0

            input = data['input']
            label = data['label']

            # forward pass
            label = data['label'].to(device)
            input = data['input'].to(device)

            output = net(input)

            # backward pass
            optim.zero_grad()

            loss = fn_loss(output, label)
            loss.backward()

            optim.step()

            # 손실함수 계산
            loss_arr += [loss.item()]

            print("TRAIN: EPOCH %04d / %04d | BATCH %04d / %04d | Batch LOSS %.4f" %
                (epoch, num_epoch, batch, num_batch_train, np.mean(loss_arr)))
        
        print("#############################################################")
        print("TRAIN: EPOCH %04d | Epoch LOSS %.4f" %
                (epoch, np.mean(loss_arr)))
        print("#############################################################")
        wandb.log({'Epoch': epoch, 'loss': np.mean(loss_arr)})



        with torch.no_grad():
            net.eval()
            loss_arr = []

            for batch, data in enumerate(loader_val, 1):
                data['label'] = data['label']/255.0

                # forward pass
                label = data['label'].to(device, dtype=torch.float32)
                input = data['input'].to(device, dtype=torch.float32)

                output = net(input)

                # 손실함수 계산하기
                loss = fn_loss(output, label)

                loss_arr += [loss.item()]

                print("VALID: EPOCH %04d / %04d | BATCH %04d / %04d | LOSS %.4f" %
                        (epoch, num_epoch, batch, num_batch_val, np.mean(loss_arr))) 

            epoch_loss = np.mean(loss_arr)

            # deep copy the model
            if epoch_loss < best_loss:
                best_loss = epoch_loss
                best_model_wts = copy.deepcopy(net.state_dict())

        print()
    
    time_elapsed = time.time() - since
    print('Training complete in {:.0f}m {:.0f}s'.format(
        time_elapsed // 60, time_elapsed % 60))
    print('Best val loss: {:4f}'.format(best_loss))

    # load best model weights
    net.load_state_dict(best_model_wts)
    return net


wandb.agent(sweep_id, function=train_model(net, fn_loss, optim, num_epoch), count=5)
model_ft = train_model(net, fn_loss, optim, num_epoch)
torch.save(model_ft.state_dict(), './model_log/model_weights.pth')
```


[Error message1]
Traceback (most recent call last):
  File "C:\Users\MI2RL\anaconda3\lib\site-packages\wandb\agents\pyagent.py", line 303, in _run_job
    self._function()
  File "C:\Users\MI2RL\anaconda3\lib\site-packages\torch\nn\modules\module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
TypeError: forward() missing 1 required positional argument: 'x'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\MI2RL\anaconda3\lib\threading.py", line 932, in _bootstrap_inner
    self.run()
  File "C:\Users\MI2RL\anaconda3\lib\threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "C:\Users\MI2RL\anaconda3\lib\site-packages\wandb\agents\pyagent.py", line 308, in _run_job
    wandb.finish(exit_code=1)
  File "C:\Users\MI2RL\anaconda3\lib\site-packages\wandb\sdk\wandb_run.py", line 2508, in finish
    wandb.run.finish(exit_code=exit_code)
  File "C:\Users\MI2RL\anaconda3\lib\site-packages\wandb\sdk\wandb_run.py", line 1300, in finish
    if self._wl and len(self._wl._global_run_stack) > 0:
  File "C:\Users\MI2RL\anaconda3\lib\site-packages\wandb\sdk\wandb_setup.py", line 234, in __getattr__
    return getattr(self._instance, name)
AttributeError: 'NoneType' object has no attribute '_global_run_stack'


[Error message2]
Exception has occurred: Exception
The wandb backend process has shutdown
  File "C:\Users\MI2RL\Desktop\UNet_segmentation\alb_train2.py", line 148, in train_model
    wandb.log({'Epoch': epoch, 'loss': np.mean(loss_arr)})
  File "C:\Users\MI2RL\Desktop\UNet_segmentation\alb_train2.py", line 193, in <module>
    model_ft = train_model(net, fn_loss, optim, num_epoch)


What is wrong??


