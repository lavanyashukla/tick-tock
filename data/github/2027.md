TITLE:
[CLI] Using a non-full backward hook leads to user warning.

LABEL:
cli,stale

STATE:
closed

BODY:
**Description**
Using wandb with the `nn.Sequential` module raises a user warning.

**Wandb features**
`wandb.watch` and `wandb.init`.

**How to reproduce**
Run this:
```
import torch
import torch.nn as nn
import wandb

with wandb.init(project="Project"):
    model = nn.Sequential(nn.Linear(100, 50),
                          nn.ReLU(),
                          nn.Linear(50, 5))
    wandb.watch(model)
    model.train()
    X = torch.randn(5, 100) # input
    criterion = nn.CrossEntropyLoss(reduction="mean")
    labels = torch.Tensor([0,1,2,3,4]).long()
    output = model(X)
    loss = criterion(output, labels)
    loss.backward()
```
Warning:
```
C:\Users\Me\anaconda3\envs\Project\lib\site-packages\torch\nn\modules\module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
```

**Environment**
- OS: Windows 10
- Python Version: 3.8.8
- PyTorch Version: 1.8.0


