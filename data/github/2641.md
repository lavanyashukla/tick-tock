TITLE:
[CLI] wandb: WARNING Config item 'hyperparam_name' was locked by 'sweep' (ignored update)

LABEL:
cli

STATE:
closed

BODY:
**Description**
When running basic hyper parameter search (following this [tutorial](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb#scrollTo=rHpan13W6UZD) very closely) I get:
`wandb: WARNING Config item 'hyperparam_name' was locked by 'sweep' (ignored update).`
for many, but not all, of my hyper parameters


**Wandb features**
Using WandB with Pytorch Lightning to perform simple hyper parameter search. WandB logger callback for pytorch lightning used to log relevant metrics.

######### Pseudo Code ######### 
```
import wandb
from pytorch_lightning.loggers import WandbLogger
import pytorch_lightning as pl


sweep_config = {
    "method" : "random",
    "metric" : {"name": "val/error_epoch", "goal": "minimize", "target": 0.05}, #error of 5%
    "early_terminate" : {"type": "hyperband", "min_iter": 20, "eta": 3},
    "parameters" :
        {
        "share_parameters": {"value": share_parameters},
        # architecture hps
        "iterations": {"values": [2, 5, 10]},
         .... more params ...
        # trainer hps
        "epochs": {"value": 500},
        "batch_size": {"values": [16, 32, 64, 128, 256]},
        }
}


def train(config=None):
    with wandb.init(config=config):
        # Config is a variable that holds and saves hyperparameters and inputs
        config = wandb.config

       # construct trainer, model, & data module  by setting relevant arguments with 'config.param_name'
       dm = my_dm(batch_size = config.batch_size, ....)
       trainer = pl.Trainer(max_epochs = config.epochs, WandbLogger(project=my_project_name), ....) 
       model = my_model(iterations=config.iterations, ...)

       trainer.fit(model, train_dataloader=dm.train_dataloader(), val_dataloaders=dm.val_dataloader())

       #if on colab
       wandb.finish()

wandb.agent(sweep_id, train)
```



**Environment**
Occurs both on my local machine (2015 MacBook Air, macOS Big Sure 11.2.3) using Python 3.7.9 and Google Colab with a GPU using Python 3.7.11. Using most up to date Pytorch Lightning and WandB.


It seems [someone else has had this issue](https://github.com/wandb/client/issues/1698) as well but it has not been resolved. Interestingly I am also getting a CUDA out of memory error; someone else in the aforementioned unresolved issue also had these two exact issues. Not sure if these issues are connected, but I will post a new issue for that soon.


Thanks,
Max


