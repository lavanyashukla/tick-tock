TITLE:
[CLI]: `KeyError(0)` when using sweep and agent

LABEL:
cli

STATE:
closed

BODY:
### Describe the bug

Hello wandb team,

I'd value help debugging a simple hyperparameter search for an Albert model. My code should be following examples from [link1](https://github.com/wandb/examples/blob/master/colabs/pytorch/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb
), [link2](https://wandb.ai/matt24/vit-snacks-sweeps/reports/Hyperparameter-Search-with-W-B-Sweeps-for-Hugging-Face-Transformer-Models--VmlldzoyMTUxNTg0
), [link3](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch-lightning/Fine_tuning_a_Transformer_with_Pytorch_Lightning.ipynb#scrollTo=R7Dm52nfoZWf
) but please point out if I've made an error.


Code excerpt includes
<!--- A minimal code snippet between the quotes below  -->
``` 
# parse command line args for the script
args, huggingface_args = parse_command_line_arguments()

# Step 1: define the sweep
sweep_config = {
    'method': 'grid',
    'metric': {
        'goal': 'minimize',
        'name': 'val_loss',
    },
}

# hyperparameters
parameters_dict = {
    'num_train_epochs': {
        'value': 10,
        },
    'per_device_train_batch_size': {
        'values': [16, 32],
        },
    'learning_rate': {
        'values': [1e-4, 2e-4, 4e-4], 
    },
}

sweep_config['parameters'] = parameters_dict


# Step 2: initialize the sweep
sweep_id = wandb.sweep(sweep_config, project=args.wandb_project)


# Step 3: run the sweep agent
training_args_dict = huggingface_args['TrainingArgs']
config_default = {"learning_rate": 1e-4, "per_device_train_batch_size": 16, "num_train_epochs": 10}
def train(config=config_default,
          training_args_dict=training_args_dict,
          args=args,
          ):
  with wandb.init(config=config): 
    # set sweep configuration
    config = wandb.config

    for key, value in config:
        training_args_dict[key] = value

    print('training_args_dict:')
    pprint(training_args_dict)
    # set training arguments
    training_args = TrainingArguments(**training_args_dict)

    # define training loop
    trainer = CustomTrainer(scaler=scaler,
                            targets=args.targets,
                            model=model,
                            args=training_args,
                            train_dataset=train_dataset,
                            eval_dataset=val_dataset,
                            tokenizer=smi_tokenizer,
                            )
    # start training loop
    trainer.train()


wandb.agent(sweep_id, function=train)

```



For now, I am just using 1 GPU. The error is a `KeyError(0)` and the training seems to never be performed. Traceback is below

<!--- A full traceback of the exception in the quotes below -->
```
wandb: Agent Starting Run: i6sptxrv with config:
wandb: 	learning_rate: 0.0002
wandb: 	num_train_epochs: 10
wandb: 	per_device_train_batch_size: 16
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in /home/kspieker/code/albert/12/checkpoint-3172165_sweep_mithrim/0/wandb/run-20220723_205914-i6sptxrv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silvery-sweep-3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/kspieker/fine_tune_albert_mithrim_sweep_test
wandb: üßπ View sweep at https://wandb.ai/kspieker/fine_tune_albert_mithrim_sweep_test/sweeps/zdxod744
wandb: üöÄ View run at https://wandb.ai/kspieker/fine_tune_albert_mithrim_sweep_test/runs/i6sptxrv
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.000 MB of 0.003 MB uploaded (0.000 MB deduped)
wandb: \ 0.003 MB of 0.003 MB uploaded (0.000 MB deduped)
wandb: / 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)
wandb:                                                                                
wandb: Synced silvery-sweep-3: https://wandb.ai/kspieker/fine_tune_albert_mithrim_sweep_test/runs/i6sptxrv
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220723_205914-i6sptxrv/logs
Run i6sptxrv errored: KeyError(0)
wandb: ERROR Run i6sptxrv errored: KeyError(0)
Detected 3 failed runs in the first 60 seconds, killing sweep.
wandb: ERROR Detected 3 failed runs in the first 60 seconds, killing sweep.
wandb: To disable this check set WANDB_AGENT_DISABLE_FLAPPING=true

```
My scripts run fine and can sync to wandb successfully when I run individual jobs i.e. not using sweep and agent. However, I would like to get sweep working so the hyperparameter search is automated. Can you please help me identify any errors in my approach? Thanks in advance.


### Additional Files

[debug.log](https://github.com/wandb/wandb/files/9174953/debug.log)
[debug-internal.log](https://github.com/wandb/wandb/files/9174954/debug-internal.log)



### Environment

Running on Ubuntu 18.04.5 LTS and my environment is using 
```
python                    3.9.12               h12debd9_1  
cudatoolkit               10.2.89              hfd86e86_1  
pytorch                   1.12.0          py3.9_cuda10.2_cudnn7.6.5_0    pytorch
transformers              4.20.1                   pypi_0    pypi
wandb                     0.12.21                  pypi_0    pypi
```

### Additional Context

_No response_

