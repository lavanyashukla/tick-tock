TITLE:
[CLI] Wandb Taking significantly long time to execute code

LABEL:
cli

STATE:
closed

BODY:
# Description
I have been performing multiple experiments by running my code with and without wandb integration and I see a significant  difference in code execution time when I integrate wandb. I am working on a Multilingual Text Classification Problem using mBERT with Mixed Precision and Multi-GPU training using Huggingface Accelerator.

**Here are my experiments:**
```
EPOCHS  | No. of GPUs | MIXED PRECISION | WANDB INTEGRATION | WALL TIME (Mins)
   2    |      2      |       True      |       False       |     49
   2    |      2      |       True      |       True        |    243
   2    |      2      |       False     |       True        |    262
   2    |      2      |       False     |       False       |     75
```

# Program Engine
```

# Weights and Biases
import wandb
wandb.login()

%%time

def run():

    with wandb.init(project = "jigsaw-multilingual-bert"):    
       
        def loss_fn(outputs, targets):
            
            """
            Function to set BCEWithLogitsLoss as the Loss Function
            
            parameters: outputs - The outputs produced by the model
                        targets - The targets as defined in the data
                        
            returns: BCEWithLogitsLoss for the input parameters
            
            """
            
            return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))

        def train_fn(data_loader, model, optimizer, scheduler):
        
            
            """
            Training Function for the Model
            
            parameters: data_loader - PyTorch DataLoader
                        model - The Model to be used for training
                        optimizer - The Optimizer to be used for training
                        scheduler - The Learning Rate Scheduler 
                        
            returns: None
            """
            accelerator = Accelerator()
            device = accelerator.device
            from accelerate import DistributedDataParallelKwargs

            ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)
            accelerator = Accelerator(kwargs_handlers=[ddp_kwargs])

            model, optimizer, data_loader = accelerator.prepare(model, optimizer, data_loader)
            model.train()

            wandb.watch(model, log="all", log_freq=10)

            for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):
                ids = d["ids"]
                token_type_ids = d["token_type_ids"]
                mask = d["mask"]
                targets = d["targets"]

                ids = ids.to(device, dtype=torch.long)
                token_type_ids = token_type_ids.to(device, dtype=torch.long)
                mask = mask.to(device, dtype=torch.long)
                targets = targets.to(device, dtype=torch.float)
                outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)

                loss = loss_fn(outputs, targets)
                
                if bi % 1000 == 0:
                    print(f"bi={bi}, loss={loss}")

                accelerator.backward(loss)
                optimizer.step()
                scheduler.step()

                optimizer.zero_grad()
                wandb.log({"epoch": epoch, "loss": loss})

        def eval_fn(data_loader, model):
            
            """
            Evaluation Function for the Model
            
            parameters: data_loader - PyTorch Data Loader
                        model - The model to be used for Evaluation
                                                
            returns: fin_outputs - Final Outputs of the Model
                     fin_targets - Final Targets from the data
            """
            
            accelerator = Accelerator()
            device = accelerator.device
            
            model.eval()
            fin_targets = []
            fin_outputs = []

            with torch.no_grad():
                for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):
                    ids = d["ids"]
                    token_type_ids = d["token_type_ids"]
                    mask = d["mask"]
                    targets = d["targets"]

                    ids = ids.to(device, dtype=torch.long)
                    token_type_ids = token_type_ids.to(device, dtype=torch.long)
                    mask = mask.to(device, dtype=torch.long)
                    targets = targets.to(device, dtype=torch.float)

                    outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)
                    fin_targets.extend(targets.cpu().detach().numpy().tolist())
                    fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())

            return fin_outputs, fin_targets
    
        # Load Training Datasets
        df1 = pd.read_csv(
            "/workspace/data/jigsaw-multilingual/input/jigsaw-data/jigsaw-toxic-comment-train.csv", 
            usecols = ["comment_text", "toxic"]    
        )
        
        df1 = df1.head(100000)

        df2 = pd.read_csv(
            "/workspace/data/jigsaw-multilingual/input/jigsaw-data/jigsaw-unintended-bias-train.csv",
            usecols = ["comment_text", "toxic"]
        )
        
        df2 = df2.head(100000)
        
        # Combine Training Datasets
        df_train = pd.concat([df1, df2], axis = 0).reset_index(drop = True)
        df_train["comment_text"] = df_train["comment_text"].apply(clean_text)
        
        # Load Validation Datasets

        df_valid = pd.read_csv("/workspace/data/jigsaw-multilingual/input/jigsaw-data/Translated Datasets/jigsaw_miltilingual_valid_translated.csv")
        df_valid["comment_text"] = df_valid["translated"]
        df_valid.drop("translated", axis = 1, inplace = True)
        df_valid["comment_text"] = df_valid["comment_text"].apply(clean_text)
  
        nlp_transform = NLPTransform()

        df_train['lang'] = 'en'
        non_toxic_sentences = set()
        for comment_text in tqdm(df_train['comment_text'], total=df.shape[0]):
            non_toxic_sentences.update(nlp_transform.get_sentences(comment_text), 'en')

        transform = AddNonToxicSentencesTransform(non_toxic_sentences=list(non_toxic_sentences), p=1.0, sentence_range=(1,2))
        
        # Define Train Dataset and Data Loader
               
        train_dataset = JigsawDataset(
           df =  df_train,
           train_transforms = get_train_transforms()
        )

        train_data_loader = torch.utils.data.DataLoader(
            train_dataset, 
            batch_size=config.TRAIN_BATCH_SIZE, 
            num_workers=4
       )
        
        # Define Validation Dataset and Validation Data Loader

        valid_dataset = JigsawDataset(
            df = df_valid,
        )

        valid_data_loader = torch.utils.data.DataLoader(
            valid_dataset, 
            batch_size=config.VALID_BATCH_SIZE, 
            num_workers=1
        )

        # Select the device and initialize model
        model = BERTModel()

        # Set Optimizer Parameters
        param_optimizer = list(model.named_parameters())
        no_decay = ["bias", "LayerNorm.bias", "LayerNorm.weight"]
        optimizer_parameters = [
            {
                "params": [
                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)
                ],
                "weight_decay": 0.001,
            },
            {
                "params": [
                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)
                ],
                "weight_decay": 0.0,
            },
        ]
        
        num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)
        
        # Define Optimizer and Scheduler
        optimizer = AdamW(optimizer_parameters, lr=config.LEARNING_RATE)
        scheduler = get_linear_schedule_with_warmup(
            optimizer, 
            num_warmup_steps=0, 
            num_training_steps=num_train_steps)
        
        # Run the model for provided number of Epochs
        best_accuracy = 0
        for epoch in range(config.EPOCHS):
            
            print(f"----------EPOCH: {epoch}----------")
            train_fn(train_data_loader, model, optimizer, scheduler)
            outputs, targets = eval_fn(valid_data_loader, model)
            targets = np.array(targets) >= 0.5
            accuracy = metrics.roc_auc_score(targets, outputs)
            print(f"----------ROC AUC Score = {accuracy}----------")
            print()
            if accuracy > best_accuracy:
                torch.save(model.state_dict(), config.MODEL_PATH)
                best_accuracy = accuracy

```

# Environment
- Jupyter Lab
- Python 3.6.10 :: Anaconda, Inc.
- GPU:  Quadro RTX 6000
- wandb, version 0.10.31

