TITLE:
[CLI]: Key Error in wandb.config when using wandb.sweep and wandb.agent for parameters tuning in pytorch

LABEL:
cli

STATE:
closed

BODY:
### Describe the bug

<!--- Description of the issue below  -->
wandb.config sometimes can not access the parameters when using wandb.sweep and wandb.agent, I followed this [instruction](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb#scrollTo=ZTRavPmr6cwU) as well as the [document](https://docs.wandb.ai/guides/sweeps/python-api) but still encountered the error
<!--- A minimal code snippet between the quotes below  -->
```
sweep_config = {
  "name" : "xxxxxx",
  "method" : "grid",
  "parameters" : {
    "nhead" : {
      "values" : [2, 3]
    },
    "d_model": {
        "value": 6
    },
     ......
  }
}

sweep_id = wandb.sweep(sweep_config, project='xxxxx')

def run(train_loader, val_loader, config=None):
    with wandb.init(config=sweep_config):
        config = wandb.config
        params = {'d_model': config.d_model, 'nhead': config.nhead, 'd_hid':  config.d_hid,
           'nlayers':  config.nlayers, 'n_class': 18, 
           'seq_size': 300, 'hid_size': config.hid_size, 'dropout': config.dropout}
        print(params)
        model = Transformer(**params).to(device)
......

wandb.agent(sweep_id, function=run(train_dataloader, val_dataloader), count=10)

```

<!--- A full traceback of the exception in the quotes below -->
```
Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to [the W&B docs](https://wandb.me/wandb-init).
Tracking run with wandb version 0.12.17
Run data is saved locally in D:\Project\xxxxxx\wandb\run-20220527_064552-173j5uls
Syncing run [eager-snowball-22](https://wandb.ai/xxxxx/runs/xxx) to [Weights & Biases](https://wandb.ai/xxxxx) ([docs](https://wandb.me/run))
Waiting for W&B process to finish... (failed 1). Press Ctrl-C to abort syncing.
Synced eager-snowball-22: https://wandb.ai/xxxxxxxxr-xxx/runs/xxxx
Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
Find logs at: .\wandb\run-20220527_064552-173j5uls\logs
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
~\AppData\Local\Temp\ipykernel_18320\3958647640.py in <cell line: 4>()
      2                       step_size=25, patience=3, params=None, sweep_config=sweep_config, tune=True)
      3 
----> 4 wandb.agent(sweep_id, function=run(kuhar_train_dataloader, kuhar_val_dataloader), count=4)

~\AppData\Local\Temp\ipykernel_18320\2749057259.py in run(train_loader, val_loader, config)
     42     with wandb.init(config=sweep_config):
     43         config = wandb.config
---> 44         params = {'d_model': config.d_model, 'nhead': config.nhead, 'd_hid':  config.d_hid,
     45            'nlayers':  config.nlayers, 'n_class': 18,
     46            'seq_size': 300, 'hid_size': config.hid_size, 'dropout': config.dropout}

D:\Softwares\Anaconda3\lib\site-packages\wandb\sdk\wandb_config.py in __getattr__(self, key)
    158 
    159     def __getattr__(self, key):
--> 160         return self.__getitem__(key)
    161 
    162     def __contains__(self, key):

D:\Softwares\Anaconda3\lib\site-packages\wandb\sdk\wandb_config.py in __getitem__(self, key)
    126 
    127     def __getitem__(self, key):
--> 128         return self._items[key]
    129 
    130     def _check_locked(self, key, ignore_locked=False) -> bool:

KeyError: 'd_model'

```
I also print ```wandb.config``` before ```params = {'d_model': config.d ......```, it shows:
```
{'method': 'random', 'metric': {'name': 'val_loss', 'goal': 'minimize'}, 'parameters': {'nhead': {'values': [2, 3]}, 'd_hid': {'values': [2, 3, 4]}, 'nlayers': {'values': [4, 6, 8]}, 'hid_size': {'values': [128, 256, 512]}, 'dropout': {'values': [0.1, 0.5, 0.8]}, 'n_class': {'value': 18}, 'seq_size': {'value': 300}, 'd_model': {'value': 6}}}
```
So to debug it, I run:
```
def test_():
    with wandb.init(config=config):
        print(wandb.config.dropout)
        print(wandb.config.nhead)
        print(wandb.config.d_hid)

wandb.agent(sweep_id, function=test_, count=1)
```
it works:
```
wandb: Agent Starting Run: tqywqzvm with config:
wandb: 	d_hid: 3
wandb: 	d_model: 6
wandb: 	dropout: 0.5
wandb: 	hid_size: 128
wandb: 	n_class: 18
wandb: 	nhead: 2
wandb: 	nlayers: 6
wandb: 	seq_size: 300
Tracking run with wandb version 0.12.17
Run data is saved locally in D:\Project\xxxxx\wandb\run-20220527_065403-tqywqzvm
Syncing run [zesty-sweep-1](https://wandb.ai/xxxx) to [Weights & Biases](https://wandb.ai/xxxx) ([docs](https://wandb.me/run))
Sweep page: https://wandb.ai/xxxxx
0.5
2
3
Waiting for W&B process to finish... (success).
Synced zesty-sweep-1: https://wandb.ai/xxxxx/runs/xxxx
Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
```
It seems work, so I run my code again and this time it also works, so weird:
```
Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to [the W&B docs](https://wandb.me/wandb-init).
Tracking run with wandb version 0.12.17
Run data is saved locally in D:\Project\xxxxxx\wandb\run-20220526_203808-5f51ptmu
Syncing run [smooth-sweep-5](https://wandb.ai/xxxxx/xxxx) to [Weights & Biases](https://wandb.ai/xxxx) ([docs](https://wandb.me/run))
Sweep page: https://wandb.ai/xxxxxx/xxxx

{'d_model': 6, 'nhead': 3, 'd_hid': 2, 'nlayers': 6, 'n_class': 18, 'seq_size': 300, 'hid_size': 128, 'dropout': 0.1}

206it [00:38,  5.31it/s]
[Epoch1] | train_loss:2.2786 | val_loss:1.7227 | train_acc:0.1833 | val_acc:0.2871 | val_f1:0.1980 | lr:1.000000e-01
206it [00:38,  5.34it/s]
[Epoch2] | train_loss:1.6901 | val_loss:1.6323 | train_acc:0.3201 | val_acc:0.3180 | val_f1:0.2638 | lr:1.000000e-01
......
```

But after the first round of ```count```, a new bug is encountered:
![image](https://user-images.githubusercontent.com/24769943/170593741-cc983266-7ff9-41e6-a9f3-6f9b1fdfdd75.png)

To conclude it, each time I want to tune my parameters, I have to run ```wandb.agent``` on the ```test_()``` function firstly, other wise I will encounter the first bug. However, after the first ```count`` finish, another bug is encountered.

### Additional Files

_No response_

### Environment

WandB version:  0.12.17

OS:  Windows-10-10.0.19044-SP0

Python version: 3.8.12

Versions of relevant libraries: 
* pytorch version 1.9.0


### Additional Context

_No response_

