TITLE:
[CLI] Report to wandb in Hugginface trainer triggers CUDA out-of-memory

LABEL:
cli

STATE:
closed

BODY:
**Description**
Reporting to wandb in Huggingface `trainer` with `unilm/layoutxlm` [model](https://github.com/microsoft/unilm/tree/master/layoutlmft) get CUDA out-of-memory. 
If I uninstall wandb and set `report_to = None` everything works fine.
The error is triggered in the middle of the second epoch with `batch_size = 1`, so the problem cannot be in the data used.
```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
/tmp/ipykernel_356/4032920361.py in <module>
----> 1 trainer.train()

/opt/conda/lib/python3.8/site-packages/transformers/trainer.py in train(self, resume_from_checkpoint, trial, **kwargs)
   1267                         tr_loss += self.training_step(model, inputs)
   1268                 else:
-> 1269                     tr_loss += self.training_step(model, inputs)
   1270                 self.current_flos += float(self.floating_point_ops(inputs))
   1271 

/opt/conda/lib/python3.8/site-packages/transformers/trainer.py in training_step(self, model, inputs)
   1770             loss = self.deepspeed.backward(loss)
   1771         else:
-> 1772             loss.backward()
   1773 
   1774         return loss.detach()

/opt/conda/lib/python3.8/site-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph)
    219                 retain_graph=retain_graph,
    220                 create_graph=create_graph)
--> 221         torch.autograd.backward(self, gradient, retain_graph, create_graph)
    222 
    223     def register_hook(self, hook):

/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)
    128         retain_graph = create_graph
    129 
--> 130     Variable._execution_engine.run_backward(
    131         tensors, grad_tensors_, retain_graph, create_graph,
    132         allow_unreachable=True)  # allow_unreachable flag

/opt/conda/lib/python3.8/site-packages/wandb/wandb_torch.py in <lambda>(grad)
    283             self.log_tensor_stats(grad.data, name)
    284 
--> 285         handle = var.register_hook(lambda grad: _callback(grad, log_track))
    286         self._hook_handles[name] = handle
    287         return handle

/opt/conda/lib/python3.8/site-packages/wandb/wandb_torch.py in _callback(grad, log_track)
    281             if not log_track_update(log_track):
    282                 return
--> 283             self.log_tensor_stats(grad.data, name)
    284 
    285         handle = var.register_hook(lambda grad: _callback(grad, log_track))

/opt/conda/lib/python3.8/site-packages/wandb/wandb_torch.py in log_tensor_stats(self, tensor, name)
    218 
    219         # Remove nans from tensor. There's no good way to represent that in histograms.
--> 220         flat = flat[~torch.isnan(flat)]
    221         flat = flat[~torch.isinf(flat)]
    222         if flat.shape == torch.Size([0]):

RuntimeError: CUDA out of memory. Tried to allocate 1.43 GiB (GPU 0; 7.79 GiB total capacity; 6.40 GiB already allocated; 365.50 MiB free; 6.47 GiB reserved in total by PyTorch)
```

**Wandb features**
Huggingface integration

**Environment**
- OS: Docker container starting `FROM pytorch/pytorch:1.7.1-cuda11.0-cudnn8-devel`
- Environment: Jupyter lab
- Python Version: 3.8.5
- wandb version: 0.11.0

