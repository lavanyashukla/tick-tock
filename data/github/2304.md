TITLE:
[CLI] Repetitive print "W&B offline, running your script from this directory will only write metadata locally."

LABEL:
cli

STATE:
closed

BODY:
**Description**
**When using the wandb for the pytorch ddp, it continues print out the same sentence over and over again.**
Here is the output log:

W&B offline, running your script from this directory will only write metadata locally.
W&B offline, running your script from this directory will only write metadata locally.
W&B offline, running your script from this directory will only write metadata locally.
Loading pretrained model:Model/Backbones/pretrained/3x3resnet50-imagenet.pth
Loading pretrained model:Model/Backbones/pretrained/3x3resnet50-imagenet.pth
[wandb][INFO] setting env: {} 
[wandb][INFO] Logging user logs to /home/xxx/work_space/Exercise_1-ALL_DDP/wandb/offline-run-20210618_095922-2pha5qs6/logs/debug.log 
[wandb][INFO] Logging internal logs to /home/xxx/work_space/Exercise_1-ALL_DDP/wandb/offline-run-20210618_095922-2pha5qs6/logs/debug-internal.log 
[wandb][INFO] calling init triggers 
[wandb][INFO] wandb.init called with sweep_config: {}
config: {'name'.....} 
[wandb][INFO] starting backend 
[wandb][INFO] multiprocessing start_methods=fork,spawn,forkserver, using: spawn 
[wandb][INFO] starting backend process... 
[wandb][INFO] started backend process with pid: 11615 
[wandb][INFO] backend started and connected 
[wandb][DEBUG] no default config file found in config-defaults.yaml 
[wandb][INFO] updated telemetry 
[wandb][INFO] starting run threads in backend 
wandb: W&B syncing is set to `offline` in this directory.  Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[wandb][INFO] atexit reg 
[wandb][INFO] redirect: SettingsConsole.REDIRECT 
[wandb][INFO] Redirecting console. 
[wandb][INFO] Redirects installed. 
[wandb][INFO] run started, returning control to user process 
[Trainer][CRITICAL] distributed data parallel training: on 
W&B offline, running your script from this directory will only write metadata locally.
W&B offline, running your script from this directory will only write metadata locally.
W&B offline, running your script from this directory will only write metadata locally.
W&B offline, running your script from this directory will only write metadata locally.
W&B offline, running your script from this directory will only write metadata locally.
W&B offline, running your script from this directory will only write metadata locally.
W&B offline, running your script from this directory will only write metadata locally.
W&B offline, running your script from this directory will only write metadata locally.
W&B offline, running your script from this directory will only write metadata locally.
W&B offline, running your script from this directory will only write metadata locally.
W&B offline, running your script from this directory will only write metadata locally.
W&B offline, running your script from this directory will only write metadata locally.
W&B offline, running your script from this directory will only write metadata locally.
W&B offline, running your script from this directory will only write metadata locally.
W&B offline, running your script from this directory will only write metadata locally.
W&B offline, running your script from this directory will only write metadata locally.
  0%|                                                   | 0/366 [00:00<?, ?it/s]

**Wandb features**
I used pytorch inbuilt mp.spawn to start parallel and I use the example code to run everything. Here is the pseudo code for my program:
```python
mp.spawn(main, nprocs=config['n_gpu'], args=(config['n_gpu'], config, args, False)) 

def main(gpu,  ngpus_per_node, config, args, resume):
    args.local_rank = gpu
    /* initialize settings of everything
       ......
     */
    if not config['ddp_training'] or args.local_rank <= 0:
        import wandb
        wandb.init(project=config['name'], name=config['experim_name'], config=config)
        trainer = Trainer(
            xxxx)
    else:
        trainer = Trainer(
            xxxx)
    wandb.finish()
```
**Environment**
- OS: Ubuntu 18.04,  
- Environment: cuda 11.2, wandb==0.10.25, torch==1.8.1+cu111, torchaudio==0.8.1, torchvision==0.9.1+cu111
- Python Version: python 3.7,


