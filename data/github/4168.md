TITLE:
[CLI]: WARNING Config item 'hyperparam_name' was locked by 'sweep' (ignored update) 

LABEL:
c:sweeps,cli

STATE:
open

BODY:
### Describe the bug

<!--- Description of the issue below  -->
`wandb` gives this warning right starting on training and testing when I run hyper parameter search using sweep via cmd.
<!--- A minimal code snippet between the quotes below  -->
```shell
# pl training 
20.8 M    Trainable params
0         Non-trainable params
20.8 M    Total params
41.615    Total estimated model params size (MB)
wandb: WARNING Config item 'kg_embedding_dim' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'graph_type' was locked by 'sweep' (ignored update).

# pl testing
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
wandb: WARNING Config item 'kg_embedding_dim' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'graph_type' was locked by 'sweep' (ignored update).


```

Only these two parameters are raised but not all, of my hyperparameters. I am using `PyTorch Lightning` and `WandB logger` callback for metrics and parameters logging.

I have double-checked that I am not calling `wandb.config.update()` anywhere in my code.

## Example sweep.yaml
<!--- A full traceback of the exception in the quotes below -->
```yaml
program: main.py
method: grid
project: test
command:
  - ${env}
  - ${interpreter}
  - ${program}
  - "--sweep"
metric:
  goal: minimize
parameters:
  name: "val/rec/epoch_loss"
  kg_embedding_dim:
    distribution: categorical
    values: [ 128, 256 ]
  n_bases:
    distribution: constant
    value: 8
  graph_type:
    distribution: categorical
    values: [ "gat", "rgcn" ]
```

## Pseudo Code
```python
parser = argparse.ArgumentParser()
parser.add_argument(
        '-c',
        '--config',
        type=str,
        default="config/default.yaml",
        help='config file(yaml) path')
parser.add_argument(
        '--sweep',
        default=False,
        help='whether to sweep hyper parameters',
        dest='sweep',
        action=argparse.BooleanOptionalAction,
    )

args = parser.parse_args()
    # load yaml file to build config dictionary
    config_dict = {}
    with open(args.config, 'r', encoding='utf-8') as f:
        config_dict |= yaml.safe_load(f.read())

if args.sweep:
    # pass default configs and 
    # let wandb control hyperparamters via sweep
    wandb.init(config=config_dict, project="test")
    config_dict = wandb.config
    config_dict["sweep"] = True
else:
    config_dict["sweep"] = False

run_system(config_dict)


## in the run_system

if self.sweep:
   self.logger = WandbLogger()

self.trainer = Trainer(logger=[self.logger]
# warning raised here
self.trainer.fit(self.model, self.data)

# warning raised from here
self.model.load_from_checkpoint()
self.trainer.test(self.model, self.data)
```


### Additional Files

_No response_

### Environment

WandB version: 0.13.2

OS: Liniux

Python version: 3.9

Versions of relevant libraries:
pytorch_lighting:  1.6.4


### Additional Context

I found similar issue #2641 but not sure if its the case as 
I use `pl.save_hyperparameters()` to save model parameters but only a few parameters are raised in the training and testing. 

These warnings are raised after lighting estimated model trainable params during training and checking local ranks during testing.

