TITLE:
Out of memory if logging gradients with HalfTensor

LABEL:
stale

STATE:
closed

BODY:
I train big model and want logging gradients. I use FP16 in torch. One layer has 180kk params. I think that reason is https://github.com/wandb/client/blob/58405877a36beaba556cce31de304527b5245aa6/wandb/wandb_torch.py#L168-L170
Tensor was coped and upconvert in Float on gpu memory. Comment is not actual. Now HalfTensor can be used on cpu.
https://pytorch.org/docs/stable/tensors.html

`
tensor = tensor.cpu().clone().type(torch.FloatTensor).detach()
`

