TITLE:
[CLI]: wandb.init fails with JAX GPU memory preallocation

LABEL:
cli,c:flexible-env

STATE:
open

BODY:
### Describe the bug

<!--- Description of the issue below  -->

By default, JAX [preallocates](https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html) 90% of GPU memory when it starts. This fraction can be toggled with a command line flag. I found that the default setting causes `wandb.init` to error, despite still having a substantial amount of memory left.

<!--- A minimal code snippet between the quotes below  -->
I apologize for being unable to provide a minimal code snippet, but I'll document the error I found below.

I found this issue when running the example MNIST classification command from the annotated-s4 repo:
https://github.com/srush/annotated-s4
```shell
python -m s4.train --dataset mnist-classification --model s4 --epochs 10 --bsz 128 --d_model 128 --ssm_n 64
```
As per the JAX documentation, this is equivalent to this preallocation flag:
```shell
XLA_PYTHON_CLIENT_MEM_FRACTION=.9 python -m s4.train --dataset mnist-classification --model s4 --epochs 10 --bsz 128 --d_model 128 --ssm_n 64
```
However, if I instead pass in `XLA_PYTHON_CLIENT_MEM_FRACTION=.85`, the `wandb.init` succeeds.

I am using an A100 GPU with 40Gb memory, so after 90% preallocation, there is still 4Gb memory left. I can't imagine that this should cause an issue.


<!--- A full traceback of the exception in the quotes below -->
I don't know that this trace is useful, because `wandb.init` consistently errors for every JAX program I use and different programs will all print different traces.
```shell
[*] Setting Randomness...
wandb: Currently logged in as: albert (hazy-research). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.12.17
wandb: Run data is saved locally in /home/workspace/annotated-s4/wandb/run-20220608_153536-1kr6thvl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glamorous-plasma-7
wandb: ⭐️ View project at https://wandb.ai/hazy-research/s4
wandb: 🚀 View run at https://wandb.ai/hazy-research/s4/runs/1kr6thvl
[*] Generating MNIST Classification Dataset...
[*] Starting `s4` Training on `mnist-classification` =>> Initializing...
[*] Starting Training Epoch 1...
  0%|                                                                                                                                                                                                                                                                                                                                                                | 0/469 [00:00<?, ?it/s]2022-06-08 15:36:13.977863: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_blas.cc:232] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2022-06-08 15:36:13.977910: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_blas.cc:234] Failure to initialize cublas may be due to OOM (cublas needs some free memory when you initialize it, and your deep-learning framework may have preallocated more than its fair share), or may be because this binary was not built with support for the GPU in your machine.
2022-06-08 15:36:13.978046: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2141] Execution of replica 0 failed: INTERNAL: Attempting to perform BLAS operation using StreamExecutor without BLAS support
  0%|                                                                                                                                                                                                                                                                                                                                                                | 0/469 [00:12<?, ?it/s]
Traceback (most recent call last):
  File "/home/workspace/annotated-s4/s4/train.py", line 444, in <module>
    example_train(
  File "/home/workspace/annotated-s4/s4/train.py", line 338, in example_train
    state, train_loss = train_epoch(
  File "/home/workspace/annotated-s4/s4/train.py", line 138, in train_epoch
    state, loss = train_step(
  File "/home/miniconda3/envs/jax/lib/python3.9/site-packages/jax/_src/traceback_util.py", line 162, in reraise_with_filtered_traceback
    return fun(*args, **kwargs)
  File "/home/miniconda3/envs/jax/lib/python3.9/site-packages/jax/_src/api.py", line 473, in cache_miss
    out_flat = xla.xla_call(
  File "/home/miniconda3/envs/jax/lib/python3.9/site-packages/jax/core.py", line 1765, in bind
    return call_bind(self, fun, *args, **params)
  File "/home/miniconda3/envs/jax/lib/python3.9/site-packages/jax/core.py", line 1781, in call_bind
    outs = top_trace.process_call(primitive, fun_, tracers, params)
  File "/home/miniconda3/envs/jax/lib/python3.9/site-packages/jax/core.py", line 678, in process_call
    return primitive.impl(f, *tracers, **params)
  File "/home/miniconda3/envs/jax/lib/python3.9/site-packages/jax/_src/dispatch.py", line 185, in _xla_call_impl
    return compiled_fun(*args)
  File "/home/miniconda3/envs/jax/lib/python3.9/site-packages/jax/_src/dispatch.py", line 615, in _execute_compiled
    out_bufs_flat = compiled.execute(input_bufs_flat)
jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Attempting to perform BLAS operation using StreamExecutor without BLAS support

The stack trace below excludes JAX-internal frames.
The preceding is the original exception that occurred, unmodified.

--------------------

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/miniconda3/envs/jax/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/miniconda3/envs/jax/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/workspace/annotated-s4/s4/train.py", line 444, in <module>
    example_train(
  File "/home/workspace/annotated-s4/s4/train.py", line 338, in example_train
    state, train_loss = train_epoch(
  File "/home/workspace/annotated-s4/s4/train.py", line 138, in train_epoch
    state, loss = train_step(
jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Attempting to perform BLAS operation using StreamExecutor without BLAS support
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb:
wandb: Synced glamorous-plasma-7: https://wandb.ai/hazy-research/s4/runs/1kr6thvl
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220608_153536-1kr6thvl/logs
```


### Additional Files

_No response_

### Environment

WandB version: `0.12.17`

OS:
```
NAME="Ubuntu"
VERSION="18.04.5 LTS (Bionic Beaver)"
```

Python version: `3.9.12`

Versions of relevant libraries:
```
jax                          0.3.13
jaxlib                       0.3.10+cuda11.cudnn82
```


### Additional Context

_No response_

