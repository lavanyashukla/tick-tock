TITLE:
improving distributed logging story

LABEL:
feature_request

STATE:
closed

BODY:
I played around with a few different ways of distributed logging in wandb and I'm learning towards recommending to log from just 1 worker for the PyTorch/AWS [tutorial](https://docs.google.com/document/d/1LuCACKdXTbptHzXb81sGA1CevRW44wQ5znxXylj8S5A/edit#heading=h.d1y77q80usbq)

IE
```
if rank=0:
  wandb.init(...)
```
then replace wandb.log with "wandb_log" where
```
def wandb_log(*args_, **kwargs_):
    if rank=0:
        wandb.log(*args_, **kwargs_)
```
Reasons:
1. 95% of the time, you don't care about logs from other workers, chief worker has all info you need. Having a mix of single runs and grouped runs complicates runs interface. People using TensorBoard only log from chief worker.
2. Using grouping doesn't really work -- if you have 8-worker trainer and run it twice, wandb will show it as 16-worker group. There's a complicated solution in https://github.com/wandb/client/issues/335#issuecomment-493284910 
3. 5% of the time, one worker crashes or is slow, so you want to look at individual workers. In that case it makes sense to log from all workers and do grouping. To avoid merging of groups, use unique group name, ie group_name='debug_attempt_523'

What wandb can do:
- replacing wandb.log with wandb_log is extra work on the user. Currently it is necessary because otherwise wandb.log crashes. One idea: `wandb.init(no_op=True)`, makes all subsequent `wandb.log` return immediately instead of crashing



