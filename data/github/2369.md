TITLE:
[Q] sync tensorboard step issue

LABEL:
feature_request,stale

STATE:
closed

BODY:
Hi,

Thanks for this amazing piece of program! 

I am switching from pytorch + tensorboard to wandb. I use the argument sync_tensorboard = True in wandb.init() and then create the tensorboard summary writer. When I add a scalar to tensorboard, I can customize the global_step argument so that each metric has a different axis (sometimes being epochs, episodes, training steps, etc.). I noticed that wandb has its own step counter and I also noticed that I can change the x-axis in the wandb online GUI. 

The problem is, when I am trying to log a metric, I pass the tensorboard writer as an object to another class defined in a different file, but this metric has a different global_step from the other logged metrics, and then when I check the online panel, the x-axis is messed up. I have no idea what that number is. When I switch to the default wandb "Step", it displays a weird number, and when I switch to "global_step" (which I believe should be the step I passed into my tensorboard writer), it becomes the same step as the other metrics, logged in the main training file. Neither of them is what I passed to the tensorboard writer.

I understand that my problem is quite hard to understand. So a general question is, what is the recommended workflow for switching from tensorboard to wandb? **Especially I am using tensorboard to log a lot of different metrics with different customized x axis**? I notice that the online doc mentioned wandb.tensorboard.pacth, what exactly is this? I did not find any online doc for this function and how is it different from setting sync_tensorboard = True in wandb.init()?

Or, if I am gonna abandon tensorboard completely, what are the most convenient ways to log different metrics, with different **CUSTOMIZED** x axis?

Thanks a lot!

Sincerely

