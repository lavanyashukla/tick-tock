TITLE:
[Q] How To Efficiently Log Moving Average

LABEL:
c:misc

STATE:
closed

BODY:
I am new to wandb. I would like to simplify logging as much as possible, ideally limiting the need for too much logging code. The examples I create will be shared with my students, so the less lines to explain -- the better.

Here is what my current train loop looks like. 

```
def train(encoder, decoder, encoder_optim, decoder_optim, criterion, iterable):

    wandb.watch(encoder, criterion, log="all", log_freq=5000)
    wandb.watch(decoder, criterion, log="all", log_freq=5000)

    encoder.train()
    decoder.train()

    ### Initialize Log
    moving_total = 0
    moving_average = []

    for iter, (input_tensor, target_tensor) in enumerate(iterable):

        ### Forward Propagation & Loss Calculation
        loss = train_iter(input_tensor, target_tensor, encoder, decoder, criterion)

        ### Initialize Gradients
        encoder_optim.zero_grad()
        decoder_optim.zero_grad()

        ### Backward Propagation
        loss.backward()

        ### Gradient Descent
        encoder_optim.step()
        decoder_optim.step()

        ### Log Results
        moving_total += loss.item() / len(target_tensor)
        if (iter + 1) % 5000 == 0:
            print("{} | Loss {:.4f}".format(iter + 1, moving_total / 5000))
            wandb.log({"Loss": moving_total / 5000}, step=iter+1)
            moving_total = 0

    return moving_average
```

Is it possible to consolidate "Initialize Log" and "Log Results"? If log_freq is too small, then training gets really slow. Ideally it would function as implemented above, where a list is constructed until loq_freq is met. I want to use only in-built wandb accounting. It seems to defeat the purpose of the framework to do the accounting externally -- which makes me think I am using it wrong.

Thank you for your suggestions and feedback.

