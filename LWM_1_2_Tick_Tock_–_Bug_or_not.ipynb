{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKI9JtLfzDZtGjgkEM1gJz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lavanyashukla/tick-tock/blob/main/LWM_1_2_Tick_Tock_%E2%80%93_Bug_or_not.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I. Setup"
      ],
      "metadata": {
        "id": "4i82ncQQx5NJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CtmahQp9cBCV"
      },
      "outputs": [],
      "source": [
        "!pip install openai -qq\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import random\n",
        "import csv\n",
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/lavanyashukla/tick-tock.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8IK_HCJbYcq",
        "outputId": "a964e096-6733-4b1e-a2d3-fc15900e3d17"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'tick-tock' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data\n",
        "input_file = 'tick-tock/data/support_tickets_clean.csv'\n",
        "\n",
        "# Open the CSV file for reading\n",
        "with open(input_file, mode='r', encoding='utf-8') as csvfile:\n",
        "    # Use csv.DictReader to read the CSV file into a dictionary\n",
        "    reader = csv.DictReader(csvfile)\n",
        "\n",
        "    # Initialize a list to store each row (as a dictionary)\n",
        "    data = []\n",
        "\n",
        "    # Iterate over the rows in the CSV file\n",
        "    for row in reader:\n",
        "        # Each row is a dictionary\n",
        "        data.append(row)\n",
        "\n",
        "# Now 'data' is a list of dictionaries, where each dictionary represents a row from the CSV\n",
        "print(data[0])\n",
        "print(len(data))"
      ],
      "metadata": {
        "id": "wFoGv3BHbjPT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a1bcea0-f0ce-47ad-f87c-c54a62d46852"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'description': '10 out of 10\\nComments: GrandMaster Training Data Platform\\nChannel: In-app', 'raw_subject': 'NPS Response', 'subject': 'NPS Response', 'priority': 'low', 'problem_id': '', 'tags': \"['nps_score', 'pendo_nps', 'personal', 'question']\", 'id': '36652', 'question': 'question', 'customer_type': \"['personal']\", 'customer_type_2': '', 'type': 'nps_score'}\n",
            "26590\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the 'question' values where available\n",
        "questions = [item.get('question', 'No question provided') for item in data]\n",
        "\n",
        "# Counting occurrences of each unique question (or the placeholder 'No question provided')\n",
        "question_counts = pd.Series(questions).value_counts().reset_index()\n",
        "question_counts.columns = ['Question', 'Count']\n",
        "\n",
        "# Display the counts in a table\n",
        "print(question_counts.to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JKtYM79mSNN",
        "outputId": "08ed74f6-19a5-42fc-bcf9-634accaec024"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Question  Count\n",
            "            question  21173\n",
            "                none   2389\n",
            "            type_bug   1536\n",
            "type_feature_request   1492\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty set to store unique questions\n",
        "unique_questions = set()\n",
        "\n",
        "# Iterate over each dictionary in the data list\n",
        "for item in data:\n",
        "    # Check if 'question' key exists in the dictionary\n",
        "    if 'question' in item:\n",
        "        # Add the question to the set (sets automatically handle uniqueness)\n",
        "        unique_questions.add(item['question'])\n",
        "\n",
        "# Convert the set back to a list to get a list of unique questions\n",
        "desired_tags = list(unique_questions)\n",
        "\n",
        "print(desired_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hqgl6ATzopAF",
        "outputId": "997009d7-4931-46b0-941f-d7c72e871478"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['type_feature_request', 'type_bug', 'none', 'question']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps:\n",
        "\n",
        "- Navigate to the \"Secrets\" pane from the left navigation bar\n",
        "- Add the OPENAI_API_KEY name and value\n",
        "- Turn on the toggle of \"Notebook access\""
      ],
      "metadata": {
        "id": "bPyiIh2wcHQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=userdata.get('OPENAI_API_KEY')\n",
        ")\n",
        "\n",
        "desired_tags = [\"question\", \"none\", \"type_bug\", \"type_feature_request\"]\n",
        "filtered_data = [row for row in data if row.get('question') in desired_tags]\n",
        "# filtered_data = data"
      ],
      "metadata": {
        "id": "niaOe97XbmAU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make a filtered dataset"
      ],
      "metadata": {
        "id": "PLe3r81Hx9Qp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's look at the data\n",
        "show_elem = 4\n",
        "print(json.dumps(data[show_elem], indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWg_Xuece4-_",
        "outputId": "783140a4-66d6-4914-ccbf-fc9e39d3b7bb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"description\": \"10 out of 10\\nComments: No message provided.\\nChannel: Email\",\n",
            "    \"raw_subject\": \"NPS Response\",\n",
            "    \"subject\": \"NPS Response\",\n",
            "    \"priority\": \"low\",\n",
            "    \"problem_id\": \"\",\n",
            "    \"tags\": \"['nps_score', 'pendo_nps', 'question']\",\n",
            "    \"id\": \"36643\",\n",
            "    \"question\": \"question\",\n",
            "    \"customer_type\": \"\",\n",
            "    \"customer_type_2\": \"\",\n",
            "    \"type\": \"nps_score\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count, score_priority, score_tags = 0, 0, 0\n",
        "acc = {}"
      ],
      "metadata": {
        "id": "yzY-EL9te5P1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Engineering"
      ],
      "metadata": {
        "id": "aIULZF-HyB8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(prompt, model=\"gpt-3.5-turbo\", temperature=0):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "evalset_start_all, evalset_end_all = 500, 520"
      ],
      "metadata": {
        "id": "f07dqQqCy8zl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OILxp2VKu6Nn",
        "outputId": "d6c25e85-a185-4343-e39a-e32aea96f7e3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# II. Basic Prompt + Evaluation"
      ],
      "metadata": {
        "id": "nj8AuHp1QIxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def basic_prompt(evalset_start=400, evalset_end=500):\n",
        "  count, score_tags = 0, 0\n",
        "  for element in filtered_data[evalset_start:evalset_end]:\n",
        "      # print(element)\n",
        "      ticket_text = element['description']\n",
        "\n",
        "      # Prompt – Classify Class\n",
        "      prompt = f\"\"\"\n",
        "      Classify the text delimited by triple backticks into one of the following classes.\n",
        "      Classes: {desired_tags}\n",
        "      Text: ```{ticket_text}```\n",
        "      Class: \"\"\"\n",
        "\n",
        "      response = get_completion(prompt)\n",
        "\n",
        "      print(\"Prediction: \"+response)\n",
        "\n",
        "      if(response == element['question']):\n",
        "          score_tags += 1\n",
        "          print(\"Correct. Actual: \"+element['question'])\n",
        "      else:\n",
        "          print(\"Incorrect. Actual: \"+element['question'])\n",
        "      count += 1\n",
        "      print()\n",
        "\n",
        "  print(\"__________________\")\n",
        "  print(f\"Priority Accuracy: {score_tags/count}\")\n",
        "  return score_tags/count\n",
        "acc['basic_prompt'] = basic_prompt(evalset_start_all, evalset_end_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnqL1a1EbHM3",
        "outputId": "50ebc920-66b6-4ebb-8b9e-3c4b966ae693"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: type_bug\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: type_feature_request\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: type_bug\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: type_feature_request\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "__________________\n",
            "Priority Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add Examples"
      ],
      "metadata": {
        "id": "EQdoc8KfQFfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add examples to the prompt. pick examples randomly unless start is set.\n",
        "# supports adding both random examples and a pre-defined set of examples\n",
        "# evaluate on examples filtered_data[evalset_start:evalset_end]\n",
        "def add_examples(num_examples=5, start=0, evalset_start=400, evalset_end=500, model=\"gpt-3.5-turbo\", temperature=0\n",
        "                 ):\n",
        "  count, score_tags = 0, 0\n",
        "  for element in filtered_data[evalset_start:evalset_end]:\n",
        "      if start==0:\n",
        "        # Ensure num_examples does not exceed the length of filtered_data\n",
        "        num_samples = min(num_examples, len(filtered_data))\n",
        "\n",
        "        # Randomly pick num_samples elements from filtered_data\n",
        "        random_elements = random.sample(filtered_data, num_samples)\n",
        "      else:\n",
        "        random_elements = filtered_data[start:start+num_examples]\n",
        "\n",
        "      examples = \"\".join([\n",
        "          f\"Text: ```{element['description']}```\\nClass: {element.get('question', 'No question')}\\n\"\n",
        "          for element in random_elements\n",
        "      ])\n",
        "\n",
        "      # print(element)\n",
        "      ticket_text = element['description']\n",
        "\n",
        "      # Prompt – Classify Class\n",
        "      prompt = f\"\"\"\n",
        "      Classify the text delimited by triple backticks into one of the following classes.\n",
        "      Classes: {desired_tags}\n",
        "\n",
        "      {examples}\n",
        "\n",
        "      Text: ```{ticket_text}```\n",
        "      Class: \"\"\"\n",
        "      response = get_completion(prompt, model=model, temperature=temperature)\n",
        "\n",
        "      # print(\"Prompt: \"+prompt)\n",
        "      print(\"Prediction: \"+response)\n",
        "\n",
        "      if(response == element['question']):\n",
        "          score_tags += 1\n",
        "          print(\"Correct. Actual: \"+element['question'])\n",
        "      else:\n",
        "          print(\"Incorrect. Actual: \"+element['question'])\n",
        "      count += 1\n",
        "      print()\n",
        "\n",
        "  print(\"__________________\")\n",
        "  print(f\"Priority Accuracy: {score_tags/count}\")\n",
        "  return score_tags/count\n",
        "acc['add_5_examples'] = add_examples(5, 1000, evalset_start_all, evalset_end_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVZp113EEid7",
        "outputId": "878e683b-64df-48e6-aca0-a171bc0f3ea7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: type_feature_request\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "__________________\n",
            "Priority Accuracy: 0.35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc['add_5_random_examples'] = add_examples(5, 0, evalset_start_all, evalset_end_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpBt0ZJkuBCZ",
        "outputId": "bcc4cd47-510f-4364-d1cc-39bbd3d37018"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: type_bug\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: type_bug\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "__________________\n",
            "Priority Accuracy: 0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc['add_5_examples_gpt4'] = add_examples(5, 1000, evalset_start_all, evalset_end_all, model=\"gpt-4\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLxdvalkEiGK",
        "outputId": "b0f1ff99-ed35-475a-a931-806f28cca390"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: type_bug\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: type_bug\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "__________________\n",
            "Priority Accuracy: 0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc['add_20_random_examples'] = add_examples(20, 0, evalset_start_all, evalset_end_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3m4_y9WE3Rx",
        "outputId": "830d02c6-71ad-4717-b2e6-73e70bb61eec"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: type_feature_request\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: type_bug\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: type_bug\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: type_feature_request\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "__________________\n",
            "Priority Accuracy: 0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc['add_20_examples'] = add_examples(20,1000, evalset_start_all, evalset_end_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFPc1HDguiD3",
        "outputId": "8de35fc0-0120-4886-98a7-3f96a344f4cb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "__________________\n",
            "Priority Accuracy: 0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Change Temperature"
      ],
      "metadata": {
        "id": "3sT4nUM_P_vQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc['add_20_examples_change_temp'] = add_examples(20, 1000, evalset_start_all, evalset_end_all, temperature=0.7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HuGEddGDqyu",
        "outputId": "fa485fbf-5bce-4512-a040-83a9af2c03fa"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "__________________\n",
            "Priority Accuracy: 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Change model to GPT-4"
      ],
      "metadata": {
        "id": "hudPh8kEP8aW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc['add_20_examples_gpt4'] = add_examples(20, 1000, evalset_start_all, evalset_end_all, model=\"gpt-4\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWmLELPlEZp_",
        "outputId": "b2f139ea-318a-4da3-9474-4fc9e6e7f2bb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: type_bug\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: type_bug\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "__________________\n",
            "Priority Accuracy: 0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Describe Classes"
      ],
      "metadata": {
        "id": "Zp1J33DqP2WN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add examples to the prompt. pick examples randomly unless start is set.\n",
        "# supports adding both random examples and a pre-defined set of examples\n",
        "# evaluate on examples filtered_data[evalset_start:evalset_end]\n",
        "def describe_classes(num_examples=5, start=0, evalset_start=2500, evalset_end=2510, model=\"gpt-3.5-turbo\", temperature=0):\n",
        "  count, score_tags = 0, 0\n",
        "  for element in filtered_data[evalset_start:evalset_end]:\n",
        "      if start==0:\n",
        "        # Ensure num_examples does not exceed the length of filtered_data\n",
        "        num_samples = min(num_examples, len(filtered_data))\n",
        "\n",
        "        # Randomly pick num_samples elements from filtered_data\n",
        "        random_elements = random.sample(filtered_data, num_samples)\n",
        "      else:\n",
        "        random_elements = filtered_data[start:start+num_examples]\n",
        "\n",
        "      examples = \"\".join([\n",
        "          f\"Text: ```{element['description']}```\\nClass: {element.get('question', 'No question')}\\n\"\n",
        "          for element in random_elements\n",
        "      ])\n",
        "\n",
        "      # print(element)\n",
        "      ticket_text = element['description']\n",
        "\n",
        "      # Prompt – Classify Class\n",
        "      prompt = f\"\"\"\n",
        "      Given the following description for each class:\n",
        "      type_feature_request: A request for a feature by a user of Weights & Biases.\n",
        "      type_bug: A bug report by a user of Weights & Biases.\n",
        "      question: If the request is not related to Weights & Biases; or doesn't fit the above 2 categories.\n",
        "\n",
        "      Classify the text delimited by triple backticks into one of the following classes.\n",
        "      Classes: {desired_tags}\n",
        "\n",
        "      {examples}\n",
        "\n",
        "      Text: ```{ticket_text}```\n",
        "      Class: \"\"\"\n",
        "      response = get_completion(prompt)\n",
        "\n",
        "      # print(\"Prompt: \"+prompt)\n",
        "      print(\"Prediction: \"+response)\n",
        "\n",
        "      if(response == element['question']):\n",
        "          score_tags += 1\n",
        "          print(\"Correct. Actual: \"+element['question'])\n",
        "      else:\n",
        "          print(\"Incorrect. Actual: \"+element['question'])\n",
        "      count += 1\n",
        "      print()\n",
        "\n",
        "  print(\"__________________\")\n",
        "  print(f\"Priority Accuracy: {score_tags/count}\")\n",
        "  print()\n",
        "  return score_tags/count\n",
        "acc['add_5_examples_describe_classes'] = describe_classes(5,1000, evalset_start_all, evalset_end_all)\n",
        "acc['add_20_examples_describe_classes'] = describe_classes(20,1000, evalset_start_all, evalset_end_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75FRFDQQJvl5",
        "outputId": "b862cd93-f7ea-49fb-f6cf-053807fa52eb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: type_bug\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: type_feature_request\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "__________________\n",
            "Priority Accuracy: 0.4\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "__________________\n",
            "Priority Accuracy: 0.9\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Specify Steps"
      ],
      "metadata": {
        "id": "7fYxSsIcPtE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add examples to the prompt. pick examples randomly unless start is set.\n",
        "# supports adding both random examples and a pre-defined set of examples\n",
        "# evaluate on examples filtered_data[evalset_start:evalset_end]\n",
        "def specify_steps(num_examples=5, start=0, evalset_start=2500, evalset_end=2510, examples=1, model=\"gpt-3.5-turbo\", temperature=0):\n",
        "  count, score_tags = 0, 0\n",
        "  for element in filtered_data[evalset_start:evalset_end]:\n",
        "      if start==0:\n",
        "        # Ensure num_examples does not exceed the length of filtered_data\n",
        "        num_samples = min(num_examples, len(filtered_data))\n",
        "\n",
        "        # Randomly pick num_samples elements from filtered_data\n",
        "        random_elements = random.sample(filtered_data, num_samples)\n",
        "      else:\n",
        "        random_elements = filtered_data[start:start+num_examples]\n",
        "\n",
        "      examples = \"\".join([\n",
        "          f\"Text: ```{element['description']}```\\nClass: {element.get('question', 'No question')}\\n\"\n",
        "          for element in random_elements\n",
        "      ])\n",
        "\n",
        "      # print(element)\n",
        "      ticket_text = element['description']\n",
        "\n",
        "      # Prompt – Classify Class\n",
        "      if examples:\n",
        "        prompt = f\"\"\"\n",
        "        Given the text delimited by triple backticks, perform the following actions:\n",
        "        1 - Summarize what the user wants in 1-2 lines.\n",
        "        2 - Recommend a next action based on the user' request.\n",
        "        3 - Determine if the request is related to the product or company 'Weights & Biases'\n",
        "        4 - Classify the into one of the following classes. Classes: {desired_tags}\n",
        "        5 - Output a json object that contains the following keys: summary, recommended_action, is_wb, class\n",
        "\n",
        "        Here are some examples help you with the classification step.\n",
        "        {examples}\n",
        "\n",
        "        And here's the text ```{ticket_text}```\"\"\"\n",
        "      else:\n",
        "        prompt = f\"\"\"\n",
        "        Given the text delimited by triple backticks, perform the following actions:\n",
        "        1 - Summarize what the user wants in 1-2 lines.\n",
        "        2 - Recommend a next action based on the user' request.\n",
        "        3 - Determine if the request is related to the product or company 'Weights & Biases'\n",
        "        4 - Classify the into one of the following classes. Classes: {desired_tags}\n",
        "        5 - Output a json object that contains the following keys: summary, recommended_action, is_wb, class\n",
        "\n",
        "        Here's the text ```{ticket_text}```\"\"\"\n",
        "\n",
        "      response_json = get_completion(prompt)\n",
        "      # print(\"Prompt: \"+prompt)\n",
        "      print(\"Prediction: \")\n",
        "      response = json.loads(response_json)\n",
        "      print(\"Summary: \"+response['summary'])\n",
        "      print(\"Recommended Action: \"+response['recommended_action'])\n",
        "      print(\"Is W&B Related: \"+str(response['is_wb']))\n",
        "      print(\"Prediction: \"+response['class'])\n",
        "\n",
        "      if(response['class'] == element['question']):\n",
        "          score_tags += 1\n",
        "          print(\"Correct. Actual: \"+element['question'])\n",
        "      else:\n",
        "          print(\"Incorrect. Actual: \"+element['question'])\n",
        "      count += 1\n",
        "      print()\n",
        "\n",
        "  print(\"__________________\")\n",
        "  print(f\"Priority Accuracy: {score_tags/count}\")\n",
        "  return score_tags/count\n",
        "\n",
        "acc['specify_steps'] = specify_steps(5,1000, evalset_start_all, evalset_end_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YXwWv3ZIF2z",
        "outputId": "f051ffe7-ae75-4512-98a9-ee57d50c2a00"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: \n",
            "Summary: User rated something 10 out of 10 with no message provided.\n",
            "Recommended Action: Ask the user for more details or feedback on what they rated.\n",
            "Is W&B Related: False\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User rated something 10 out of 10 with no message provided.\n",
            "Recommended Action: Ask the user for more details or feedback on what they rated.\n",
            "Is W&B Related: False\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: The user is promoting an event by The Wall Street Journal about the future of transportation.\n",
            "Recommended Action: Consider registering for the event to learn about e-bikes and map apps.\n",
            "Is W&B Related: False\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User is experiencing loading issues with their local environment and has a question.\n",
            "Recommended Action: Check stdout or system logs for error messages and consider restarting the server with the environment variable LOCAL_RESTORE=true if unable to login.\n",
            "Is W&B Related: False\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User sent a message with contact information and details about their chat session.\n",
            "Recommended Action: Follow up with Luis regarding their message and chat session.\n",
            "Is W&B Related: True\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User is having a conversation with Luis.\n",
            "Recommended Action: Continue the conversation with Luis.\n",
            "Is W&B Related: True\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User is offering to sell AWS re:Invent Attendees Data List 2022.\n",
            "Recommended Action: If interested, the user can respond to Susan Miller for more information.\n",
            "Is W&B Related: False\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User is facing an error 'wandb: ERROR Error while calling W&B API: user is not logged in (Response [401])' despite running 'wandb login' multiple times after switching to team's WandB account.\n",
            "Recommended Action: Contact the support team of Weights & Biases for assistance with the account setup/authentication.\n",
            "Is W&B Related: True\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User rates the experience as 10 out of 10 and finds it convenient.\n",
            "Recommended Action: Consider asking the user for more detailed feedback or suggestions for improvement.\n",
            "Is W&B Related: False\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User wants to cancel their team plan named ct-unet and needs help with the process.\n",
            "Recommended Action: Recommend providing instructions on how to cancel the team plan.\n",
            "Is W&B Related: True\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User rated something 8 out of 10 with no message provided.\n",
            "Recommended Action: Ask the user for more details or feedback on what they rated.\n",
            "Is W&B Related: False\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: The user wants to inform about the product updates in October and share an article about solving deep learning development problems with Scaletorch Workstations.\n",
            "Recommended Action: Check out the product updates and read the article to learn how Scaletorch Workstations can solve deep learning development problems.\n",
            "Is W&B Related: False\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: The user is requesting to cancel the approval and change the credit card for renewal of their Seegene account, as well as renew with a reduced account.\n",
            "Recommended Action: Contact Seegene customer support to assist with canceling the approval, changing the credit card, and renewing the account with reduced seats.\n",
            "Is W&B Related: False\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User is reporting a rating of 7 out of 10 with no additional comments.\n",
            "Recommended Action: Request more information or feedback from the user to understand the reason behind the rating.\n",
            "Is W&B Related: False\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User rated something 10 out of 10 with no message provided.\n",
            "Recommended Action: Ask the user for more details or feedback on what they rated.\n",
            "Is W&B Related: False\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User wants to know about the rating and comments left on a channel.\n",
            "Recommended Action: Check the channel for the rating and comments.\n",
            "Is W&B Related: False\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: Researcher at an autonomous driving startup company interested in consulting the price of Private Hosting for 10-20 potential users.\n",
            "Recommended Action: Provide information on Private Hosting pricing and potentially offer a demo to showcase the product.\n",
            "Is W&B Related: False\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User rated something 10 out of 10 with no message provided.\n",
            "Recommended Action: Ask the user for more details or feedback on what they rated.\n",
            "Is W&B Related: False\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User is looking to change their username on the platform but cannot find the option to do so.\n",
            "Recommended Action: Check the platform's settings or contact customer support for assistance in changing the username.\n",
            "Is W&B Related: False\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User rated something 10 out of 10 with no message provided.\n",
            "Recommended Action: Ask the user for more details or feedback on what they rated.\n",
            "Is W&B Related: False\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "__________________\n",
            "Priority Accuracy: 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# acc['specify_steps_no_examples'] = specify_steps(5,1000, evalset_start_all, evalset_end_all, examples=0)"
      ],
      "metadata": {
        "id": "q0b1FJ7HMzLh"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explain LLM Reasoning Behind Steps"
      ],
      "metadata": {
        "id": "JuUlt_atPYnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add examples to the prompt. pick examples randomly unless start is set.\n",
        "# supports adding both random examples and a pre-defined set of examples\n",
        "# evaluate on examples filtered_data[evalset_start:evalset_end]\n",
        "def specify_steps_explain_reasoning(num_examples=5, start=0, evalset_start=2500, evalset_end=2510, examples=1, model=\"gpt-3.5-turbo\", temperature=0):\n",
        "  count, score_tags = 0, 0\n",
        "  for element in filtered_data[evalset_start:evalset_end]:\n",
        "      if start==0:\n",
        "        # Ensure num_examples does not exceed the length of filtered_data\n",
        "        num_samples = min(num_examples, len(filtered_data))\n",
        "\n",
        "        # Randomly pick num_samples elements from filtered_data\n",
        "        random_elements = random.sample(filtered_data, num_samples)\n",
        "      else:\n",
        "        random_elements = filtered_data[start:start+num_examples]\n",
        "\n",
        "      examples = \"\".join([\n",
        "          f\"Text: ```{element['description']}```\\nClass: {element.get('question', 'No question')}\\n\"\n",
        "          for element in random_elements\n",
        "      ])\n",
        "\n",
        "      # print(element)\n",
        "      ticket_text = element['description']\n",
        "\n",
        "      # Prompt – Classify Class\n",
        "      if examples:\n",
        "        prompt = f\"\"\"\n",
        "        Given the text delimited by triple backticks, perform the following actions:\n",
        "        1 - summary: Summarize what the user wants in 1-2 lines.\n",
        "        2 - summary_reasoning: Explain your reasoning for the summary.\n",
        "        3 - recommended_action: Recommend a next action based on the user' request.\n",
        "        4 - recommended_action_reasoning: Explain your reasoning for the recommended next action.\n",
        "        5 - is_wb: Determine if the request is related to the product or company 'Weights & Biases'.\n",
        "        6 - is_wb_reasoning: Explain your reasoning for detemining if the request is W&B related.\n",
        "        7 - class: Classify the into one of the following classes. Classes: {desired_tags}\n",
        "        8 - Output a json object that contains the following keys: summary, summary_reasoning, recommended_action, recommended_action_reasoning, is_wb, is_wb_reasoning, class\n",
        "\n",
        "        Here are some examples help you with the classification step.\n",
        "        {examples}\n",
        "\n",
        "        And here's the text ```{ticket_text}```.\n",
        "\n",
        "        Make sure the output is only a json object.\n",
        "        \"\"\"\n",
        "      else:\n",
        "        prompt = f\"\"\"\n",
        "        Given the text delimited by triple backticks, perform the following actions:\n",
        "        1 - summary: Summarize what the user wants in 1-2 lines.\n",
        "        2 - summary_reasoning: Explain your reasoning for the summary.\n",
        "        3 - recommended_action: Recommend a next action based on the user' request.\n",
        "        4 - recommended_action_reasoning: Explain your reasoning for the recommended next action.\n",
        "        5 - is_wb: Determine if the request is related to the product or company 'Weights & Biases'.\n",
        "        6 - is_wb_reasoning: Explain your reasoning for detemining if the request is W&B related.\n",
        "        7 - class: Classify the into one of the following classes. Classes: {desired_tags}\n",
        "        8 - Output a json object that contains the following keys: summary, summary_reasoning, recommended_action, recommended_action_reasoning, is_wb, is_wb_reasoning, class\n",
        "\n",
        "        Here's the text ```{ticket_text}```\"\"\"\n",
        "\n",
        "      response_json = get_completion(prompt)\n",
        "      # print(\"Prompt: \"+prompt)\n",
        "      print(\"Prediction: \")\n",
        "      response = json.loads(response_json)\n",
        "      print(\"Summary: \"+response['summary'])\n",
        "      print(\"Summary Reasoning: \"+response['summary_reasoning'])\n",
        "      print(\"Recommended Action: \"+response['recommended_action'])\n",
        "      print(\"Recommended Reasoning: \"+response['recommended_action_reasoning'])\n",
        "      print(\"Is W&B Related: \"+str(response['is_wb']))\n",
        "      print(\"Is W&B Related Reasoning: \"+response['is_wb_reasoning'])\n",
        "      print(\"Prediction: \"+response['class'])\n",
        "\n",
        "      if(response['class'] == element['question']):\n",
        "          score_tags += 1\n",
        "          print(\"Correct. Actual: \"+element['question'])\n",
        "      else:\n",
        "          print(\"Incorrect. Actual: \"+element['question'])\n",
        "      count += 1\n",
        "      print()\n",
        "\n",
        "  print(\"__________________\")\n",
        "  print(f\"Priority Accuracy: {score_tags/count}\")\n",
        "  return score_tags/count\n",
        "\n",
        "acc['specify_steps_explain_reasoning'] = specify_steps_explain_reasoning(5,1000, evalset_start_all, evalset_end_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AMtIb6uOuHh",
        "outputId": "3d9549cf-d3df-43ac-e417-3e65efd212e5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: \n",
            "Summary: User is requesting feedback on a rating of 10 out of 10 with no message provided.\n",
            "Summary Reasoning: The user is seeking feedback or clarification on a specific rating without any accompanying message.\n",
            "Recommended Action: Provide feedback or ask for clarification on the rating given.\n",
            "Recommended Reasoning: Engaging with the user to understand the reason behind the rating can help improve user experience or address any concerns.\n",
            "Is W&B Related: False\n",
            "Is W&B Related Reasoning: The request does not mention or relate to Weights & Biases.\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User is requesting assistance with a rating of 10 out of 10 and no message provided.\n",
            "Summary Reasoning: The user is seeking help or clarification regarding a specific rating without any additional context.\n",
            "Recommended Action: Request more information from the user to understand the issue or provide general assistance.\n",
            "Recommended Reasoning: Since the user did not provide any message along with the rating, it is important to gather more details to effectively address their request.\n",
            "Is W&B Related: False\n",
            "Is W&B Related Reasoning: The request does not seem to be related to Weights & Biases.\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: Invitation to The Future of Transportation event by The Wall Street Journal.\n",
            "Summary Reasoning: The text contains an invitation to an event discussing e-bikes and map apps.\n",
            "Recommended Action: Register for the event to learn about the future of transportation.\n",
            "Recommended Reasoning: The text provides registration links and details about the event.\n",
            "Is W&B Related: False\n",
            "Is W&B Related Reasoning: The text is not related to Weights & Biases.\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User is experiencing issues with loading their local environment and seeking assistance.\n",
            "Summary Reasoning: The text mentions loading issues with the local environment and suggests checking logs for error messages.\n",
            "Recommended Action: Recommend checking system logs for error messages and considering restarting the server with the specified environment variable.\n",
            "Recommended Reasoning: This action can help diagnose the loading issue and potentially resolve it.\n",
            "Is W&B Related: False\n",
            "Is W&B Related Reasoning: The request does not mention anything related to Weights & Biases.\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User sent a message with contact information and chat details.\n",
            "Summary Reasoning: The text contains details of a message sent by a user with contact information and chat details.\n",
            "Recommended Action: Follow up with the user regarding the message or chat.\n",
            "Recommended Reasoning: The recommended action is to engage with the user based on the information provided in the message.\n",
            "Is W&B Related: True\n",
            "Is W&B Related Reasoning: The text contains a URL related to Weights & Biases (W&B).\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User is having a conversation with Luis and mentions a URL related to Weights & Biases.\n",
            "Summary Reasoning: The text indicates a conversation with Luis and includes a URL related to Weights & Biases, suggesting a discussion or interaction with the platform.\n",
            "Recommended Action: Engage in the conversation with Luis regarding the URL provided.\n",
            "Recommended Reasoning: Since the user is mentioning a specific URL related to Weights & Biases, it would be beneficial to further explore the content of the conversation and the link shared.\n",
            "Is W&B Related: True\n",
            "Is W&B Related Reasoning: The text includes a URL that directly points to Weights & Biases, indicating a connection to the product or company.\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User is offering AWS re:Invent Attendees Data List 2022 for sale.\n",
            "Summary Reasoning: The text indicates the user is offering a data list for sale containing information about AWS re:Invent Attendees.\n",
            "Recommended Action: Consider responding if interested in purchasing the data list.\n",
            "Recommended Reasoning: The user is seeking potential buyers for the data list, so responding if interested could lead to more information or a potential purchase.\n",
            "Is W&B Related: False\n",
            "Is W&B Related Reasoning: The text does not mention Weights & Biases or any related products or services.\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User is facing authentication issues with their WandB account while setting up experiments.\n",
            "Summary Reasoning: The user mentions switching to a WandB account, creating a new project, and encountering an error related to authentication.\n",
            "Recommended Action: Contact WandB support for assistance with the authentication issue.\n",
            "Recommended Reasoning: Since the user has already tried running 'wandb login' multiple times without success, reaching out to WandB support for specialized help is the best course of action.\n",
            "Is W&B Related: True\n",
            "Is W&B Related Reasoning: The text explicitly mentions 'WandB account' and 'wandb login', indicating a clear connection to Weights & Biases.\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User rates the experience as 10 out of 10 and finds it convenient.\n",
            "Summary Reasoning: The user is providing feedback on their experience with a rating and positive comment.\n",
            "Recommended Action: Acknowledge the positive feedback and thank the user for their input.\n",
            "Recommended Reasoning: Acknowledging positive feedback can encourage further engagement and loyalty.\n",
            "Is W&B Related: False\n",
            "Is W&B Related Reasoning: The text does not mention Weights & Biases or any related terms.\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User wants to cancel their team plan named ct-unet and needs help with the process.\n",
            "Summary Reasoning: The user explicitly states their intention to cancel their team plan and requests assistance on how to do it.\n",
            "Recommended Action: Provide instructions on how to cancel the team plan named ct-unet.\n",
            "Recommended Reasoning: The user's request is clear and specific, asking for guidance on the cancellation process.\n",
            "Is W&B Related: True\n",
            "Is W&B Related Reasoning: The URL provided in the text belongs to Weights & Biases (W&B), indicating the request is related to the company.\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: Request for a record demo with no message provided.\n",
            "Summary Reasoning: The user is asking for a demo recording but did not provide any additional information.\n",
            "Recommended Action: Reach out to the user to gather more details about the specific requirements for the demo.\n",
            "Recommended Reasoning: Since the user did not provide any message, it is important to clarify the details of the demo they are requesting.\n",
            "Is W&B Related: False\n",
            "Is W&B Related Reasoning: The request does not mention Weights & Biases or any related keywords.\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: Announcement of product updates and solutions provided by Scaletorch in October.\n",
            "Summary Reasoning: The text highlights new product updates and solutions offered by Scaletorch in October.\n",
            "Recommended Action: Explore the new product updates and solutions provided by Scaletorch.\n",
            "Recommended Reasoning: The user should explore the new features and solutions to understand how they can benefit from them.\n",
            "Is W&B Related: False\n",
            "Is W&B Related Reasoning: The text does not mention Weights & Biases, so it is not related to the company.\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User is requesting to cancel the approval and change credit card for renewal of Seegene account.\n",
            "Summary Reasoning: The user explicitly mentions the issue with the credit card used for renewal and requests to cancel the approval and change the credit card.\n",
            "Recommended Action: Recommend contacting Seegene customer support via email to address the credit card renewal issue.\n",
            "Recommended Reasoning: Since the user mentions that the chat app is not working, suggesting to contact via email is a feasible alternative to resolve the credit card renewal problem.\n",
            "Is W&B Related: False\n",
            "Is W&B Related Reasoning: The request is related to Seegene account renewal, not Weights & Biases.\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User wants to know about the rating and comments for a specific channel.\n",
            "Summary Reasoning: The text mentions a rating of 7 out of 10 and the absence of comments for a specific channel.\n",
            "Recommended Action: Provide the user with the missing information about the comments for the channel.\n",
            "Recommended Reasoning: The user seems interested in knowing the comments associated with the rating, so providing this information would be helpful.\n",
            "Is W&B Related: False\n",
            "Is W&B Related Reasoning: The request does not seem to be related to Weights & Biases.\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: Request for a rating of 10 out of 10 with no message provided in the comments on an in-app channel.\n",
            "Summary Reasoning: The user is asking for feedback or a rating without leaving any additional comments on an in-app channel.\n",
            "Recommended Action: Acknowledge the rating and thank the user for their feedback.\n",
            "Recommended Reasoning: It is important to acknowledge and appreciate user feedback, even if no additional comments were provided.\n",
            "Is W&B Related: False\n",
            "Is W&B Related Reasoning: The request does not mention or relate to Weights & Biases.\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User wants to know about a rating of 9 out of 10 with no message provided in an in-app channel.\n",
            "Summary Reasoning: The text mentions a rating and lack of message in an in-app channel.\n",
            "Recommended Action: Request more information about the context or reason for the rating.\n",
            "Recommended Reasoning: To better understand the situation and provide appropriate assistance.\n",
            "Is W&B Related: False\n",
            "Is W&B Related Reasoning: The text does not mention anything related to Weights & Biases.\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User is inquiring about the price of Private Hosting for their autonomous driving startup company with 10-20 potential users.\n",
            "Summary Reasoning: The user specifically mentions their role, company, potential number of users, current usage of open source alternatives, personal interest in the product, and intention to recommend it to their team if the price is suitable.\n",
            "Recommended Action: Provide the user with information on Private Hosting pricing and potentially offer a demo or consultation to discuss their specific needs.\n",
            "Recommended Reasoning: Since the user is interested in the product and willing to recommend it to their team, providing pricing details and further assistance can potentially lead to a new customer.\n",
            "Is W&B Related: False\n",
            "Is W&B Related Reasoning: The request is not related to the product or company 'Weights & Biases'.\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: Request for feedback rating of 10 out of 10 with no message provided.\n",
            "Summary Reasoning: The user is asking for feedback rating without any additional message.\n",
            "Recommended Action: Provide a platform for the user to leave feedback or ask for more details on the rating.\n",
            "Recommended Reasoning: Since the user has given a rating without a message, it would be beneficial to provide a way for them to elaborate on their feedback.\n",
            "Is W&B Related: False\n",
            "Is W&B Related Reasoning: The request does not mention Weights & Biases or any related terms.\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User is looking for information on how to change their username on the platform.\n",
            "Summary Reasoning: The user mentions that they thought they could change their username but couldn't find the option, indicating a request for guidance on this specific issue.\n",
            "Recommended Action: Recommend the user to check the platform's settings or contact support for assistance in changing their username.\n",
            "Recommended Reasoning: Since the user couldn't find the option to change their username, suggesting them to look into the platform's settings or reach out to support would be the appropriate next step.\n",
            "Is W&B Related: False\n",
            "Is W&B Related Reasoning: The request is not related to Weights & Biases.\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User is providing feedback with a rating of 10 out of 10 but did not leave any message.\n",
            "Summary Reasoning: The text indicates a rating given by the user without any additional comments.\n",
            "Recommended Action: Reach out to the user to gather more detailed feedback or ask for specific areas of improvement.\n",
            "Recommended Reasoning: Since the user did not provide any message along with the rating, it would be beneficial to follow up and gather more information for better understanding.\n",
            "Is W&B Related: False\n",
            "Is W&B Related Reasoning: The text does not mention anything related to Weights & Biases.\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "__________________\n",
            "Priority Accuracy: 0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc['specify_steps_explain_reasoning_gpt4'] = specify_steps_explain_reasoning(5,1000, evalset_start_all, evalset_end_all, model=\"gpt-4\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5UD4mg5ZVxB",
        "outputId": "d130964d-816e-4d9e-aac6-cd4fa1fc44bd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: \n",
            "Summary: User is requesting assistance with a rating of 10 out of 10 and no message provided in the comments.\n",
            "Summary Reasoning: The user is seeking help or clarification regarding a specific rating without any additional comments.\n",
            "Recommended Action: Reach out to the user to gather more information or provide assistance based on the rating provided.\n",
            "Recommended Reasoning: Since the user did not provide any message along with the rating, it is important to follow up to understand the context or address any potential issues.\n",
            "Is W&B Related: False\n",
            "Is W&B Related Reasoning: The request does not seem to be related to Weights & Biases.\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User is requesting assistance with a rating of 10 out of 10 in-app with no message provided.\n",
            "Summary Reasoning: The user is seeking help or clarification regarding a specific rating given in-app.\n",
            "Recommended Action: Reach out to the user to gather more information about the rating and provide assistance if needed.\n",
            "Recommended Reasoning: It is important to follow up with the user to understand the context of the rating and address any concerns or questions they may have.\n",
            "Is W&B Related: False\n",
            "Is W&B Related Reasoning: The request does not mention or relate to Weights & Biases.\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: Invitation to The Future of Transportation event by The Wall Street Journal.\n",
            "Summary Reasoning: The text contains an invitation to an event discussing e-bikes and map apps by The Wall Street Journal.\n",
            "Recommended Action: Consider registering for the event to learn more about the future of transportation.\n",
            "Recommended Reasoning: The text provides registration links and details about the event, indicating interest in attending.\n",
            "Is W&B Related: False\n",
            "Is W&B Related Reasoning: The text is not related to Weights & Biases.\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User is experiencing issues with loading their local environment and seeking assistance.\n",
            "Summary Reasoning: The user mentions having a question related to their local environment loading issue and seeks guidance on resolving it.\n",
            "Recommended Action: Provide troubleshooting steps for resolving the local environment loading issue.\n",
            "Recommended Reasoning: The user is facing a technical issue with loading their local environment, so providing troubleshooting steps can help them resolve the issue.\n",
            "Is W&B Related: False\n",
            "Is W&B Related Reasoning: The text does not mention anything related to Weights & Biases.\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: Request for a record demo.\n",
            "Summary Reasoning: The user is asking for a demo to be recorded.\n",
            "Recommended Action: Provide information on how to access a recorded demo.\n",
            "Recommended Reasoning: The user is likely interested in viewing a demo and providing information on how to access it would be helpful.\n",
            "Is W&B Related: True\n",
            "Is W&B Related Reasoning: The URL provided in the text is related to Weights & Biases.\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User is having a conversation with Luis and mentions a URL related to Weights & Biases.\n",
            "Summary Reasoning: The text indicates a conversation with Luis and includes a URL related to Weights & Biases, suggesting a discussion or interaction with the platform.\n",
            "Recommended Action: Engage with Luis regarding the conversation or the URL provided.\n",
            "Recommended Reasoning: Since the text involves a conversation with Luis and a URL related to Weights & Biases, it would be beneficial to further engage with Luis to discuss the topic or explore the provided URL.\n",
            "Is W&B Related: True\n",
            "Is W&B Related Reasoning: The text includes a URL related to Weights & Biases, indicating a connection to the product or company.\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: Offer to acquire AWS re:Invent Attendees Data List 2022 with details and cost.\n",
            "Summary Reasoning: The user is offering a data list with specific details and cost to the recipient.\n",
            "Recommended Action: Respond to the email expressing interest or declining the offer.\n",
            "Recommended Reasoning: The recommended action is to either express interest in acquiring the data list or politely decline the offer based on the recipient's needs.\n",
            "Is W&B Related: False\n",
            "Is W&B Related Reasoning: The request is not related to Weights & Biases.\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User is facing an error with W&B API authentication despite logging in multiple times and needs assistance with their account setup.\n",
            "Summary Reasoning: The user mentions switching to a team's WandB account, creating a new project, and encountering a 401 error despite successful logins, indicating a potential account setup/authentication issue.\n",
            "Recommended Action: Recommend reaching out to the support team for assistance with the account setup/authentication issue.\n",
            "Recommended Reasoning: Since the user has already tried logging in multiple times without success, contacting the support team for personalized assistance with the account setup/authentication issue would be the best course of action.\n",
            "Is W&B Related: True\n",
            "Is W&B Related Reasoning: The text mentions 'WandB account' and 'W&B API', indicating a clear relation to Weights & Biases.\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User rates the experience as 10 out of 10 and finds it convenient through in-app channel.\n",
            "Summary Reasoning: The user is providing feedback on their experience with a rating and positive comment.\n",
            "Recommended Action: Acknowledge the positive feedback and thank the user for their input.\n",
            "Recommended Reasoning: Acknowledging positive feedback can encourage further engagement and loyalty.\n",
            "Is W&B Related: False\n",
            "Is W&B Related Reasoning: The text does not mention anything related to Weights & Biases.\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User wants to cancel their team plan named ct-unet and needs help with the process.\n",
            "Summary Reasoning: The user explicitly states their intention to cancel their team plan and requests assistance on how to do it.\n",
            "Recommended Action: Provide instructions on how to cancel the team plan named ct-unet.\n",
            "Recommended Reasoning: The user's request is clear and specific, asking for guidance on the cancellation process.\n",
            "Is W&B Related: True\n",
            "Is W&B Related Reasoning: The URL provided in the text is related to Weights & Biases (W&B), indicating a potential connection to the company.\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: Request for feedback on a rating with no message provided in-app.\n",
            "Summary Reasoning: The user is seeking feedback on a rating given with no accompanying message through an in-app channel.\n",
            "Recommended Action: Reach out to the user to gather more information about their rating and address any concerns they may have.\n",
            "Recommended Reasoning: Engaging with the user to understand the reason behind the rating can help improve user experience and address any issues they may have encountered.\n",
            "Is W&B Related: False\n",
            "Is W&B Related Reasoning: The request does not mention or relate to Weights & Biases.\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: Announcement of product updates and a solution to deep learning development problems by Scaletorch.\n",
            "Summary Reasoning: The text discusses new product updates and how Scaletorch can solve deep learning development problems.\n",
            "Recommended Action: Explore the new product updates and consider how Scaletorch can help solve deep learning development problems.\n",
            "Recommended Reasoning: The text provides links to learn more about the product updates and the solution offered by Scaletorch.\n",
            "Is W&B Related: False\n",
            "Is W&B Related Reasoning: The text does not mention Weights & Biases.\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: Request to cancel approval, change credit card for renewal, and reduce account seats for Seegene subscription.\n",
            "Summary Reasoning: The user is asking to cancel the approval, change the credit card for renewal, and reduce the number of account seats for their Seegene subscription due to legal issues and a team member leaving.\n",
            "Recommended Action: Contact the user to assist with canceling the approval, changing the credit card, and adjusting the account seats as requested.\n",
            "Recommended Reasoning: The user specifically asks for these actions to be taken, so reaching out to provide assistance and guidance would be the appropriate next step.\n",
            "Is W&B Related: False\n",
            "Is W&B Related Reasoning: The request is related to a Seegene subscription, not Weights & Biases.\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User wants to know the rating and comments for a specific channel interaction.\n",
            "Summary Reasoning: The user is inquiring about the rating and comments associated with a channel interaction.\n",
            "Recommended Action: Provide the user with the requested rating and comments for the specified channel interaction.\n",
            "Recommended Reasoning: The user's request can be fulfilled by retrieving the rating and comments for the mentioned channel interaction.\n",
            "Is W&B Related: False\n",
            "Is W&B Related Reasoning: The request does not pertain to Weights & Biases.\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User is requesting feedback on a rating of 10 out of 10 with no message provided.\n",
            "Summary Reasoning: The user is seeking feedback or acknowledgment for a rating given without any additional comments.\n",
            "Recommended Action: Provide a response acknowledging the rating and thanking the user for their feedback.\n",
            "Recommended Reasoning: Acknowledging the user's rating and expressing gratitude for their feedback can help maintain a positive interaction.\n",
            "Is W&B Related: False\n",
            "Is W&B Related Reasoning: The request does not mention or relate to the product or company 'Weights & Biases'.\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: Request for feedback or rating with no message provided.\n",
            "Summary Reasoning: The user is asking for feedback or rating without providing any specific message or context.\n",
            "Recommended Action: Prompt the user to provide more details or context for feedback.\n",
            "Recommended Reasoning: As the user did not provide any message, prompting for more details can help in understanding the feedback or rating request.\n",
            "Is W&B Related: False\n",
            "Is W&B Related Reasoning: The request does not seem to be related to Weights & Biases.\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User is inquiring about the price of Private Hosting for their autonomous driving startup company with 10-20 potential users.\n",
            "Summary Reasoning: The user specifically mentions their role, company, potential number of users, current usage of open source alternatives, personal interest in the product, and intention to recommend it to their team if the price is suitable.\n",
            "Recommended Action: Provide the user with information on the pricing of Private Hosting and any relevant details for their potential user range.\n",
            "Recommended Reasoning: The user has expressed interest in the product and is considering recommending it to their team, so providing pricing details and additional information can help them make an informed decision.\n",
            "Is W&B Related: False\n",
            "Is W&B Related Reasoning: The request does not mention 'Weights & Biases' or any related terms, indicating it is not related to the company.\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User is requesting assistance with a rating of 10 out of 10 and no message provided in the comments.\n",
            "Summary Reasoning: The user is seeking help or clarification regarding a specific rating without any additional comments.\n",
            "Recommended Action: Reach out to the user to gather more information or provide assistance based on the rating provided.\n",
            "Recommended Reasoning: Since the user did not provide any message along with the rating, it is important to follow up to understand the context or address any potential issues.\n",
            "Is W&B Related: False\n",
            "Is W&B Related Reasoning: The request does not seem to be related to Weights & Biases.\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User is looking for information on how to change their username on the platform.\n",
            "Summary Reasoning: The user explicitly mentions that they couldn't find the option to change their username and only found options to change other details.\n",
            "Recommended Action: Recommend the user to check the platform's settings or contact support for guidance on changing their username.\n",
            "Recommended Reasoning: Since the user couldn't find the option to change their username, suggesting them to explore the platform's settings or seek support would be helpful.\n",
            "Is W&B Related: False\n",
            "Is W&B Related Reasoning: The request is not related to Weights & Biases.\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: \n",
            "Summary: User is requesting assistance with a rating of 10 out of 10 in-app with no message provided.\n",
            "Summary Reasoning: The user is seeking help or clarification regarding a specific rating given in-app without leaving a message.\n",
            "Recommended Action: Reach out to the user to gather more information about their experience and provide any necessary assistance.\n",
            "Recommended Reasoning: It is important to follow up with the user to understand the reason behind the rating and address any potential issues or feedback they may have.\n",
            "Is W&B Related: False\n",
            "Is W&B Related Reasoning: The request does not mention or relate to Weights & Biases.\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "__________________\n",
            "Priority Accuracy: 0.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Chaining"
      ],
      "metadata": {
        "id": "Hq2EgDCCPnuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add examples to the prompt. pick examples randomly unless start is set.\n",
        "# supports adding both random examples and a pre-defined set of examples\n",
        "# evaluate on examples filtered_data[evalset_start:evalset_end]\n",
        "def prompt_chaining(num_examples=5, start=0, evalset_start=2500, evalset_end=2510, examples=1, model=\"gpt-3.5-turbo\", temperature=0):\n",
        "  count, score_tags = 0, 0\n",
        "  for element in filtered_data[evalset_start:evalset_end]:\n",
        "      if start==0:\n",
        "        # Ensure num_examples does not exceed the length of filtered_data\n",
        "        num_samples = min(num_examples, len(filtered_data))\n",
        "\n",
        "        # Randomly pick num_samples elements from filtered_data\n",
        "        random_elements = random.sample(filtered_data, num_samples)\n",
        "      else:\n",
        "        random_elements = filtered_data[start:start+num_examples]\n",
        "\n",
        "      examples = \"\".join([\n",
        "          f\"Text: ```{element['description']}```\\nClass: {element.get('question', 'No question')}\\n\"\n",
        "          for element in random_elements\n",
        "      ])\n",
        "\n",
        "      # print(element)\n",
        "      ticket_text = element['description']\n",
        "\n",
        "      # Prompt – Classify Class\n",
        "      prompt = f\"\"\"\n",
        "      Given the text delimited by triple backticks, perform the following actions:\n",
        "      1 - summary: Summarize what the user wants in 1-2 lines.\n",
        "      2 - summary_reasoning: Explain your reasoning for the summary.\n",
        "      3 - recommended_action: Recommend a next action based on the user' request.\n",
        "      4 - recommended_action_reasoning: Explain your reasoning for the recommended next action.\n",
        "      5 - is_wb: Determine if the request is related to the product or company 'Weights & Biases'.\n",
        "      6 - is_wb_reasoning: Explain your reasoning for detemining if the request is W&B related.\n",
        "      7 - Output a json object that contains the following keys: summary, summary_reasoning, recommended_action, recommended_action_reasoning, is_wb, is_wb_reasoning, class\n",
        "\n",
        "      And here's the text ```{ticket_text}```.\n",
        "\n",
        "      Make sure the output is only a json object.\n",
        "      \"\"\"\n",
        "\n",
        "      response_json = get_completion(prompt)\n",
        "      # print(\"Prompt: \"+prompt)\n",
        "      print(\"Prediction: \")\n",
        "      response = json.loads(response_json)\n",
        "      print(\"Summary: \"+response['summary'])\n",
        "      print(\"Summary Reasoning: \"+response['summary_reasoning'])\n",
        "      print(\"Recommended Action: \"+response['recommended_action'])\n",
        "      print(\"Recommended Reasoning: \"+response['recommended_action_reasoning'])\n",
        "      print(\"Is W&B Related: \"+str(response['is_wb']))\n",
        "      print(\"Is W&B Related Reasoning: \"+response['is_wb_reasoning'])\n",
        "\n",
        "\n",
        "      prompt_2 = f\"\"\"\n",
        "        Given the following info about a user request:\n",
        "        1 - text delimited by triple backticks: ```{ticket_text}```\n",
        "        2 - summary of what the user wants: {response['summary']}\n",
        "        3 - recommended next action based on the user' request: {response['recommended_action']}\n",
        "        4 - whether the request is related to the product or company 'Weights & Biases': {response['is_wb']}\n",
        "\n",
        "        Classify the text into one of the following classes. Classes: {desired_tags}\n",
        "\n",
        "        Here are some examples help you with the classification step.\n",
        "        {examples}\n",
        "\n",
        "        Only print the name of the class\"\"\"\n",
        "      response_class = get_completion(prompt_2)\n",
        "      print(\"Prediction: \"+response_class)\n",
        "\n",
        "      if(response_class == element['question']):\n",
        "          score_tags += 1\n",
        "          print(\"Correct. Actual: \"+element['question'])\n",
        "      else:\n",
        "          print(\"Incorrect. Actual: \"+element['question'])\n",
        "      count += 1\n",
        "      print()\n",
        "\n",
        "  print(\"__________________\")\n",
        "  print(f\"Priority Accuracy: {score_tags/count}\")\n",
        "  return score_tags/count\n",
        "\n",
        "# acc['prompt_chaining'] = prompt_chaining(5,1000, evalset_start_all, evalset_end_all)"
      ],
      "metadata": {
        "id": "0UgIuJb8PMJy"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(list(acc.items()), columns=['Prompt', 'Accuracy']).sort_values(by='Accuracy', ascending=True)\n",
        "def highlight_max_accuracy(data, color='mediumpurple'):\n",
        "    highlight = pd.DataFrame('', index=data.index, columns=data.columns)\n",
        "    max_accuracy = data['Accuracy'] == data['Accuracy'].max()\n",
        "    highlight[max_accuracy] = f'background-color: {color}'\n",
        "    return highlight\n",
        "\n",
        "# Applying the styling\n",
        "df = df.style.apply(highlight_max_accuracy, axis=None)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "Tdp-rWd9Kp-K",
        "outputId": "8630301a-c382-4140-e807-7e5bb56550c0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7b88cff3e6b0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_c0ef2_row11_col0, #T_c0ef2_row11_col1, #T_c0ef2_row12_col0, #T_c0ef2_row12_col1 {\n",
              "  background-color: mediumpurple;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_c0ef2\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_c0ef2_level0_col0\" class=\"col_heading level0 col0\" >Prompt</th>\n",
              "      <th id=\"T_c0ef2_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_c0ef2_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_c0ef2_row0_col0\" class=\"data row0 col0\" >basic_prompt</td>\n",
              "      <td id=\"T_c0ef2_row0_col1\" class=\"data row0 col1\" >0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c0ef2_level0_row1\" class=\"row_heading level0 row1\" >2</th>\n",
              "      <td id=\"T_c0ef2_row1_col0\" class=\"data row1 col0\" >add_5_random_examples</td>\n",
              "      <td id=\"T_c0ef2_row1_col1\" class=\"data row1 col1\" >0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c0ef2_level0_row2\" class=\"row_heading level0 row2\" >3</th>\n",
              "      <td id=\"T_c0ef2_row2_col0\" class=\"data row2 col0\" >add_5_examples_gpt4</td>\n",
              "      <td id=\"T_c0ef2_row2_col1\" class=\"data row2 col1\" >0.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c0ef2_level0_row3\" class=\"row_heading level0 row3\" >7</th>\n",
              "      <td id=\"T_c0ef2_row3_col0\" class=\"data row3 col0\" >add_20_examples_gpt4</td>\n",
              "      <td id=\"T_c0ef2_row3_col1\" class=\"data row3 col1\" >0.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c0ef2_level0_row4\" class=\"row_heading level0 row4\" >1</th>\n",
              "      <td id=\"T_c0ef2_row4_col0\" class=\"data row4 col0\" >add_5_examples</td>\n",
              "      <td id=\"T_c0ef2_row4_col1\" class=\"data row4 col1\" >0.350000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c0ef2_level0_row5\" class=\"row_heading level0 row5\" >8</th>\n",
              "      <td id=\"T_c0ef2_row5_col0\" class=\"data row5 col0\" >add_5_examples_describe_classes</td>\n",
              "      <td id=\"T_c0ef2_row5_col1\" class=\"data row5 col1\" >0.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c0ef2_level0_row6\" class=\"row_heading level0 row6\" >4</th>\n",
              "      <td id=\"T_c0ef2_row6_col0\" class=\"data row6 col0\" >add_20_random_examples</td>\n",
              "      <td id=\"T_c0ef2_row6_col1\" class=\"data row6 col1\" >0.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c0ef2_level0_row7\" class=\"row_heading level0 row7\" >11</th>\n",
              "      <td id=\"T_c0ef2_row7_col0\" class=\"data row7 col0\" >specify_steps_explain_reasoning</td>\n",
              "      <td id=\"T_c0ef2_row7_col1\" class=\"data row7 col1\" >0.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c0ef2_level0_row8\" class=\"row_heading level0 row8\" >12</th>\n",
              "      <td id=\"T_c0ef2_row8_col0\" class=\"data row8 col0\" >specify_steps_explain_reasoning_gpt4</td>\n",
              "      <td id=\"T_c0ef2_row8_col1\" class=\"data row8 col1\" >0.850000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c0ef2_level0_row9\" class=\"row_heading level0 row9\" >5</th>\n",
              "      <td id=\"T_c0ef2_row9_col0\" class=\"data row9 col0\" >add_20_examples</td>\n",
              "      <td id=\"T_c0ef2_row9_col1\" class=\"data row9 col1\" >0.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c0ef2_level0_row10\" class=\"row_heading level0 row10\" >9</th>\n",
              "      <td id=\"T_c0ef2_row10_col0\" class=\"data row10 col0\" >add_20_examples_describe_classes</td>\n",
              "      <td id=\"T_c0ef2_row10_col1\" class=\"data row10 col1\" >0.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c0ef2_level0_row11\" class=\"row_heading level0 row11\" >6</th>\n",
              "      <td id=\"T_c0ef2_row11_col0\" class=\"data row11 col0\" >add_20_examples_change_temp</td>\n",
              "      <td id=\"T_c0ef2_row11_col1\" class=\"data row11 col1\" >0.950000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c0ef2_level0_row12\" class=\"row_heading level0 row12\" >10</th>\n",
              "      <td id=\"T_c0ef2_row12_col0\" class=\"data row12 col0\" >specify_steps</td>\n",
              "      <td id=\"T_c0ef2_row12_col1\" class=\"data row12 col1\" >0.950000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next Steps\n",
        "- [ ] Improve performance, make eval set more expansive\n",
        "- [ ] Improve prompt chaining\n",
        "- [ ] Improve summarization + suggest next steps using docs + support tickets (try RAG)\n",
        "- [ ] How to evaluate the summary + next steps?\n",
        "- [ ] RLHF for summary + next steps"
      ],
      "metadata": {
        "id": "dLZ5W6faQdx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt  Accuracy\n",
        "               basic_prompt      0.00\n",
        "             add_5_examples      0.35\n",
        "      add_5_random_examples      0.10\n",
        "        add_5_examples_gpt4      0.30\n",
        "     add_20_random_examples      0.60\n",
        "            add_20_examples      0.90\n",
        "add_20_examples_change_temp      0.80\n",
        "       add_20_examples_gpt4      0.30\n",
        "           describe_classes      0.40"
      ],
      "metadata": {
        "id": "L5KitCtXGKNa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mv7M_t05_zb_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}