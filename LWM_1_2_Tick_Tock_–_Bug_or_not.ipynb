{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRl4kh8dHyMqBehLAErlev",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lavanyashukla/tick-tock/blob/main/LWM_1_2_Tick_Tock_%E2%80%93_Bug_or_not.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I. Setup"
      ],
      "metadata": {
        "id": "4i82ncQQx5NJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CtmahQp9cBCV"
      },
      "outputs": [],
      "source": [
        "!pip install openai -qq\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import random\n",
        "import csv\n",
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/lavanyashukla/tick-tock.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8IK_HCJbYcq",
        "outputId": "a964e096-6733-4b1e-a2d3-fc15900e3d17"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'tick-tock' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data\n",
        "input_file = 'tick-tock/data/support_tickets_clean.csv'\n",
        "\n",
        "# Open the CSV file for reading\n",
        "with open(input_file, mode='r', encoding='utf-8') as csvfile:\n",
        "    # Use csv.DictReader to read the CSV file into a dictionary\n",
        "    reader = csv.DictReader(csvfile)\n",
        "\n",
        "    # Initialize a list to store each row (as a dictionary)\n",
        "    data = []\n",
        "\n",
        "    # Iterate over the rows in the CSV file\n",
        "    for row in reader:\n",
        "        # Each row is a dictionary\n",
        "        data.append(row)\n",
        "\n",
        "# Now 'data' is a list of dictionaries, where each dictionary represents a row from the CSV\n",
        "print(data[0])\n",
        "print(len(data))"
      ],
      "metadata": {
        "id": "wFoGv3BHbjPT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a1bcea0-f0ce-47ad-f87c-c54a62d46852"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'description': '10 out of 10\\nComments: GrandMaster Training Data Platform\\nChannel: In-app', 'raw_subject': 'NPS Response', 'subject': 'NPS Response', 'priority': 'low', 'problem_id': '', 'tags': \"['nps_score', 'pendo_nps', 'personal', 'question']\", 'id': '36652', 'question': 'question', 'customer_type': \"['personal']\", 'customer_type_2': '', 'type': 'nps_score'}\n",
            "26590\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the 'question' values where available\n",
        "questions = [item.get('question', 'No question provided') for item in data]\n",
        "\n",
        "# Counting occurrences of each unique question (or the placeholder 'No question provided')\n",
        "question_counts = pd.Series(questions).value_counts().reset_index()\n",
        "question_counts.columns = ['Question', 'Count']\n",
        "\n",
        "# Display the counts in a table\n",
        "print(question_counts.to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JKtYM79mSNN",
        "outputId": "08ed74f6-19a5-42fc-bcf9-634accaec024"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Question  Count\n",
            "            question  21173\n",
            "                none   2389\n",
            "            type_bug   1536\n",
            "type_feature_request   1492\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty set to store unique questions\n",
        "unique_questions = set()\n",
        "\n",
        "# Iterate over each dictionary in the data list\n",
        "for item in data:\n",
        "    # Check if 'question' key exists in the dictionary\n",
        "    if 'question' in item:\n",
        "        # Add the question to the set (sets automatically handle uniqueness)\n",
        "        unique_questions.add(item['question'])\n",
        "\n",
        "# Convert the set back to a list to get a list of unique questions\n",
        "desired_tags = list(unique_questions)\n",
        "\n",
        "print(desired_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hqgl6ATzopAF",
        "outputId": "997009d7-4931-46b0-941f-d7c72e871478"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['type_feature_request', 'type_bug', 'none', 'question']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps:\n",
        "\n",
        "- Navigate to the \"Secrets\" pane from the left navigation bar\n",
        "- Add the OPENAI_API_KEY name and value\n",
        "- Turn on the toggle of \"Notebook access\""
      ],
      "metadata": {
        "id": "bPyiIh2wcHQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=userdata.get('OPENAI_API_KEY')\n",
        ")\n",
        "\n",
        "desired_tags = [\"question\", \"none\", \"type_bug\", \"type_feature_request\"]\n",
        "filtered_data = [row for row in data if row.get('question') in desired_tags]\n",
        "# filtered_data = data"
      ],
      "metadata": {
        "id": "niaOe97XbmAU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make a filtered dataset"
      ],
      "metadata": {
        "id": "PLe3r81Hx9Qp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's look at the data\n",
        "show_elem = 4\n",
        "print(json.dumps(data[show_elem], indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWg_Xuece4-_",
        "outputId": "783140a4-66d6-4914-ccbf-fc9e39d3b7bb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"description\": \"10 out of 10\\nComments: No message provided.\\nChannel: Email\",\n",
            "    \"raw_subject\": \"NPS Response\",\n",
            "    \"subject\": \"NPS Response\",\n",
            "    \"priority\": \"low\",\n",
            "    \"problem_id\": \"\",\n",
            "    \"tags\": \"['nps_score', 'pendo_nps', 'question']\",\n",
            "    \"id\": \"36643\",\n",
            "    \"question\": \"question\",\n",
            "    \"customer_type\": \"\",\n",
            "    \"customer_type_2\": \"\",\n",
            "    \"type\": \"nps_score\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count, score_priority, score_tags = 0, 0, 0\n",
        "acc = {}"
      ],
      "metadata": {
        "id": "yzY-EL9te5P1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Engineering"
      ],
      "metadata": {
        "id": "aIULZF-HyB8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(prompt, model=\"gpt-3.5-turbo\", temperature=0):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "evalset_start_all, evalset_end_all = 500, 520"
      ],
      "metadata": {
        "id": "f07dqQqCy8zl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OILxp2VKu6Nn",
        "outputId": "d6c25e85-a185-4343-e39a-e32aea96f7e3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# II. Basic Prompt + Evaluation"
      ],
      "metadata": {
        "id": "nj8AuHp1QIxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def basic_prompt(evalset_start=400, evalset_end=500):\n",
        "  count, score_tags = 0, 0\n",
        "  for element in filtered_data[evalset_start:evalset_end]:\n",
        "      # print(element)\n",
        "      ticket_text = element['description']\n",
        "\n",
        "      # Prompt – Classify Class\n",
        "      prompt = f\"\"\"\n",
        "      Classify the text delimited by triple backticks into one of the following classes.\n",
        "      Classes: {desired_tags}\n",
        "      Text: ```{ticket_text}```\n",
        "      Class: \"\"\"\n",
        "\n",
        "      response = get_completion(prompt)\n",
        "\n",
        "      print(\"Prediction: \"+response)\n",
        "\n",
        "      if(response == element['question']):\n",
        "          score_tags += 1\n",
        "          print(\"Correct. Actual: \"+element['question'])\n",
        "      else:\n",
        "          print(\"Incorrect. Actual: \"+element['question'])\n",
        "      count += 1\n",
        "      print()\n",
        "\n",
        "  print(\"__________________\")\n",
        "  print(f\"Priority Accuracy: {score_tags/count}\")\n",
        "  return score_tags/count\n",
        "acc['basic_prompt'] = basic_prompt(evalset_start_all, evalset_end_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnqL1a1EbHM3",
        "outputId": "50ebc920-66b6-4ebb-8b9e-3c4b966ae693"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: type_bug\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: type_feature_request\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: type_bug\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: type_feature_request\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "__________________\n",
            "Priority Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add Examples"
      ],
      "metadata": {
        "id": "EQdoc8KfQFfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add examples to the prompt. pick examples randomly unless start is set.\n",
        "# supports adding both random examples and a pre-defined set of examples\n",
        "# evaluate on examples filtered_data[evalset_start:evalset_end]\n",
        "def add_examples(num_examples=5, start=0, evalset_start=400, evalset_end=500, model=\"gpt-3.5-turbo\", temperature=0\n",
        "                 ):\n",
        "  count, score_tags = 0, 0\n",
        "  for element in filtered_data[evalset_start:evalset_end]:\n",
        "      if start==0:\n",
        "        # Ensure num_examples does not exceed the length of filtered_data\n",
        "        num_samples = min(num_examples, len(filtered_data))\n",
        "\n",
        "        # Randomly pick num_samples elements from filtered_data\n",
        "        random_elements = random.sample(filtered_data, num_samples)\n",
        "      else:\n",
        "        random_elements = filtered_data[start:start+num_examples]\n",
        "\n",
        "      examples = \"\".join([\n",
        "          f\"Text: ```{element['description']}```\\nClass: {element.get('question', 'No question')}\\n\"\n",
        "          for element in random_elements\n",
        "      ])\n",
        "\n",
        "      # print(element)\n",
        "      ticket_text = element['description']\n",
        "\n",
        "      # Prompt – Classify Class\n",
        "      prompt = f\"\"\"\n",
        "      Classify the text delimited by triple backticks into one of the following classes.\n",
        "      Classes: {desired_tags}\n",
        "\n",
        "      {examples}\n",
        "\n",
        "      Text: ```{ticket_text}```\n",
        "      Class: \"\"\"\n",
        "      response = get_completion(prompt, model=model, temperature=temperature)\n",
        "\n",
        "      # print(\"Prompt: \"+prompt)\n",
        "      print(\"Prediction: \"+response)\n",
        "\n",
        "      if(response == element['question']):\n",
        "          score_tags += 1\n",
        "          print(\"Correct. Actual: \"+element['question'])\n",
        "      else:\n",
        "          print(\"Incorrect. Actual: \"+element['question'])\n",
        "      count += 1\n",
        "      print()\n",
        "\n",
        "  print(\"__________________\")\n",
        "  print(f\"Priority Accuracy: {score_tags/count}\")\n",
        "  return score_tags/count\n",
        "acc['add_5_examples'] = add_examples(5, 1000, evalset_start_all, evalset_end_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVZp113EEid7",
        "outputId": "878e683b-64df-48e6-aca0-a171bc0f3ea7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: type_feature_request\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "__________________\n",
            "Priority Accuracy: 0.35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc['add_5_random_examples'] = add_examples(5, 0, evalset_start_all, evalset_end_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpBt0ZJkuBCZ",
        "outputId": "bcc4cd47-510f-4364-d1cc-39bbd3d37018"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: type_bug\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: type_bug\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "__________________\n",
            "Priority Accuracy: 0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc['add_5_examples_gpt4'] = add_examples(5, 1000, evalset_start_all, evalset_end_all, model=\"gpt-4\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLxdvalkEiGK",
        "outputId": "b0f1ff99-ed35-475a-a931-806f28cca390"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: type_bug\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: type_bug\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "__________________\n",
            "Priority Accuracy: 0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc['add_20_random_examples'] = add_examples(20, 0, evalset_start_all, evalset_end_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3m4_y9WE3Rx",
        "outputId": "830d02c6-71ad-4717-b2e6-73e70bb61eec"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: type_feature_request\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: type_bug\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: type_bug\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: type_feature_request\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: none\n",
            "Incorrect. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "__________________\n",
            "Priority Accuracy: 0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc['add_20_examples'] = add_examples(20,1000, evalset_start_all, evalset_end_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFPc1HDguiD3",
        "outputId": "e6397a1e-9287-436e-c5f4-e733dcc6c8a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n",
            "Prediction: question\n",
            "Correct. Actual: question\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Change Temperature"
      ],
      "metadata": {
        "id": "3sT4nUM_P_vQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc['add_20_examples_change_temp'] = add_examples(20, 1000, evalset_start_all, evalset_end_all, temperature=0.7)"
      ],
      "metadata": {
        "id": "3HuGEddGDqyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Change model to GPT-4"
      ],
      "metadata": {
        "id": "hudPh8kEP8aW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc['add_20_examples_gpt4'] = add_examples(20, 1000, evalset_start_all, evalset_end_all, model=\"gpt-4\")"
      ],
      "metadata": {
        "id": "ZWmLELPlEZp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Describe Classes"
      ],
      "metadata": {
        "id": "Zp1J33DqP2WN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add examples to the prompt. pick examples randomly unless start is set.\n",
        "# supports adding both random examples and a pre-defined set of examples\n",
        "# evaluate on examples filtered_data[evalset_start:evalset_end]\n",
        "def describe_classes(num_examples=5, start=0, evalset_start=2500, evalset_end=2510, model=\"gpt-3.5-turbo\", temperature=0):\n",
        "  count, score_tags = 0, 0\n",
        "  for element in filtered_data[evalset_start:evalset_end]:\n",
        "      if start==0:\n",
        "        # Ensure num_examples does not exceed the length of filtered_data\n",
        "        num_samples = min(num_examples, len(filtered_data))\n",
        "\n",
        "        # Randomly pick num_samples elements from filtered_data\n",
        "        random_elements = random.sample(filtered_data, num_samples)\n",
        "      else:\n",
        "        random_elements = filtered_data[start:start+num_examples]\n",
        "\n",
        "      examples = \"\".join([\n",
        "          f\"Text: ```{element['description']}```\\nClass: {element.get('question', 'No question')}\\n\"\n",
        "          for element in random_elements\n",
        "      ])\n",
        "\n",
        "      # print(element)\n",
        "      ticket_text = element['description']\n",
        "\n",
        "      # Prompt – Classify Class\n",
        "      prompt = f\"\"\"\n",
        "      Given the following description for each class:\n",
        "      type_feature_request: A request for a feature by a user of Weights & Biases.\n",
        "      type_bug: A bug report by a user of Weights & Biases.\n",
        "      question: If the request is not related to Weights & Biases; or doesn't fit the above 2 categories.\n",
        "\n",
        "      Classify the text delimited by triple backticks into one of the following classes.\n",
        "      Classes: {desired_tags}\n",
        "\n",
        "      {examples}\n",
        "\n",
        "      Text: ```{ticket_text}```\n",
        "      Class: \"\"\"\n",
        "      response = get_completion(prompt)\n",
        "\n",
        "      # print(\"Prompt: \"+prompt)\n",
        "      print(\"Prediction: \"+response)\n",
        "\n",
        "      if(response == element['question']):\n",
        "          score_tags += 1\n",
        "          print(\"Correct. Actual: \"+element['question'])\n",
        "      else:\n",
        "          print(\"Incorrect. Actual: \"+element['question'])\n",
        "      count += 1\n",
        "      print()\n",
        "\n",
        "  print(\"__________________\")\n",
        "  print(f\"Priority Accuracy: {score_tags/count}\")\n",
        "  print()\n",
        "  return score_tags/count\n",
        "acc['add_5_examples_describe_classes'] = describe_classes(5,1000, evalset_start_all, evalset_end_all)\n",
        "acc['add_20_examples_describe_classes'] = describe_classes(20,1000, evalset_start_all, evalset_end_all)"
      ],
      "metadata": {
        "id": "75FRFDQQJvl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Specify Steps"
      ],
      "metadata": {
        "id": "7fYxSsIcPtE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add examples to the prompt. pick examples randomly unless start is set.\n",
        "# supports adding both random examples and a pre-defined set of examples\n",
        "# evaluate on examples filtered_data[evalset_start:evalset_end]\n",
        "def specify_steps(num_examples=5, start=0, evalset_start=2500, evalset_end=2510, examples=1, model=\"gpt-3.5-turbo\", temperature=0):\n",
        "  count, score_tags = 0, 0\n",
        "  for element in filtered_data[evalset_start:evalset_end]:\n",
        "      if start==0:\n",
        "        # Ensure num_examples does not exceed the length of filtered_data\n",
        "        num_samples = min(num_examples, len(filtered_data))\n",
        "\n",
        "        # Randomly pick num_samples elements from filtered_data\n",
        "        random_elements = random.sample(filtered_data, num_samples)\n",
        "      else:\n",
        "        random_elements = filtered_data[start:start+num_examples]\n",
        "\n",
        "      examples = \"\".join([\n",
        "          f\"Text: ```{element['description']}```\\nClass: {element.get('question', 'No question')}\\n\"\n",
        "          for element in random_elements\n",
        "      ])\n",
        "\n",
        "      # print(element)\n",
        "      ticket_text = element['description']\n",
        "\n",
        "      # Prompt – Classify Class\n",
        "      if examples:\n",
        "        prompt = f\"\"\"\n",
        "        Given the text delimited by triple backticks, perform the following actions:\n",
        "        1 - Summarize what the user wants in 1-2 lines.\n",
        "        2 - Recommend a next action based on the user' request.\n",
        "        3 - Determine if the request is related to the product or company 'Weights & Biases'\n",
        "        4 - Classify the into one of the following classes. Classes: {desired_tags}\n",
        "        5 - Output a json object that contains the following keys: summary, recommended_action, is_wb, class\n",
        "\n",
        "        Here are some examples help you with the classification step.\n",
        "        {examples}\n",
        "\n",
        "        And here's the text ```{ticket_text}```\"\"\"\n",
        "      else:\n",
        "        prompt = f\"\"\"\n",
        "        Given the text delimited by triple backticks, perform the following actions:\n",
        "        1 - Summarize what the user wants in 1-2 lines.\n",
        "        2 - Recommend a next action based on the user' request.\n",
        "        3 - Determine if the request is related to the product or company 'Weights & Biases'\n",
        "        4 - Classify the into one of the following classes. Classes: {desired_tags}\n",
        "        5 - Output a json object that contains the following keys: summary, recommended_action, is_wb, class\n",
        "\n",
        "        Here's the text ```{ticket_text}```\"\"\"\n",
        "\n",
        "      response_json = get_completion(prompt)\n",
        "      # print(\"Prompt: \"+prompt)\n",
        "      print(\"Prediction: \")\n",
        "      response = json.loads(response_json)\n",
        "      print(\"Summary: \"+response['summary'])\n",
        "      print(\"Recommended Action: \"+response['recommended_action'])\n",
        "      print(\"Is W&B Related: \"+str(response['is_wb']))\n",
        "      print(\"Prediction: \"+response['class'])\n",
        "\n",
        "      if(response['class'] == element['question']):\n",
        "          score_tags += 1\n",
        "          print(\"Correct. Actual: \"+element['question'])\n",
        "      else:\n",
        "          print(\"Incorrect. Actual: \"+element['question'])\n",
        "      count += 1\n",
        "      print()\n",
        "\n",
        "  print(\"__________________\")\n",
        "  print(f\"Priority Accuracy: {score_tags/count}\")\n",
        "  return score_tags/count\n",
        "\n",
        "acc['specify_steps'] = specify_steps(5,1000, evalset_start_all, evalset_end_all)"
      ],
      "metadata": {
        "id": "_YXwWv3ZIF2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# acc['specify_steps_no_examples'] = specify_steps(5,1000, evalset_start_all, evalset_end_all, examples=0)"
      ],
      "metadata": {
        "id": "q0b1FJ7HMzLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explain LLM Reasoning Behind Steps"
      ],
      "metadata": {
        "id": "JuUlt_atPYnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add examples to the prompt. pick examples randomly unless start is set.\n",
        "# supports adding both random examples and a pre-defined set of examples\n",
        "# evaluate on examples filtered_data[evalset_start:evalset_end]\n",
        "def specify_steps_explain_reasoning(num_examples=5, start=0, evalset_start=2500, evalset_end=2510, examples=1, model=\"gpt-3.5-turbo\", temperature=0):\n",
        "  count, score_tags = 0, 0\n",
        "  for element in filtered_data[evalset_start:evalset_end]:\n",
        "      if start==0:\n",
        "        # Ensure num_examples does not exceed the length of filtered_data\n",
        "        num_samples = min(num_examples, len(filtered_data))\n",
        "\n",
        "        # Randomly pick num_samples elements from filtered_data\n",
        "        random_elements = random.sample(filtered_data, num_samples)\n",
        "      else:\n",
        "        random_elements = filtered_data[start:start+num_examples]\n",
        "\n",
        "      examples = \"\".join([\n",
        "          f\"Text: ```{element['description']}```\\nClass: {element.get('question', 'No question')}\\n\"\n",
        "          for element in random_elements\n",
        "      ])\n",
        "\n",
        "      # print(element)\n",
        "      ticket_text = element['description']\n",
        "\n",
        "      # Prompt – Classify Class\n",
        "      if examples:\n",
        "        prompt = f\"\"\"\n",
        "        Given the text delimited by triple backticks, perform the following actions:\n",
        "        1 - summary: Summarize what the user wants in 1-2 lines.\n",
        "        2 - summary_reasoning: Explain your reasoning for the summary.\n",
        "        3 - recommended_action: Recommend a next action based on the user' request.\n",
        "        4 - recommended_action_reasoning: Explain your reasoning for the recommended next action.\n",
        "        5 - is_wb: Determine if the request is related to the product or company 'Weights & Biases'.\n",
        "        6 - is_wb_reasoning: Explain your reasoning for detemining if the request is W&B related.\n",
        "        7 - class: Classify the into one of the following classes. Classes: {desired_tags}\n",
        "        8 - Output a json object that contains the following keys: summary, summary_reasoning, recommended_action, recommended_action_reasoning, is_wb, is_wb_reasoning, class\n",
        "\n",
        "        Here are some examples help you with the classification step.\n",
        "        {examples}\n",
        "\n",
        "        And here's the text ```{ticket_text}```.\n",
        "\n",
        "        Make sure the output is only a json object.\n",
        "        \"\"\"\n",
        "      else:\n",
        "        prompt = f\"\"\"\n",
        "        Given the text delimited by triple backticks, perform the following actions:\n",
        "        1 - summary: Summarize what the user wants in 1-2 lines.\n",
        "        2 - summary_reasoning: Explain your reasoning for the summary.\n",
        "        3 - recommended_action: Recommend a next action based on the user' request.\n",
        "        4 - recommended_action_reasoning: Explain your reasoning for the recommended next action.\n",
        "        5 - is_wb: Determine if the request is related to the product or company 'Weights & Biases'.\n",
        "        6 - is_wb_reasoning: Explain your reasoning for detemining if the request is W&B related.\n",
        "        7 - class: Classify the into one of the following classes. Classes: {desired_tags}\n",
        "        8 - Output a json object that contains the following keys: summary, summary_reasoning, recommended_action, recommended_action_reasoning, is_wb, is_wb_reasoning, class\n",
        "\n",
        "        Here's the text ```{ticket_text}```\"\"\"\n",
        "\n",
        "      response_json = get_completion(prompt)\n",
        "      # print(\"Prompt: \"+prompt)\n",
        "      print(\"Prediction: \")\n",
        "      response = json.loads(response_json)\n",
        "      print(\"Summary: \"+response['summary'])\n",
        "      print(\"Summary Reasoning: \"+response['summary_reasoning'])\n",
        "      print(\"Recommended Action: \"+response['recommended_action'])\n",
        "      print(\"Recommended Reasoning: \"+response['recommended_action_reasoning'])\n",
        "      print(\"Is W&B Related: \"+str(response['is_wb']))\n",
        "      print(\"Is W&B Related Reasoning: \"+response['is_wb_reasoning'])\n",
        "      print(\"Prediction: \"+response['class'])\n",
        "\n",
        "      if(response['class'] == element['question']):\n",
        "          score_tags += 1\n",
        "          print(\"Correct. Actual: \"+element['question'])\n",
        "      else:\n",
        "          print(\"Incorrect. Actual: \"+element['question'])\n",
        "      count += 1\n",
        "      print()\n",
        "\n",
        "  print(\"__________________\")\n",
        "  print(f\"Priority Accuracy: {score_tags/count}\")\n",
        "  return score_tags/count\n",
        "\n",
        "acc['specify_steps_explain_reasoning'] = specify_steps_explain_reasoning(5,1000, evalset_start_all, evalset_end_all)"
      ],
      "metadata": {
        "id": "3AMtIb6uOuHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc['specify_steps_explain_reasoning_gpt4'] = specify_steps_explain_reasoning(5,1000, evalset_start_all, evalset_end_all, model=\"gpt-4\")"
      ],
      "metadata": {
        "id": "y5UD4mg5ZVxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Chaining"
      ],
      "metadata": {
        "id": "Hq2EgDCCPnuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add examples to the prompt. pick examples randomly unless start is set.\n",
        "# supports adding both random examples and a pre-defined set of examples\n",
        "# evaluate on examples filtered_data[evalset_start:evalset_end]\n",
        "def prompt_chaining(num_examples=5, start=0, evalset_start=2500, evalset_end=2510, examples=1, model=\"gpt-3.5-turbo\", temperature=0):\n",
        "  count, score_tags = 0, 0\n",
        "  for element in filtered_data[evalset_start:evalset_end]:\n",
        "      if start==0:\n",
        "        # Ensure num_examples does not exceed the length of filtered_data\n",
        "        num_samples = min(num_examples, len(filtered_data))\n",
        "\n",
        "        # Randomly pick num_samples elements from filtered_data\n",
        "        random_elements = random.sample(filtered_data, num_samples)\n",
        "      else:\n",
        "        random_elements = filtered_data[start:start+num_examples]\n",
        "\n",
        "      examples = \"\".join([\n",
        "          f\"Text: ```{element['description']}```\\nClass: {element.get('question', 'No question')}\\n\"\n",
        "          for element in random_elements\n",
        "      ])\n",
        "\n",
        "      # print(element)\n",
        "      ticket_text = element['description']\n",
        "\n",
        "      # Prompt – Classify Class\n",
        "      prompt = f\"\"\"\n",
        "      Given the text delimited by triple backticks, perform the following actions:\n",
        "      1 - summary: Summarize what the user wants in 1-2 lines.\n",
        "      2 - summary_reasoning: Explain your reasoning for the summary.\n",
        "      3 - recommended_action: Recommend a next action based on the user' request.\n",
        "      4 - recommended_action_reasoning: Explain your reasoning for the recommended next action.\n",
        "      5 - is_wb: Determine if the request is related to the product or company 'Weights & Biases'.\n",
        "      6 - is_wb_reasoning: Explain your reasoning for detemining if the request is W&B related.\n",
        "      7 - Output a json object that contains the following keys: summary, summary_reasoning, recommended_action, recommended_action_reasoning, is_wb, is_wb_reasoning, class\n",
        "\n",
        "      And here's the text ```{ticket_text}```.\n",
        "\n",
        "      Make sure the output is only a json object.\n",
        "      \"\"\"\n",
        "\n",
        "      response_json = get_completion(prompt)\n",
        "      # print(\"Prompt: \"+prompt)\n",
        "      print(\"Prediction: \")\n",
        "      response = json.loads(response_json)\n",
        "      print(\"Summary: \"+response['summary'])\n",
        "      print(\"Summary Reasoning: \"+response['summary_reasoning'])\n",
        "      print(\"Recommended Action: \"+response['recommended_action'])\n",
        "      print(\"Recommended Reasoning: \"+response['recommended_action_reasoning'])\n",
        "      print(\"Is W&B Related: \"+str(response['is_wb']))\n",
        "      print(\"Is W&B Related Reasoning: \"+response['is_wb_reasoning'])\n",
        "\n",
        "\n",
        "      prompt_2 = f\"\"\"\n",
        "        Given the following info about a user request:\n",
        "        1 - text delimited by triple backticks: ```{ticket_text}```\n",
        "        2 - summary of what the user wants: {response['summary']}\n",
        "        3 - recommended next action based on the user' request: {response['recommended_action']}\n",
        "        4 - whether the request is related to the product or company 'Weights & Biases': {response['is_wb']}\n",
        "\n",
        "        Classify the text into one of the following classes. Classes: {desired_tags}\n",
        "\n",
        "        Here are some examples help you with the classification step.\n",
        "        {examples}\n",
        "\n",
        "        Only print the name of the class\"\"\"\n",
        "      response_class = get_completion(prompt_2)\n",
        "      print(\"Prediction: \"+response_class)\n",
        "\n",
        "      if(response_class == element['question']):\n",
        "          score_tags += 1\n",
        "          print(\"Correct. Actual: \"+element['question'])\n",
        "      else:\n",
        "          print(\"Incorrect. Actual: \"+element['question'])\n",
        "      count += 1\n",
        "      print()\n",
        "\n",
        "  print(\"__________________\")\n",
        "  print(f\"Priority Accuracy: {score_tags/count}\")\n",
        "  return score_tags/count\n",
        "\n",
        "# acc['prompt_chaining'] = prompt_chaining(5,1000, evalset_start_all, evalset_end_all)"
      ],
      "metadata": {
        "id": "0UgIuJb8PMJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(list(acc.items()), columns=['Prompt', 'Accuracy']).sort_values(by='Accuracy', ascending=True)\n",
        "df.style.highlight_max(color = 'mediumpurple')"
      ],
      "metadata": {
        "id": "Tdp-rWd9Kp-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next Steps\n",
        "- [ ] Improve performance, make eval set more expansive\n",
        "- [ ] Improve prompt chaining\n",
        "- [ ] Improve summarization + suggest next steps using docs + support tickets (try RAG)\n",
        "- [ ] How to evaluate the summary + next steps?\n",
        "- [ ] RLHF for summary + next steps"
      ],
      "metadata": {
        "id": "dLZ5W6faQdx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt  Accuracy\n",
        "               basic_prompt      0.00\n",
        "             add_5_examples      0.35\n",
        "      add_5_random_examples      0.10\n",
        "        add_5_examples_gpt4      0.30\n",
        "     add_20_random_examples      0.60\n",
        "            add_20_examples      0.90\n",
        "add_20_examples_change_temp      0.80\n",
        "       add_20_examples_gpt4      0.30\n",
        "           describe_classes      0.40"
      ],
      "metadata": {
        "id": "L5KitCtXGKNa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mv7M_t05_zb_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}