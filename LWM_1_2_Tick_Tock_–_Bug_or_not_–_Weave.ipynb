{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGSkBP7JokllQtlVM61ogm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d7af64ab372f48889f94877f79c0f956": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_b8092122d7704f9a9b457aa1539b2ff7",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[32m‚†¶\u001b[0m Evaluating...\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">‚†¶</span> Evaluating...\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "b8092122d7704f9a9b457aa1539b2ff7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e7f3517786245ff8093dbebfdb9c289": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_e4c513dc0cfd4879960646fb6ab531bf",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[32m‚†∏\u001b[0m Summarizing...\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">‚†∏</span> Summarizing...\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "e4c513dc0cfd4879960646fb6ab531bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lavanyashukla/tick-tock/blob/main/LWM_1_2_Tick_Tock_%E2%80%93_Bug_or_not_%E2%80%93_Weave.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I. Setup"
      ],
      "metadata": {
        "id": "4i82ncQQx5NJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CtmahQp9cBCV"
      },
      "outputs": [],
      "source": [
        "!pip install openai -qq\n",
        "!pip install weave -qq\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "from collections import Counter\n",
        "import weave\n",
        "from weave import weaveflow\n",
        "import pandas as pd\n",
        "import asyncio\n",
        "import random\n",
        "import csv\n",
        "import json\n",
        "import os\n",
        "from weave.weaveflow import Model, Evaluation, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHEd_dUcHVqT",
        "outputId": "742ed2c2-1c5e-48fc-c154-95f4db62082a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weave.init('tick-tock')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hYu7R7jHT9V",
        "outputId": "9e040872-fc1d-409c-9c8e-0d51dfdd2f8c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GraphClientTraceWithArtifactStorage()"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/lavanyashukla/tick-tock.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8IK_HCJbYcq",
        "outputId": "66ec32c4-6159-4a01-8b7f-fd733324d281"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'tick-tock'...\n",
            "remote: Enumerating objects: 2555, done.\u001b[K\n",
            "remote: Counting objects: 100% (2555/2555), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2539/2539), done.\u001b[K\n",
            "remote: Total 2555 (delta 34), reused 2524 (delta 12), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (2555/2555), 12.29 MiB | 7.10 MiB/s, done.\n",
            "Resolving deltas: 100% (34/34), done.\n",
            "Updating files: 100% (2493/2493), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data\n",
        "input_file = 'tick-tock/data/support_tickets_clean.csv'\n",
        "\n",
        "# Open the CSV file for reading\n",
        "with open(input_file, mode='r', encoding='utf-8') as csvfile:\n",
        "    # Use csv.DictReader to read the CSV file into a dictionary\n",
        "    reader = csv.DictReader(csvfile)\n",
        "\n",
        "    # Initialize a list to store each row (as a dictionary)\n",
        "    data = []\n",
        "\n",
        "    # Iterate over the rows in the CSV file\n",
        "    for row in reader:\n",
        "        # Each row is a dictionary\n",
        "        data.append(row)\n",
        "\n",
        "# Now 'data' is a list of dictionaries, where each dictionary represents a row from the CSV\n",
        "print(data[0])\n",
        "print(len(data))\n",
        "\n",
        "dataset = weaveflow.Dataset(data)\n",
        "dataset_ref = weave.publish(dataset, 'tick-tock-dataset')\n",
        "\n",
        "dataset = weaveflow.Dataset(data[:2])\n",
        "dataset_ref = weave.publish(dataset, 'tick-tock-dataset-micro')"
      ],
      "metadata": {
        "id": "wFoGv3BHbjPT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a053217-3de8-4ce9-d36a-1ebb18704610"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'description': '10 out of 10\\nComments: GrandMaster Training Data Platform\\nChannel: In-app', 'raw_subject': 'NPS Response', 'subject': 'NPS Response', 'priority': 'low', 'problem_id': '', 'tags': \"['nps_score', 'pendo_nps', 'personal', 'question']\", 'id': '36652', 'question': 'question', 'customer_type': \"['personal']\", 'customer_type_2': '', 'type': 'nps_score'}\n",
            "26590\n",
            "Published Dataset to https://wandb.ai/llm-play/tick-tock/weaveflow/objects/tick-tock-dataset/versions/450adad76f7af89f0ac3\n",
            "Published Dataset to https://wandb.ai/llm-play/tick-tock/weaveflow/objects/tick-tock-dataset-micro/versions/58d51bb88f89c2497563\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPEer7odGVNx",
        "outputId": "7248d35e-aff2-4310-a708-f462febfdb37"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlavanyashukla\u001b[0m (\u001b[33mllm-play\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Ep2tYEaGOUe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the 'question' values where available\n",
        "questions = [item.get('question', 'No question provided') for item in data]\n",
        "\n",
        "# Counting occurrences of each unique question (or the placeholder 'No question provided')\n",
        "question_counts = pd.Series(questions).value_counts().reset_index()\n",
        "question_counts.columns = ['Question', 'Count']\n",
        "\n",
        "# Display the counts in a table\n",
        "print(question_counts.to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JKtYM79mSNN",
        "outputId": "da45297a-50c7-4aca-eff8-d34af66e1c4c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Question  Count\n",
            "            question  21173\n",
            "                none   2389\n",
            "            type_bug   1536\n",
            "type_feature_request   1492\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty set to store unique questions\n",
        "unique_questions = set()\n",
        "\n",
        "# Iterate over each dictionary in the data list\n",
        "for item in data:\n",
        "    # Check if 'question' key exists in the dictionary\n",
        "    if 'question' in item:\n",
        "        # Add the question to the set (sets automatically handle uniqueness)\n",
        "        unique_questions.add(item['question'])\n",
        "\n",
        "# Convert the set back to a list to get a list of unique questions\n",
        "desired_tags = list(unique_questions)\n",
        "\n",
        "print(desired_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hqgl6ATzopAF",
        "outputId": "93cb347f-9461-453d-e278-5e939f7162fb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['type_bug', 'type_feature_request', 'none', 'question']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps:\n",
        "\n",
        "- Navigate to the \"Secrets\" pane from the left navigation bar\n",
        "- Add the OPENAI_API_KEY name and value\n",
        "- Turn on the toggle of \"Notebook access\""
      ],
      "metadata": {
        "id": "bPyiIh2wcHQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "desired_tags = [\"question\", \"none\", \"type_bug\", \"type_feature_request\"]\n",
        "filtered_data = [row for row in data if row.get('question') in desired_tags]\n",
        "filtered_data = data"
      ],
      "metadata": {
        "id": "niaOe97XbmAU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make a filtered dataset"
      ],
      "metadata": {
        "id": "PLe3r81Hx9Qp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's look at the data\n",
        "show_elem = 4\n",
        "print(json.dumps(data[show_elem], indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWg_Xuece4-_",
        "outputId": "dbf48937-1732-4f55-dc18-21c03cebf913"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"description\": \"10 out of 10\\nComments: No message provided.\\nChannel: Email\",\n",
            "    \"raw_subject\": \"NPS Response\",\n",
            "    \"subject\": \"NPS Response\",\n",
            "    \"priority\": \"low\",\n",
            "    \"problem_id\": \"\",\n",
            "    \"tags\": \"['nps_score', 'pendo_nps', 'question']\",\n",
            "    \"id\": \"36643\",\n",
            "    \"question\": \"question\",\n",
            "    \"customer_type\": \"\",\n",
            "    \"customer_type_2\": \"\",\n",
            "    \"type\": \"nps_score\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count, score_priority, score_tags = 0, 0, 0\n",
        "acc = {}"
      ],
      "metadata": {
        "id": "yzY-EL9te5P1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Engineering"
      ],
      "metadata": {
        "id": "aIULZF-HyB8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@weave.type()\n",
        "class SupportTicketClassificationModel(Model):\n",
        "    model_name: str = \"gpt-3.5-turbo\"\n",
        "    system_prompt = \" \"\n",
        "\n",
        "    @weave.op()\n",
        "    async def predict(self, prompt, temperature=0) -> str:\n",
        "        client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        response = client.chat.completions.create(\n",
        "            model=self.model_name,\n",
        "            messages=messages,\n",
        "            temperature=temperature,\n",
        "        )\n",
        "        extracted = response.choices[0].message.content\n",
        "        return extracted\n",
        "\n",
        "\n",
        "evalset_start_all, evalset_end_all = 500, 520"
      ],
      "metadata": {
        "id": "f07dqQqCy8zl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OILxp2VKu6Nn",
        "outputId": "6a257238-d5ae-4d92-ce93-c01a60c8cc41"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# II. Basic Prompt + Evaluation"
      ],
      "metadata": {
        "id": "nj8AuHp1QIxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weave.init('tick-tock')\n",
        "element = filtered_data[100]\n",
        "ticket_text = element['description']\n",
        "\n",
        "# Prompt ‚Äì Classify Class\n",
        "prompt = f\"\"\"\n",
        "Classify the text delimited by triple backticks into one of the following classes.\n",
        "Classes: {desired_tags}\n",
        "Text: ```{ticket_text}```\n",
        "Class: \"\"\"\n",
        "\n",
        "model = SupportTicketClassificationModel(\"gpt-3.5-turbo\")\n",
        "response = asyncio.run(model.predict(prompt))\n",
        "\n",
        "# print(\"Prediction: \"+response)\n",
        "\n",
        "# if(response == element['question']):\n",
        "#     score_tags += 1\n",
        "#     print(\"Correct. Actual: \"+element['question'])\n",
        "# else:\n",
        "#     print(\"Incorrect. Actual: \"+element['question'])\n",
        "# count += 1\n",
        "# print()\n",
        "\n",
        "evaluation = Evaluation(\n",
        "    dataset, scores=[matchy_matchy, matchy_matchy2], example_to_model_input=example_to_model_input\n",
        ")\n",
        "# asyncio.run(evaluation.evaluate(model))\n",
        "await evaluation.evaluate(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328,
          "referenced_widgets": [
            "d7af64ab372f48889f94877f79c0f956",
            "b8092122d7704f9a9b457aa1539b2ff7",
            "0e7f3517786245ff8093dbebfdb9c289",
            "e4c513dc0cfd4879960646fb6ab531bf"
          ]
        },
        "id": "u7BuwxEWMZl0",
        "outputId": "f6bcde77-5a7c-46c0-da8a-4f547f6d051b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-26 (_process_batches):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace_server/async_batch_processor.py\", line 61, in _process_batches\n",
            "    self.processor_fn(current_batch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/weave/trace_server/remote_http_trace_server.py\", line 54, in _flush_calls\n",
            "    r.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 1021, in raise_for_status\n",
            "    raise HTTPError(http_error_msg, response=self)\n",
            "requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://trace.wandb.ai/call/upsert_batch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üç© View call: https://wandb.ai/llm-play/tick-tock/weaveflow/calls/6075308e-221d-4ca8-85d6-5f4470e66481?convertToPeek=true\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d7af64ab372f48889f94877f79c0f956"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0e7f3517786245ff8093dbebfdb9c289"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üç© View call: https://wandb.ai/llm-play/tick-tock/weaveflow/calls/432534e4-42be-47f5-96de-1f0c616f8391?convertToPeek=true\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'matchy_matchy': {'correct': {'true_count': 0, 'true_fraction': 0.0}}}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest_asyncio\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTHjVU4BO-K2",
        "outputId": "42ff506c-2018-4c4d-ae3f-4411d0604eb7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@weave.op()\n",
        "def matchy_matchy(example: dict, prediction: str) -> dict:\n",
        "      return {'correct': example['question'] == prediction}\n",
        "\n",
        "@weave.op()\n",
        "def matchy_matchy2(example: dict, prediction: str) -> dict:\n",
        "      return {'correct': example['question'] == prediction}\n",
        "\n",
        "@weave.op()\n",
        "def example_to_model_input(example: dict) -> str:\n",
        "    return example[\"description\"]"
      ],
      "metadata": {
        "id": "UyDnXt_LSYFC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def basic_prompt(evalset_start=400, evalset_end=500):\n",
        "  count, score_tags = 0, 0\n",
        "  for element in filtered_data[evalset_start:evalset_end]:\n",
        "      # print(element)\n",
        "      ticket_text = element['description']\n",
        "\n",
        "      # Prompt ‚Äì Classify Class\n",
        "      prompt = f\"\"\"\n",
        "      Classify the text delimited by triple backticks into one of the following classes.\n",
        "      Classes: {desired_tags}\n",
        "      Text: ```{ticket_text}```\n",
        "      Class: \"\"\"\n",
        "\n",
        "      response = get_completion(prompt)\n",
        "      model = SupportTicketClassificationModel()\n",
        "      print(asyncio.run(model.predict(prompt)))\n",
        "\n",
        "      print(\"Prediction: \"+response)\n",
        "\n",
        "      if(response == element['question']):\n",
        "          score_tags += 1\n",
        "          print(\"Correct. Actual: \"+element['question'])\n",
        "      else:\n",
        "          print(\"Incorrect. Actual: \"+element['question'])\n",
        "      count += 1\n",
        "      print()\n",
        "\n",
        "  print(\"__________________\")\n",
        "  print(f\"Priority Accuracy: {score_tags/count}\")\n",
        "  return score_tags/count\n",
        "acc['basic_prompt'] = basic_prompt(evalset_start_all, evalset_end_all)"
      ],
      "metadata": {
        "id": "GnqL1a1EbHM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add Examples"
      ],
      "metadata": {
        "id": "EQdoc8KfQFfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add examples to the prompt. pick examples randomly unless start is set.\n",
        "# supports adding both random examples and a pre-defined set of examples\n",
        "# evaluate on examples filtered_data[evalset_start:evalset_end]\n",
        "def add_examples(num_examples=5, start=0, evalset_start=400, evalset_end=500, model=\"gpt-3.5-turbo\", temperature=0\n",
        "                 ):\n",
        "  count, score_tags = 0, 0\n",
        "  for element in filtered_data[evalset_start:evalset_end]:\n",
        "      if start==0:\n",
        "        # Ensure num_examples does not exceed the length of filtered_data\n",
        "        num_samples = min(num_examples, len(filtered_data))\n",
        "\n",
        "        # Randomly pick num_samples elements from filtered_data\n",
        "        random_elements = random.sample(filtered_data, num_samples)\n",
        "      else:\n",
        "        random_elements = filtered_data[start:start+num_examples]\n",
        "\n",
        "      examples = \"\".join([\n",
        "          f\"Text: ```{element['description']}```\\nClass: {element.get('question', 'No question')}\\n\"\n",
        "          for element in random_elements\n",
        "      ])\n",
        "\n",
        "      # print(element)\n",
        "      ticket_text = element['description']\n",
        "\n",
        "      # Prompt ‚Äì Classify Class\n",
        "      prompt = f\"\"\"\n",
        "      Classify the text delimited by triple backticks into one of the following classes.\n",
        "      Classes: {desired_tags}\n",
        "\n",
        "      {examples}\n",
        "\n",
        "      Text: ```{ticket_text}```\n",
        "      Class: \"\"\"\n",
        "      response = get_completion(prompt, model=model, temperature=temperature)\n",
        "\n",
        "      # print(\"Prompt: \"+prompt)\n",
        "      print(\"Prediction: \"+response)\n",
        "\n",
        "      if(response == element['question']):\n",
        "          score_tags += 1\n",
        "          print(\"Correct. Actual: \"+element['question'])\n",
        "      else:\n",
        "          print(\"Incorrect. Actual: \"+element['question'])\n",
        "      count += 1\n",
        "      print()\n",
        "\n",
        "  print(\"__________________\")\n",
        "  print(f\"Priority Accuracy: {score_tags/count}\")\n",
        "  return score_tags/count\n",
        "acc['add_5_examples'] = add_examples(5, 1000, evalset_start_all, evalset_end_all)"
      ],
      "metadata": {
        "id": "gVZp113EEid7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc['add_5_random_examples'] = add_examples(5, 0, evalset_start_all, evalset_end_all)"
      ],
      "metadata": {
        "id": "WpBt0ZJkuBCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc['add_5_examples_gpt4'] = add_examples(5, 1000, evalset_start_all, evalset_end_all, model=\"gpt-4\")"
      ],
      "metadata": {
        "id": "hLxdvalkEiGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc['add_20_random_examples'] = add_examples(20, 0, evalset_start_all, evalset_end_all)"
      ],
      "metadata": {
        "id": "S3m4_y9WE3Rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc['add_20_examples'] = add_examples(20,1000, evalset_start_all, evalset_end_all)"
      ],
      "metadata": {
        "id": "fFPc1HDguiD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Change Temperature"
      ],
      "metadata": {
        "id": "3sT4nUM_P_vQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc['add_20_examples_change_temp'] = add_examples(20, 1000, evalset_start_all, evalset_end_all, temperature=0.7)"
      ],
      "metadata": {
        "id": "3HuGEddGDqyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Change model to GPT-4"
      ],
      "metadata": {
        "id": "hudPh8kEP8aW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc['add_20_examples_gpt4'] = add_examples(20, 1000, evalset_start_all, evalset_end_all, model=\"gpt-4\")"
      ],
      "metadata": {
        "id": "ZWmLELPlEZp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Describe Classes"
      ],
      "metadata": {
        "id": "Zp1J33DqP2WN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add examples to the prompt. pick examples randomly unless start is set.\n",
        "# supports adding both random examples and a pre-defined set of examples\n",
        "# evaluate on examples filtered_data[evalset_start:evalset_end]\n",
        "def describe_classes(num_examples=5, start=0, evalset_start=2500, evalset_end=2510, model=\"gpt-3.5-turbo\", temperature=0):\n",
        "  count, score_tags = 0, 0\n",
        "  for element in filtered_data[evalset_start:evalset_end]:\n",
        "      if start==0:\n",
        "        # Ensure num_examples does not exceed the length of filtered_data\n",
        "        num_samples = min(num_examples, len(filtered_data))\n",
        "\n",
        "        # Randomly pick num_samples elements from filtered_data\n",
        "        random_elements = random.sample(filtered_data, num_samples)\n",
        "      else:\n",
        "        random_elements = filtered_data[start:start+num_examples]\n",
        "\n",
        "      examples = \"\".join([\n",
        "          f\"Text: ```{element['description']}```\\nClass: {element.get('question', 'No question')}\\n\"\n",
        "          for element in random_elements\n",
        "      ])\n",
        "\n",
        "      # print(element)\n",
        "      ticket_text = element['description']\n",
        "\n",
        "      # Prompt ‚Äì Classify Class\n",
        "      prompt = f\"\"\"\n",
        "      Given the following description for each class:\n",
        "      type_feature_request: A request for a feature by a user of Weights & Biases.\n",
        "      type_bug: A bug report by a user of Weights & Biases.\n",
        "      question: If the request is not related to Weights & Biases; or doesn't fit the above 2 categories.\n",
        "\n",
        "      Classify the text delimited by triple backticks into one of the following classes.\n",
        "      Classes: {desired_tags}\n",
        "\n",
        "      {examples}\n",
        "\n",
        "      Text: ```{ticket_text}```\n",
        "      Class: \"\"\"\n",
        "      response = get_completion(prompt)\n",
        "\n",
        "      # print(\"Prompt: \"+prompt)\n",
        "      print(\"Prediction: \"+response)\n",
        "\n",
        "      if(response == element['question']):\n",
        "          score_tags += 1\n",
        "          print(\"Correct. Actual: \"+element['question'])\n",
        "      else:\n",
        "          print(\"Incorrect. Actual: \"+element['question'])\n",
        "      count += 1\n",
        "      print()\n",
        "\n",
        "  print(\"__________________\")\n",
        "  print(f\"Priority Accuracy: {score_tags/count}\")\n",
        "  print()\n",
        "  return score_tags/count\n",
        "acc['add_5_examples_describe_classes'] = describe_classes(5,1000, evalset_start_all, evalset_end_all)\n",
        "acc['add_20_examples_describe_classes'] = describe_classes(20,1000, evalset_start_all, evalset_end_all)"
      ],
      "metadata": {
        "id": "75FRFDQQJvl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Specify Steps"
      ],
      "metadata": {
        "id": "7fYxSsIcPtE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add examples to the prompt. pick examples randomly unless start is set.\n",
        "# supports adding both random examples and a pre-defined set of examples\n",
        "# evaluate on examples filtered_data[evalset_start:evalset_end]\n",
        "def specify_steps(num_examples=5, start=0, evalset_start=2500, evalset_end=2510, examples=1, model=\"gpt-3.5-turbo\", temperature=0):\n",
        "  count, score_tags = 0, 0\n",
        "  for element in filtered_data[evalset_start:evalset_end]:\n",
        "      if start==0:\n",
        "        # Ensure num_examples does not exceed the length of filtered_data\n",
        "        num_samples = min(num_examples, len(filtered_data))\n",
        "\n",
        "        # Randomly pick num_samples elements from filtered_data\n",
        "        random_elements = random.sample(filtered_data, num_samples)\n",
        "      else:\n",
        "        random_elements = filtered_data[start:start+num_examples]\n",
        "\n",
        "      examples = \"\".join([\n",
        "          f\"Text: ```{element['description']}```\\nClass: {element.get('question', 'No question')}\\n\"\n",
        "          for element in random_elements\n",
        "      ])\n",
        "\n",
        "      # print(element)\n",
        "      ticket_text = element['description']\n",
        "\n",
        "      # Prompt ‚Äì Classify Class\n",
        "      if examples:\n",
        "        prompt = f\"\"\"\n",
        "        Given the text delimited by triple backticks, perform the following actions:\n",
        "        1 - Summarize what the user wants in 1-2 lines.\n",
        "        2 - Recommend a next action based on the user' request.\n",
        "        3 - Determine if the request is related to the product or company 'Weights & Biases'\n",
        "        4 - Classify the into one of the following classes. Classes: {desired_tags}\n",
        "        5 - Output a json object that contains the following keys: summary, recommended_action, is_wb, class\n",
        "\n",
        "        Here are some examples help you with the classification step.\n",
        "        {examples}\n",
        "\n",
        "        And here's the text ```{ticket_text}```\"\"\"\n",
        "      else:\n",
        "        prompt = f\"\"\"\n",
        "        Given the text delimited by triple backticks, perform the following actions:\n",
        "        1 - Summarize what the user wants in 1-2 lines.\n",
        "        2 - Recommend a next action based on the user' request.\n",
        "        3 - Determine if the request is related to the product or company 'Weights & Biases'\n",
        "        4 - Classify the into one of the following classes. Classes: {desired_tags}\n",
        "        5 - Output a json object that contains the following keys: summary, recommended_action, is_wb, class\n",
        "\n",
        "        Here's the text ```{ticket_text}```\"\"\"\n",
        "\n",
        "      response_json = get_completion(prompt)\n",
        "      # print(\"Prompt: \"+prompt)\n",
        "      print(\"Prediction: \")\n",
        "      response = json.loads(response_json)\n",
        "      print(\"Summary: \"+response['summary'])\n",
        "      print(\"Recommended Action: \"+response['recommended_action'])\n",
        "      print(\"Is W&B Related: \"+str(response['is_wb']))\n",
        "      print(\"Prediction: \"+response['class'])\n",
        "\n",
        "      if(response['class'] == element['question']):\n",
        "          score_tags += 1\n",
        "          print(\"Correct. Actual: \"+element['question'])\n",
        "      else:\n",
        "          print(\"Incorrect. Actual: \"+element['question'])\n",
        "      count += 1\n",
        "      print()\n",
        "\n",
        "  print(\"__________________\")\n",
        "  print(f\"Priority Accuracy: {score_tags/count}\")\n",
        "  return score_tags/count\n",
        "\n",
        "acc['specify_steps'] = specify_steps(5,1000, evalset_start_all, evalset_end_all)"
      ],
      "metadata": {
        "id": "_YXwWv3ZIF2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# acc['specify_steps_no_examples'] = specify_steps(5,1000, evalset_start_all, evalset_end_all, examples=0)"
      ],
      "metadata": {
        "id": "q0b1FJ7HMzLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explain LLM Reasoning Behind Steps"
      ],
      "metadata": {
        "id": "JuUlt_atPYnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add examples to the prompt. pick examples randomly unless start is set.\n",
        "# supports adding both random examples and a pre-defined set of examples\n",
        "# evaluate on examples filtered_data[evalset_start:evalset_end]\n",
        "def specify_steps_explain_reasoning(num_examples=5, start=0, evalset_start=2500, evalset_end=2510, examples=1, model=\"gpt-3.5-turbo\", temperature=0):\n",
        "  count, score_tags = 0, 0\n",
        "  for element in filtered_data[evalset_start:evalset_end]:\n",
        "      if start==0:\n",
        "        # Ensure num_examples does not exceed the length of filtered_data\n",
        "        num_samples = min(num_examples, len(filtered_data))\n",
        "\n",
        "        # Randomly pick num_samples elements from filtered_data\n",
        "        random_elements = random.sample(filtered_data, num_samples)\n",
        "      else:\n",
        "        random_elements = filtered_data[start:start+num_examples]\n",
        "\n",
        "      examples = \"\".join([\n",
        "          f\"Text: ```{element['description']}```\\nClass: {element.get('question', 'No question')}\\n\"\n",
        "          for element in random_elements\n",
        "      ])\n",
        "\n",
        "      # print(element)\n",
        "      ticket_text = element['description']\n",
        "\n",
        "      # Prompt ‚Äì Classify Class\n",
        "      if examples:\n",
        "        prompt = f\"\"\"\n",
        "        Given the text delimited by triple backticks, perform the following actions:\n",
        "        1 - summary: Summarize what the user wants in 1-2 lines.\n",
        "        2 - summary_reasoning: Explain your reasoning for the summary.\n",
        "        3 - recommended_action: Recommend a next action based on the user' request.\n",
        "        4 - recommended_action_reasoning: Explain your reasoning for the recommended next action.\n",
        "        5 - is_wb: Determine if the request is related to the product or company 'Weights & Biases'.\n",
        "        6 - is_wb_reasoning: Explain your reasoning for detemining if the request is W&B related.\n",
        "        7 - class: Classify the into one of the following classes. Classes: {desired_tags}\n",
        "        8 - Output a json object that contains the following keys: summary, summary_reasoning, recommended_action, recommended_action_reasoning, is_wb, is_wb_reasoning, class\n",
        "\n",
        "        Here are some examples help you with the classification step.\n",
        "        {examples}\n",
        "\n",
        "        And here's the text ```{ticket_text}```.\n",
        "\n",
        "        Make sure the output is only a json object.\n",
        "        \"\"\"\n",
        "      else:\n",
        "        prompt = f\"\"\"\n",
        "        Given the text delimited by triple backticks, perform the following actions:\n",
        "        1 - summary: Summarize what the user wants in 1-2 lines.\n",
        "        2 - summary_reasoning: Explain your reasoning for the summary.\n",
        "        3 - recommended_action: Recommend a next action based on the user' request.\n",
        "        4 - recommended_action_reasoning: Explain your reasoning for the recommended next action.\n",
        "        5 - is_wb: Determine if the request is related to the product or company 'Weights & Biases'.\n",
        "        6 - is_wb_reasoning: Explain your reasoning for detemining if the request is W&B related.\n",
        "        7 - class: Classify the into one of the following classes. Classes: {desired_tags}\n",
        "        8 - Output a json object that contains the following keys: summary, summary_reasoning, recommended_action, recommended_action_reasoning, is_wb, is_wb_reasoning, class\n",
        "\n",
        "        Here's the text ```{ticket_text}```\"\"\"\n",
        "\n",
        "      response_json = get_completion(prompt)\n",
        "      # print(\"Prompt: \"+prompt)\n",
        "      print(\"Prediction: \")\n",
        "      response = json.loads(response_json)\n",
        "      print(\"Summary: \"+response['summary'])\n",
        "      print(\"Summary Reasoning: \"+response['summary_reasoning'])\n",
        "      print(\"Recommended Action: \"+response['recommended_action'])\n",
        "      print(\"Recommended Reasoning: \"+response['recommended_action_reasoning'])\n",
        "      print(\"Is W&B Related: \"+str(response['is_wb']))\n",
        "      print(\"Is W&B Related Reasoning: \"+response['is_wb_reasoning'])\n",
        "      print(\"Prediction: \"+response['class'])\n",
        "\n",
        "      if(response['class'] == element['question']):\n",
        "          score_tags += 1\n",
        "          print(\"Correct. Actual: \"+element['question'])\n",
        "      else:\n",
        "          print(\"Incorrect. Actual: \"+element['question'])\n",
        "      count += 1\n",
        "      print()\n",
        "\n",
        "  print(\"__________________\")\n",
        "  print(f\"Priority Accuracy: {score_tags/count}\")\n",
        "  return score_tags/count\n",
        "\n",
        "acc['specify_steps_explain_reasoning'] = specify_steps_explain_reasoning(5,1000, evalset_start_all, evalset_end_all)"
      ],
      "metadata": {
        "id": "3AMtIb6uOuHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc['specify_steps_explain_reasoning_gpt4'] = specify_steps_explain_reasoning(5,1000, evalset_start_all, evalset_end_all, model=\"gpt-4\")"
      ],
      "metadata": {
        "id": "y5UD4mg5ZVxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Chaining"
      ],
      "metadata": {
        "id": "Hq2EgDCCPnuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add examples to the prompt. pick examples randomly unless start is set.\n",
        "# supports adding both random examples and a pre-defined set of examples\n",
        "# evaluate on examples filtered_data[evalset_start:evalset_end]\n",
        "def prompt_chaining(num_examples=5, start=0, evalset_start=2500, evalset_end=2510, examples=1, model=\"gpt-3.5-turbo\", temperature=0):\n",
        "  count, score_tags = 0, 0\n",
        "  for element in filtered_data[evalset_start:evalset_end]:\n",
        "      if start==0:\n",
        "        # Ensure num_examples does not exceed the length of filtered_data\n",
        "        num_samples = min(num_examples, len(filtered_data))\n",
        "\n",
        "        # Randomly pick num_samples elements from filtered_data\n",
        "        random_elements = random.sample(filtered_data, num_samples)\n",
        "      else:\n",
        "        random_elements = filtered_data[start:start+num_examples]\n",
        "\n",
        "      examples = \"\".join([\n",
        "          f\"Text: ```{element['description']}```\\nClass: {element.get('question', 'No question')}\\n\"\n",
        "          for element in random_elements\n",
        "      ])\n",
        "\n",
        "      # print(element)\n",
        "      ticket_text = element['description']\n",
        "\n",
        "      # Prompt ‚Äì Classify Class\n",
        "      prompt = f\"\"\"\n",
        "      Given the text delimited by triple backticks, perform the following actions:\n",
        "      1 - summary: Summarize what the user wants in 1-2 lines.\n",
        "      2 - summary_reasoning: Explain your reasoning for the summary.\n",
        "      3 - recommended_action: Recommend a next action based on the user' request.\n",
        "      4 - recommended_action_reasoning: Explain your reasoning for the recommended next action.\n",
        "      5 - is_wb: Determine if the request is related to the product or company 'Weights & Biases'.\n",
        "      6 - is_wb_reasoning: Explain your reasoning for detemining if the request is W&B related.\n",
        "      7 - Output a json object that contains the following keys: summary, summary_reasoning, recommended_action, recommended_action_reasoning, is_wb, is_wb_reasoning, class\n",
        "\n",
        "      And here's the text ```{ticket_text}```.\n",
        "\n",
        "      Make sure the output is only a json object.\n",
        "      \"\"\"\n",
        "\n",
        "      response_json = get_completion(prompt)\n",
        "      # print(\"Prompt: \"+prompt)\n",
        "      print(\"Prediction: \")\n",
        "      response = json.loads(response_json)\n",
        "      print(\"Summary: \"+response['summary'])\n",
        "      print(\"Summary Reasoning: \"+response['summary_reasoning'])\n",
        "      print(\"Recommended Action: \"+response['recommended_action'])\n",
        "      print(\"Recommended Reasoning: \"+response['recommended_action_reasoning'])\n",
        "      print(\"Is W&B Related: \"+str(response['is_wb']))\n",
        "      print(\"Is W&B Related Reasoning: \"+response['is_wb_reasoning'])\n",
        "\n",
        "\n",
        "      prompt_2 = f\"\"\"\n",
        "        Given the following info about a user request:\n",
        "        1 - text delimited by triple backticks: ```{ticket_text}```\n",
        "        2 - summary of what the user wants: {response['summary']}\n",
        "        3 - recommended next action based on the user' request: {response['recommended_action']}\n",
        "        4 - whether the request is related to the product or company 'Weights & Biases': {response['is_wb']}\n",
        "\n",
        "        Classify the text into one of the following classes. Classes: {desired_tags}\n",
        "\n",
        "        Here are some examples help you with the classification step.\n",
        "        {examples}\n",
        "\n",
        "        Only print the name of the class\"\"\"\n",
        "      response_class = get_completion(prompt_2)\n",
        "      print(\"Prediction: \"+response_class)\n",
        "\n",
        "      if(response_class == element['question']):\n",
        "          score_tags += 1\n",
        "          print(\"Correct. Actual: \"+element['question'])\n",
        "      else:\n",
        "          print(\"Incorrect. Actual: \"+element['question'])\n",
        "      count += 1\n",
        "      print()\n",
        "\n",
        "  print(\"__________________\")\n",
        "  print(f\"Priority Accuracy: {score_tags/count}\")\n",
        "  return score_tags/count\n",
        "\n",
        "# acc['prompt_chaining'] = prompt_chaining(5,1000, evalset_start_all, evalset_end_all)"
      ],
      "metadata": {
        "id": "0UgIuJb8PMJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(list(acc.items()), columns=['Prompt', 'Accuracy']).sort_values(by='Accuracy', ascending=True)\n",
        "def highlight_max_accuracy(data, color='mediumpurple'):\n",
        "    highlight = pd.DataFrame('', index=data.index, columns=data.columns)\n",
        "    max_accuracy = data['Accuracy'] == data['Accuracy'].max()\n",
        "    highlight[max_accuracy] = f'background-color: {color}'\n",
        "    return highlight\n",
        "\n",
        "# Applying the styling\n",
        "df = df.style.apply(highlight_max_accuracy, axis=None)\n",
        "df"
      ],
      "metadata": {
        "id": "Tdp-rWd9Kp-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next Steps\n",
        "- [ ] Improve performance, make eval set more expansive\n",
        "- [ ] Improve prompt chaining\n",
        "- [ ] Improve summarization + suggest next steps using docs + support tickets (try RAG)\n",
        "- [ ] How to evaluate the summary + next steps?\n",
        "- [ ] RLHF for summary + next steps"
      ],
      "metadata": {
        "id": "dLZ5W6faQdx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt  Accuracy\n",
        "               basic_prompt      0.00\n",
        "             add_5_examples      0.35\n",
        "      add_5_random_examples      0.10\n",
        "        add_5_examples_gpt4      0.30\n",
        "     add_20_random_examples      0.60\n",
        "            add_20_examples      0.90\n",
        "add_20_examples_change_temp      0.80\n",
        "       add_20_examples_gpt4      0.30\n",
        "           describe_classes      0.40"
      ],
      "metadata": {
        "id": "L5KitCtXGKNa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mv7M_t05_zb_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}