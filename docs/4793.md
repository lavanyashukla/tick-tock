TITLE:
[CLI]: Overwriting old timestep data when resuming DDP job with PyTorch lightning & wandb

LABEL:
cli,c:rewind

STATE:
open

BODY:
### Describe the bug

<!--- Description of the issue below  -->
Current issue is very similar to the issues described previously here: #2971. I'm using PyTorch lighting (tested with 1.8.0 through 1.8.6) and the latest release of the wandb client library. My jobs are all running in 'live' mode, with the initial run working well without any errors. However if I attempt to resume the job, while the models appear to train appropriately and losses trend down from the last loaded checkpoint, the previously graphed losses progressively begin to get overwritten on the online wandb dashboard. Instead of resuming at step 2520 for example, the steps begin overwriting from step 0. This is using DDP for multigpu training.

Things I have tried to exclude so far:
1. Ensuring `self.global_step` with my code is loaded from the checkpoint correctly (it does)
2. Isolated to one system or multiple: Reproduced same results across 2x different linux systems
3. Troubleshoot specific issues related to how I'm using wandb: The way my code is setup, I'm using PyTorch lightning to log at the end of each training and validation step using the inbuilt self.log hooks. With this setup I have tried adding dummy variables to log during the training_step and validation_step phases to see if that changes anything (It doesn't). I don't see any documentation suggesting my current implementation is not compatible with the wandblogger. Example skeleton code shown below: 
 ```python
class ExampleModule(LightningModule):
	def __init__():
		self.loss_fn = nn.CrossEntropyLoss()
		self.accuracy = torchmetrics.Accuracy(top_k=1)

	def forward(self, inptus, target):
		# some model here that returns outputs

	def training_step(self, batch, batch_idx):
		inputs, target = batch
		outputs = self.forward(inputs, target)

		return training_step_outputs


	def training_step_end(self, training_step_outputs):
		'''
		Collects outputs across workers to perform some calculations 
		'''

		vector_1 = self.all_gather(training_step_outputs[0], sync_grads=True)
		vector_2 = self.all_gather(training_step_outputs[1], sync_grads=True)

		vector_1 = vector_1.reshape(self.batch_size*vector_1.shape[0], -1)
		vector_2 = vector_2.reshape(self.batch_size*vector_2.shape[0], -1)

		### Some math here to generate logits ###

		logits = torch.mm(vector_1, vector_2.transpose(1,0))
		
		# Calculate Loss
		labels = torch.cuda.LongTensor(list(range(vector_1.shape[0])))
		loss = self.loss_fn(logits, labels)
		probs = nn.functional.softmax(logits, dim=1)
		accuracy = self.accuracy(probs, labels)
		
                #LOGGING :
		self.log('loss', loss, on_step=True, on_epoch=True, prog_bar=True, batch_size=self.batch_size, sync_dist=True)
		self.log('acc', accuracy, on_step=True, on_epoch=True, prog_bar=False, batch_size=self.batch_size, sync_dist=True)

		return loss
```

4. I have tried forcing the 'step' variable in wandb to equal the self.global_step counter from the checkpoint as suggested in #2971, this does not appear possible with the wandblogger framework. 

Picture below to illustrate the 'overwriting':
<img width="486" alt="Screenshot 2023-01-14 at 10 59 18 AM" src="https://user-images.githubusercontent.com/12033026/212503096-70def417-a2a2-4e60-8907-00a420d070bf.png">




### Additional Files

_No response_

### Environment

WandB version: 0.13.9

OS: Ubuntu & CentOS

Python version: 3.7.9 and 3.9.12

Versions of relevant libraries:
torch 1.12 and 1.11 (cuda)
pytorch_lightning 1.8.0, 1.8.3, 1.8.6



### Additional Context

_No response_

