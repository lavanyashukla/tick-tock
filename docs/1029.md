TITLE:
Looking for advice on evaluating model in separate process

LABEL:
c:misc

STATE:
closed

BODY:
`wandb --version && python --version && uname`

* Weights and Biases version: 0.8.35
* Python version: 3.7.5
* Operating System: Linux

### Description

I'm looking for some advice on how to interface with wandb when I want to evaluate my model periodically in a separate process.

I usually evaluate my models periodically (e.g., every epoch) on a validation set and get a set of metrics averaged over the validation set, which I then record to wandb. However, evaluating can take a long time (several minutes or longer), and because later code doesn't always depend on the results of the evaluation, I want to spawn another process to evaluate on the validation set so that the training is not being blocked.

Usually what I do is run a call to evaluation, then log in wandb using the current global "step" at the time of evaluation (which is usually something like the number of batch updates to the model so far, because I also log batch-level metrics, like the loss). 

However, I'm not sure how I can maintain the same way I use wandb, notably because I can't set values for old history rows. If I want my evaluation step number to line up with when evaluation started (e.g., at step N), but I've proceeded to train after evaluation started in parallel (and now wandb is at step N+M), I won't be able to sent the evaluation results associated with step N because it's not supported by wandb (as far as I know).

Unless adding to old history rows is supported (and I am just missing something), there are a few ways I can think to get around this while still logging batch-level metrics, but none I am particularly happy with:

1. Use a separate wandb experiment that's only for the validation results. This would make the wandb dashboard disorganized.
1. Maintain my own global "queue" of things to log that wandb pulls from, and when evaluation ends, log its results as well as the batch-level results that were put in the queue while evaluation was running. This seems like a real headache to implement and also means that viewing training progress is blocked while evaluation is happening.
1. Just log the validation results according to the current wandb step when evaluation finishes. However, then the validation results will appear as if they are evaluating the model at time N+M, when really they were evaluating the model at time N.

Any other suggestions? Or am I mistaken, and writing to old history rows is supported?

Thanks!

