TITLE:
Best Practices for logging image predictions while referencing only the original version-controlled artifact media files[Q]

LABEL:
stale

STATE:
closed

BODY:
This references tssweeney's PR here: #1438

### Overview

I have a question about the best way to implement this as a workflow. I'd like to repeatedly log predictions on a version-controlled test set of images with class labels, while:

A. Ensuring that the prediction tables correctly associate any subsequent predictions on sample X with unique ID X_id across multiple evaluations of different models.

B. Ensure that the image file corresponding with a particular prediction is correctly uploaded essentially as a reference to the original server-side version-controlled image file, instead of uploading a duplicate image file upon every round of predictions.

### Example artifacts layout:
```

artifact 1  
     /media  
            /image1  
            /image2  
            ...

artifact 2
    /tables
            /test_prediction_table.json
    /media
            /image1
            /image2
            ...
```


In the artifacts above, artifact 2 contains a table with one column referencing a hosted image file, and some other columns each representing a prediction on that image. As it stands now, every time I upload a prediction table, it results in a situation like this, where each separate artifact contains its own copy of the same images. What I would like is for the test_prediction_table.json to only reference the images in artifact 1/media, and keep the media subdir for artifact 2 empty.

tldr; I can successfully track predictions per-image across multiple evaluations logged at different times, but do not yet have a clear method for preventing the redundant duplication of raw image data upon every new log event.

---------------------

### Current possibilities

I guess there are a few ways I could imagine this working.

1. My default strategy, performing predictions normally then wrapping the image in a wandb.Image constructor every time, allows minimal refactoring of my workflow. However, this seems unlikely to be well-suited for perfectly reproducing images automatically in a way that links them to any pre-existing server-side artifacts. At the moment, it appears this is uploading an entire copy of every image for each round of predictions. While I am successfully able to compare predictions across models based on always including a common unique image ID, the value of this is greatly diminished if Im rapidly using up all storage space.
2. Using artifact.get for each image individually seems like it would come with some serious computational and software design-related inefficiencies, in addition to requiring users to refactor their data loading pipelines around WandB's artifact API in a more fundamental way than simply using it to download a collection of files to a local directory.
3. A user must also consider when and where to apply standard image preprocessing to an downloaded image being used for prediction, as this isnt always (and shouldnt be required to be) already applied to version-controlled images.
4. 2 of the strategies mentioned in the above-mentioned pull request (#1438), are:
`artifact.add_reference("wandb-artifact://<UPSTREAM_ARTIFACT_ID>/RESOURCE_PATH", "TARGET_PATH")`
and
`artifact.add_reference(other_artifact.get_path(<ASSET_NAME>), )`

Both are closer to what I have in mind while seeming still impractical for real deployment. Requiring proper artifact URIs for each individual image to be accessible at any point in a pipeline at which the user logs an image prediction attaches a massive burden to the designer. In ML/DL workflows we're often working with large batches of images stacked as raw numpy/tensor arrays with either no additional image info or simply a single integer label per-image. If we now need to keep track of wandb entity, project, artifact, and image URI locations for every one, the commitment necessary to really integrate wandb into these types of experiments may become higher than a programmer can justify.


Much of this is a recontextualized version of a comment I made on the wandb slack a week or two ago (~May 17th on channel #ask-for-feedback) which unfortunately did not land me any answers, reproduced below. 




> Another question regarding the new Dataset Visualization utility:
Topic: Regarding best practices to minimize redundant image data storage + only require upload of raw image once for many predictions
In the Image Classification with DSViz report (Really inspiring stuff btw), there's some lack of clarity around how exactly to organize image prediction logging while preventing the duplication of raw image data. Specifically, I think there would be immense value in getting some clarity targeted towards 2 distinct stages of the process, namely:
During an experiment, how to download a version-controlled image dataset from WandB -> mark as inputs for the run -> During prediction, the user must upload a wandb.Table with rows each containing a value for <???> in order to directly link the image used locally for prediction to an image stored on WandB's servers.
The tutorial has us upload both an id and the actual image during prediction, even though the documentation suggests only a shared id is required to benefit from the relational table comparison capabilities.
While allowing the user to upload the exact image used for prediction may help diagnose possible bugs introduced since downloading the images (e.g. the id may have become misaligned from its true image object), it also adds a significant increased demand in total data I/O over the network that requires the user to be more parsimonious with when to repeat predictions during training, or risk radically slowing down training.
From another perspective, this is a lost opportunity for maximally benefitting from the computational efficiency inherent to passing around image references (e.g. id) rather than the raw images themselves.
> 2. From the server perspective: How should we reference images during local artifact & table creation so that on the server, when inspecting our prediction tables, the image file referenced in each row correctly references an existing version-controlled image (possibly in another artifact), rather than creating a new reference to a new duplicate of the original image (e.g. the server containing 2 distinct files containing the image data for a single original sample, with each file possibly referenced by a different table).
Apologies for the wall of text. As I was typing this I realized it may be more suited as a GitHub issue or Pull request, so let me know if you think I should post it elsewhere. Regardless, I think clarity on this still somewhat abstracted aspect of the new data vis workflow would enable making it immensely more accessible. Thanks again!


