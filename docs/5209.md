TITLE:
Issue with running training code[Q]

LABEL:
c:misc

STATE:
closed

BODY:
Hi, I am using the below training code and am having trouble getting the code to run. This exact code has worked for me in the past and works on my other device, so I believe the issue may be with package versions but am not completely sure. My code is below (note is directly copied from this tutorial: https://towardsdatascience.com/hyperparameter-optimization-for-optimum-transformer-models-b95a32b70949). One thing I noticed is that when I terminated the jupyter notebook process when it was working the error was just a keyboard interrupt, but now when i terminate the process I get a bunch of Process ForkPoolWarning errors/messages. Note, the code still runs, but when training it is stuck at 0 and never moves forward. I have tried installing and reinstalling wandb, but that didn't work, as well as trying a completely new wandb account.

import logging
from statistics import mean

import pandas as pd
from sklearn.metrics import accuracy_score

import wandb
from simpletransformers.classification import ClassificationArgs, ClassificationModel
#from utils import load_rte_data_file


logging.basicConfig(level=logging.INFO)
transformers_logger = logging.getLogger("transformers")
transformers_logger.setLevel(logging.WARNING)

# Preparing train data
train_df = load_rte_data_file("./FinalDS/testTrainDS.csv")
eval_df = load_rte_data_file("./FinalDS/testValidDS.csv")
test_df = load_rte_data_file("./FinalDS/testTestDS.csv")
model_args = ClassificationArgs()
model_args.eval_batch_size = 32
model_args.evaluate_during_training = True
model_args.evaluate_during_training_silent = False
model_args.evaluate_during_training_steps = -1
model_args.save_eval_checkpoints = False
model_args.save_model_every_epoch = False
model_args.learning_rate = 1e-5
model_args.manual_seed = 4
model_args.max_seq_length = 256
model_args.multiprocessing_chunksize = 5000
model_args.no_cache = True
model_args.num_train_epochs = 3
model_args.overwrite_output_dir = True
model_args.reprocess_input_data = True
model_args.train_batch_size = 16
model_args.gradient_accumulation_steps = 2
model_args.labels_list = ["not_entailment", "entailment"]
model_args.output_dir = "default_output"
model_args.best_model_dir = "default_output/best_model"
model_args.wandb_project = "RTE - Hyperparameter Optimization"
model_args.wandb_kwargs = {"name": "default"}

# Create a TransformerModel
model = ClassificationModel("roberta", "roberta-large", use_cuda=False, args=model_args)

# Train the model
model.train_model(
    train_df,
    eval_df=eval_df,
    accuracy=lambda truth, predictions: accuracy_score(
        truth, [round(p) for p in predictions]
    ),
)

model.eval_model(test_df, verbose=True)

