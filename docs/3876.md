TITLE:
[Feature]: Alternate Sweep/Early Stopping Algorithms eg. ASHA

LABEL:
feature_request,c:sweeps

STATE:
open

BODY:
### Description

When researching ML tuning libraries, it seemed that many people recommended [ray tune](https://www.ray.io/ray-tune) had because it had advanced sweep algorithms, like [ASHA](https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/) and Population-based Training. I'm wondering if there is interest or plans to support more complex algorithms?

Right now, W&B appears to support grid search, random, and bayes for parameter selection and hyperband for early stopping. 

### Suggested Solution

It would be cool to see alternative early stopping and hyperparameter selection algorithms as options in sweep configuration. I'm specifically hurting for ASHA right now.

### Alternatives

Raytune has W&B integration, but I tried it and found W&B to be much easier to set up and I love the native sweep/chart integrationâ€”it would be cool to have feature parity and be able to stay W&B native.


### Additional Context

I would be interested in taking a crack at implementation. Does [this](https://github.com/wandb/sweeps/tree/master/src/sweeps) seem like the right place to implement? I'm not sure I understand the interfaces these files use.

