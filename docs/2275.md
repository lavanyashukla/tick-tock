TITLE:
[CLI] ValueError: You can only call `wandb.watch` once per model.

LABEL:
cli

STATE:
closed

BODY:
# Description
I am training a multilingual BERT model with HuggingFace Aceelerator and wandb integration. The code gives a `ValueError` when I run it on a single GPU. I have only 1 instance of the model, then why do I get this error?

# Error
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<timed exec> in <module>

/opt/conda/lib/python3.6/site-packages/accelerate/notebook_launcher.py in notebook_launcher(function, args, num_processes, use_fp16, use_port)
    120             else:
    121                 print("Launching training on CPU.")
--> 122             function(*args)

<timed exec> in run()

<timed exec> in train_fn(data_loader, model, optimizer, scheduler)

/opt/conda/lib/python3.6/site-packages/wandb/sdk/wandb_watch.py in watch(models, criterion, log, log_freq, idx)
     93 
     94         graph = wandb.wandb_torch.TorchGraph.hook_torch(
---> 95             model, criterion, graph_idx=global_idx
     96         )
     97         graphs.append(graph)

/opt/conda/lib/python3.6/site-packages/wandb/wandb_torch.py in hook_torch(cls, model, criterion, graph_idx)
    321     def hook_torch(cls, model, criterion=None, graph_idx=0):
    322         graph = TorchGraph()
--> 323         graph.hook_torch_modules(model, criterion, graph_idx=graph_idx)
    324         return graph
    325 

/opt/conda/lib/python3.6/site-packages/wandb/wandb_torch.py in hook_torch_modules(self, module, criterion, prefix, graph_idx)
    373         if hasattr(module, "_wandb_watch_called") and module._wandb_watch_called:
    374             raise ValueError(
--> 375                 "You can only call `wandb.watch` once per model.  Pass a new instance of the model if you need to call wandb.watch again in your code."
    376             )
    377         module._wandb_watch_called = True

ValueError: You can only call `wandb.watch` once per model.  Pass a new instance of the model if you need to call wandb.watch again in your code.
```

# Program Engine
```

# Weights and Biases
import wandb
wandb.login()

%%time

def run():

    with wandb.init(project = "jigsaw-multilingual-bert"):    
       
        def loss_fn(outputs, targets):
            
            """
            Function to set BCEWithLogitsLoss as the Loss Function
            
            parameters: outputs - The outputs produced by the model
                        targets - The targets as defined in the data
                        
            returns: BCEWithLogitsLoss for the input parameters
            
            """
            
            return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))

        def train_fn(data_loader, model, optimizer, scheduler):
        
            
            """
            Training Function for the Model
            
            parameters: data_loader - PyTorch DataLoader
                        model - The Model to be used for training
                        optimizer - The Optimizer to be used for training
                        scheduler - The Learning Rate Scheduler 
                        
            returns: None
            """
            accelerator = Accelerator()
            device = accelerator.device
            from accelerate import DistributedDataParallelKwargs

            ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)
            accelerator = Accelerator(kwargs_handlers=[ddp_kwargs])

            model, optimizer, data_loader = accelerator.prepare(model, optimizer, data_loader)
            model.train()

            wandb.watch(model, log="all", log_freq=10)

            for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):
                ids = d["ids"]
                token_type_ids = d["token_type_ids"]
                mask = d["mask"]
                targets = d["targets"]

                ids = ids.to(device, dtype=torch.long)
                token_type_ids = token_type_ids.to(device, dtype=torch.long)
                mask = mask.to(device, dtype=torch.long)
                targets = targets.to(device, dtype=torch.float)
                outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)

                loss = loss_fn(outputs, targets)
                
                if bi % 1000 == 0:
                    print(f"bi={bi}, loss={loss}")

                accelerator.backward(loss)
                optimizer.step()
                scheduler.step()

                optimizer.zero_grad()
                wandb.log({"epoch": epoch, "loss": loss})

        def eval_fn(data_loader, model):
            
            """
            Evaluation Function for the Model
            
            parameters: data_loader - PyTorch Data Loader
                        model - The model to be used for Evaluation
                                                
            returns: fin_outputs - Final Outputs of the Model
                     fin_targets - Final Targets from the data
            """
            
            accelerator = Accelerator()
            device = accelerator.device
            
            model.eval()
            fin_targets = []
            fin_outputs = []

            with torch.no_grad():
                for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):
                    ids = d["ids"]
                    token_type_ids = d["token_type_ids"]
                    mask = d["mask"]
                    targets = d["targets"]

                    ids = ids.to(device, dtype=torch.long)
                    token_type_ids = token_type_ids.to(device, dtype=torch.long)
                    mask = mask.to(device, dtype=torch.long)
                    targets = targets.to(device, dtype=torch.float)

                    outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)
                    fin_targets.extend(targets.cpu().detach().numpy().tolist())
                    fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())

            return fin_outputs, fin_targets
    
        # Load Training Datasets
        df1 = pd.read_csv(
            "/workspace/data/jigsaw-multilingual/input/jigsaw-data/jigsaw-toxic-comment-train.csv", 
            usecols = ["comment_text", "toxic"]    
        )
        
        df1 = df1.head(100000)

        df2 = pd.read_csv(
            "/workspace/data/jigsaw-multilingual/input/jigsaw-data/jigsaw-unintended-bias-train.csv",
            usecols = ["comment_text", "toxic"]
        )
        
        df2 = df2.head(100000)
        
        # Combine Training Datasets
        df_train = pd.concat([df1, df2], axis = 0).reset_index(drop = True)
        df_train["comment_text"] = df_train["comment_text"].apply(clean_text)
        
        # Load Validation Datasets

        df_valid = pd.read_csv("/workspace/data/jigsaw-multilingual/input/jigsaw-data/Translated Datasets/jigsaw_miltilingual_valid_translated.csv")
        df_valid["comment_text"] = df_valid["translated"]
        df_valid.drop("translated", axis = 1, inplace = True)
        df_valid["comment_text"] = df_valid["comment_text"].apply(clean_text)
  
        nlp_transform = NLPTransform()

        df_train['lang'] = 'en'
        non_toxic_sentences = set()
        for comment_text in tqdm(df_train['comment_text'], total=df.shape[0]):
            non_toxic_sentences.update(nlp_transform.get_sentences(comment_text), 'en')

        transform = AddNonToxicSentencesTransform(non_toxic_sentences=list(non_toxic_sentences), p=1.0, sentence_range=(1,2))
        
        # Define Train Dataset and Data Loader
               
        train_dataset = JigsawDataset(
           df =  df_train,
           train_transforms = get_train_transforms()
        )

        train_data_loader = torch.utils.data.DataLoader(
            train_dataset, 
            batch_size=config.TRAIN_BATCH_SIZE, 
            num_workers=4
       )
        
        # Define Validation Dataset and Validation Data Loader

        valid_dataset = JigsawDataset(
            df = df_valid,
        )

        valid_data_loader = torch.utils.data.DataLoader(
            valid_dataset, 
            batch_size=config.VALID_BATCH_SIZE, 
            num_workers=1
        )

        # Select the device and initialize model
        model = BERTModel()

        # Set Optimizer Parameters
        param_optimizer = list(model.named_parameters())
        no_decay = ["bias", "LayerNorm.bias", "LayerNorm.weight"]
        optimizer_parameters = [
            {
                "params": [
                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)
                ],
                "weight_decay": 0.001,
            },
            {
                "params": [
                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)
                ],
                "weight_decay": 0.0,
            },
        ]
        
        num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)
        
        # Define Optimizer and Scheduler
        optimizer = AdamW(optimizer_parameters, lr=config.LEARNING_RATE)
        scheduler = get_linear_schedule_with_warmup(
            optimizer, 
            num_warmup_steps=0, 
            num_training_steps=num_train_steps)
        
        # Run the model for provided number of Epochs
        best_accuracy = 0
        for epoch in range(config.EPOCHS):
            
            print(f"----------EPOCH: {epoch}----------")
            train_fn(train_data_loader, model, optimizer, scheduler)
            outputs, targets = eval_fn(valid_data_loader, model)
            targets = np.array(targets) >= 0.5
            accuracy = metrics.roc_auc_score(targets, outputs)
            print(f"----------ROC AUC Score = {accuracy}----------")
            print()
            if accuracy > best_accuracy:
                torch.save(model.state_dict(), config.MODEL_PATH)
                best_accuracy = accuracy

```

# Environment
- Jupyter Lab
- Python 3.6.10 :: Anaconda, Inc.
- GPU:  Quadro RTX 6000
- wandb, version 0.10.31

