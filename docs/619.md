TITLE:
wandb loses "global_step" in PyTorch event_writer

LABEL:
bug

STATE:
closed

BODY:
There doesn't seem to be a correspondence between `global_step` and `step` in my graphs. I'm using wandb through eventwriter API in PyTorch. Debugging a bit, it seems

`step` gets converted to `global_step` here
https://github.com/wandb/client/blob/6417dd926abe76dbb7c56e7017d2ee7d1c918eb5/wandb/tensorboard/__init__.py#L205

Then `wandb.log` gets called with this dict, it doesn't see `step` so it assigns one automatically

Custom global step is useful to compare data efficiency consistency across runs -- using "forward calls" as x-axis means your curves look 2x better when doubling batch size or number of workers. Using global data counter for steps gives easier to interpret curves
@vanpelt 

