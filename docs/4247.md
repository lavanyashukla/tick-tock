TITLE:
[Feature]: Support for multi-threaded / parallel experiments

LABEL:
feature_request,c:core

STATE:
open

BODY:
### Description

To my understanding, it's currently only possible to log experiments done sequentially, or experiments done in using several sub-processes. However, a big part of out work consists in running multiple experiments using a ThreadPool (as this is much cheaper than using processes, and still provides speed-up for many libraries releasing the GIL for work). 

As such, I wonder if there is any way of tracking these kind of experiments right now.
As an example, think about evaluating 10 different hyperparameter choices on one and the same data set, e.g. using 3-fold cross-validation, and running each of the 10 models in parallel. How would we go ahead and track (average) metrics of all 10 models? How would we go about tracking each set of 3*10 results?

I can see https://github.com/wandb/examples/blob/master/examples/wandb-sweeps/sweeps-cross-validation/train-cross-validation.py, but this uses multiprocessing and parallelizes over splits, not across models (or across both at same time)

### Suggested Solution

I think just being able to use the following approach, independent on whether its sequential, parallel or thread-parallel, would be very helpful:

    def run_model_on_split(model_id, hyperparams, x_train, x_test, y_train, y_test):
      with wandb.init(group_name=model_id, ...) as run:
          ...
          run.log(acc)
          return acc

    with ThreadPoolExecutor(10) as tpe: // may as well be processpool
        for n_estimators in range(10):
           for split_data in cv.split(x, y):
              future = tpe.submit(run_model_on_split, n_estimators, {"n_estimators": n_estimators}, *split_data)
            

 

### Alternatives

_No response_

### Additional Context

_No response_

