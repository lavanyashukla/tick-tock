TITLE:
WandB incorrectly recognising transformers import as PyTorch (should be dynamic to PT or TF)

LABEL:
bug,stale

STATE:
closed

BODY:

* Weights and Biases version: 0.9.5
* Python version: 3.6
* Operating System: MacOS

### Description
Using colab, I was trying to run a sweep on a tf.keras model, the embeddings for which were created using huggingface's transformers tokenizer.

However, on import of transformers, WandB by default assumes you want to run a PyTorch model (checked by calling `wandb.config.as_dict()`, and so when running the sweep, despite the outputs showing correct setup, nothing happens after the links to the various project pages/runs are posted in the console output. 


### What I Did
```
!pip install transformers
!pip install wandb

import transformers
import wandb

!wandb login

wandb.init()
wandb.config.as_dict()
> {'_wandb': {'desc': None,
  'value': {'cli_version': '0.9.5',
   'framework': 'torch',
   'huggingface_version': '3.0.2',
   'is_jupyter_run': True,
   'is_kaggle_kernel': False,
   'python_version': '3.6.9'}}}
```

A workaround for now is to load a separate colab workbook, convert the inputs to embeddings as normal using the Huggingface tokenizer, but then save the embeddings as a numpy array to your colab directory.

In your other wandb notebook, don't import transformers, just load the saved embeddings. On doing this, `wandb.config.as_dict()` gives 'framework: 'tensorflow', and the sweep runs as expected. 



