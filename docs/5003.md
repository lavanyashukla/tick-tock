TITLE:
[Q] PyTorch Lightning minimum sweep example

LABEL:
c:sweeps

STATE:
open

BODY:
Hi, I tried to play to test hyperparameter sweep with PyTorch Lightning with a minimum example (1-D regression with 1 layer hidden layer). The sweep appears to start but I keep getting error messages 

> Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.


 I also got messages

> Run r58zdscy errored: RuntimeError('attach in the same process is not supported currently')
> wandb: ERROR Run r58zdscy errored: RuntimeError('attach in the same process is not supported currently')
> wandb: Agent Starting Run: jcqeoutz with config:
> wandb: 	lr: 0.11136099933551429
> wandb: 	n_hidden: 10

Below is my code. Anyone has any idea what I did wrong? Thanks ahead!

```
import pytorch_lightning as pl
import torch.nn.functional as F
import torch
from pytorch_lightning.loggers import WandbLogger
import wandb
from torch.utils.data import DataLoader,Dataset

class MyDataset(Dataset):
    def __init__(self, data, labels):
        self.data = data
        self.labels = labels

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        x = self.data[index]
        y = self.labels[index]
        return x, y

class Net(torch.nn.Module):
    def __init__(self, n_feature, n_hidden, n_output):
        super(Net, self).__init__()
        self.hidden = torch.nn.Linear(n_feature, n_hidden)
        self.predict = torch.nn.Linear(n_hidden, n_output)
    def forward(self, x):
        x= F.relu(self.hidden(x))
        x = self.predict(x)
        return x

class myLightningModule(pl.LightningModule):
    def __init__(self, config): 
        super().__init__()
        self.net = Net(n_feature=1, n_hidden=config.n_hidden, n_output=1)
        self.lr = config.lr
        self.save_hyperparameters()

    def training_step(self, batch, batch_idx):
        # training_step defines the train loop.
        x, y = batch
        y_hat = self.net(x)
        loss = F.mse_loss(y_hat, x)
        self.log('training_loss',loss,on_step=True, on_epoch=True,prog_bar=True)
        return loss

    def configure_optimizers(self):
        optimizer = torch.optim.SGD(self.parameters(), lr=self.lr)
        return optimizer
    
def train_model():
    wandb.init()
    wandb_logger = WandbLogger()
        
    lightmodule = myLightningModule(wandb.config)
    trainer = pl.Trainer(accelerator='gpu', devices=1, max_epochs=20, 
        default_root_dir="./lightning-example", logger=wandb_logger)
    wandb.require(experiment="service")
    trainer.fit(model=lightmodule, train_dataloaders=my_dataloader)        

if __name__ == '__main__':
    sweep_config = {
        'method': 'random',
        'name': 'first_sweep',
        'metric': {
            'goal': 'minimize',
            'name': 'training_loss'
        },
        'parameters': {
            'n_hidden': {'values': [2,3,5,10]},
            'lr': {'max': 1.0, 'min': 0.0001}
        }
    }

    N=10000   
    x = torch.unsqueeze(torch.linspace(-1, 1, N), dim=1)                    # x data (tensor), shape=(100, 1)
    y = x.pow(2) + 0.2*torch.rand(x.size())                                  # noisy y data (tensor), shape=(100, 1)

    my_dataset = MyDataset(x,y)
    my_dataloader = DataLoader(my_dataset, batch_size=100, shuffle=True)

    sweep_id=wandb.sweep(sweep_config, project="test_sweep")
    wandb.agent(sweep_id=sweep_id, function=train_model, count=10)


```

