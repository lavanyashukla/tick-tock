TITLE:
[CLI] Wandb patched root_dir does not sync all tensorboard logs

LABEL:
c:sync,c:tensorboard,cli,stale

STATE:
closed

BODY:
**Description**
Wandb's patched tensorboard root directory does not sync all events, such as embeddings.
Logged events such as losses are being synced (`tfevents` files). Patch command is run before wandb.init().
The tensorboard tab in the web does not appear either, even if the `.pbtxt` and `.tsv` files are copied manually.

This occurs using both TensorBoardLogger and WandbLogger within pytorch lightning.

**Wandb features**
Tensorboard integration.

**How to reproduce**
```
import os
import pytorch_lightning as pl
import torch
import torch.nn as nn
import torch.nn.functional as F
import wandb
from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger
from torch.utils.data import DataLoader
from torchvision import transforms
from torchvision.datasets import MNIST


class AutoEncoder(pl.LightningModule):

    def __init__(self):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(28 * 28, 64),
            nn.ReLU(),
            nn.Linear(64, 3)
        )
        self.decoder = nn.Sequential(
            nn.Linear(3, 64),
            nn.ReLU(),
            nn.Linear(64, 28 * 28)
        )

    def forward(self, x):
        return self.encoder(x)

    def training_step(self, batch, batch_idx):
        x, y = batch
        x = x.view(x.size(0), -1)
        z = self.encoder(x)
        x_hat = self.decoder(z)
        loss = F.mse_loss(x_hat, x)
        self.log('train_loss', loss)
        return {"loss": loss, "z": z}

    def training_epoch_end(self, outputs):
        z = torch.cat([output["z"] for output in outputs])
        tb_logger = self.trainer.logger[1]
        tb_logger.experiment.add_embedding(z, global_step=self.trainer.current_epoch)

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=1e-3)


root_logdir = "lightning_logs"
wandb.tensorboard.patch(root_logdir=root_logdir)
wandb_logger = WandbLogger(project="mwe")
tb_logger = TensorBoardLogger(save_dir=root_logdir, name="")
logger = [wandb_logger, tb_logger]

dataset = MNIST(os.getcwd(), train=False, download=True, transform=transforms.ToTensor())
loader = DataLoader(dataset, batch_size=2000)
gpus = 1 if torch.cuda.is_available() else 0
autoencoder = AutoEncoder()
trainer = pl.Trainer(max_epochs=5, logger=logger, gpus=gpus, log_every_n_steps=1, checkpoint_callback=False)
trainer.fit(autoencoder, loader)
trainer.logger[0].experiment.finish()
```

**Environment**
- OS: Linux 20.04
- Environment: wandb local
- Python Version: 3.8.5


