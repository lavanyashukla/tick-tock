TITLE:
[Feature]: Overwrite nested default config in sweep

LABEL:
feature_request,c:sweeps

STATE:
open

BODY:
### Description

I would love a better handling of nested configs during sweeping. Specifically, I would like to be able to provide a default config in `wandb.init(config=defaults)` whose parameters are only update by the sweep config.
This is an example sweep configuration (in Jupyter):
`sweep_configuration = {
    'method': 'bayes',
    'metric': {
        'goal': 'minimize', 
        'name': 'valid/best_loss'
    },
    'parameters': {
        'model': {
            'parameters': {
                'cgnn': {
                    'parameters': {
                        'train': {
                            'parameters': {
                                'batch_s': {'values': [4, 8, 16]},
                                'lr': {'max': 0.1, 'min': 0.0001}
                            }
                        },
                        'model': {
                            'parameters': {
                                'num_layers': {'values': [2, 4, 6]},
                                'head_num': {'values': [4, 8, 16]},
                                'embed_dim': {'values': [64, 128]},
                                'ff_hidden_dim': {'values': [256, 512]},
                                'softmax': {'values': [True, False]},
                                'extra_out': {'values': [True, False]},
                            }
                        }
                    }
                }
            }
        }
    }
}`
If there is a better way to define a nested sweep config let me know. A solution could massively reduce overhead when setting up a sweep config.

### Suggested Solution

When providing a nested default config in `wandb.init(config=defaults)` the config provided by the sweep should preserve the original nested structure of the default config when adding/overwriting values. 
For the above sweep configuration, this would mean the `wandb.config` would also contain additional training parameters like weight decay etc.
I think a solution should just do a recursive config merge.


### Alternatives

_No response_

### Additional Context

_No response_

