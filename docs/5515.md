TITLE:
[CLI]: Run time blows to infinity after 1 sweep--potentially callback, hanging or memory issues?

LABEL:
cli

STATE:
open

BODY:
### Describe the bug

<!--- Description of the issue below  -->
Hi there. I'm attempting to run a sweep on a fairly simple hybrid neural network. What's happened several times, is that after running training to completion for a given configuration of the model, the second configuration is failing, usually after quite a few epochs have run. I'm running locally/no cluster, and I've tried running wandb through a jupyter notebook as well as in the command line. I've also tried running wandb in offline mode thinking that maybe something was getting hung up on the server. I'm using amphetamine on mac to prevent it from sleeping while the sweep runs (overnight, ideally). I had some issues before when trying to save weights locally, and had to disable this for wandb to run, and I'm not sure if maybe there are some weird issues with M1 macs, or the versions of tensorflow/keras/wandb I'm using. 
<!--- A minimal code snippet between the quotes below  -->
```python
# compile model model
auroc = tf.keras.metrics.AUC(curve='ROC', name='auroc')
aupr = tf.keras.metrics.AUC(curve='PR', name='aupr')


# early stopping callback
es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_auroc', 
                                            patience=10, 
                                            verbose=1, 
                                            mode='max', 
                                            restore_best_weights=True)
# reduce learning rate callback
reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_auroc', 
                                                factor=0.2,
                                                patience=4, 
                                                min_lr=1e-7,
                                                mode='max',
                                                verbose=1) 

def train():
    config_defaults={
        "dense_number":16,
        "num_filters":50,
        "filter_width":5,
        "batch_size":100,
        "epochs":50,
        "regularization":1e-3,
        "keylen":12,
        "num_heads":8,
        "learning_rate":0.001,
    }
    pool_size="no"
    # tf.keras.backend.clear_session()
    # Initialize a new wandb run
    wandb.init(config=config_defaults)
    
    # Config is a variable that holds and saves hyperparameters and inputs
    config = wandb.config
    model=get_model_attention_pk_pool_masked_globrelpos_enet(input_length=bigtrain_x.shape[1],
                             dense_number=config.dense_number,
                             num_filters=config.num_filters,
                             filter_width=config.filter_width,
                             pool_size=pool_size,
                             key_len=config.keylen,
                             num_heads=config.num_heads,
                             l2_regularization=config.regularization,
                             learning_rate=config.learning_rate)
    model_name="wandb_example"
    model_name+=str(config.dense_number)+"_"+str(config.num_filters)+"_"
    model_name+=str(config.filter_width)+"_"+str(config.batch_size)
    model_name+="_"+str(config.epochs)
    model_name+="_"+str(config.regularization) + "pool_" + str(pool_size)
    model_name+="_" + str(config.keylen) + "_es_" + str(10) 
    model_name+= "numheads"+str(config.num_heads)
    model_name+="_lr_"+str(config.learning_rate)
    results_path="/Users/mthompson/nnk_amyloid/saved_models/"
    print(model_name)
    model_dir = os.path.join(results_path, model_name+'_weights.h5')
    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=model_dir,
    save_weights_only=True,
    monitor='val_auroc',
    mode='max',
    save_best_only=True)
    # early stopping callback
    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_auroc', 
                                            patience=10, 
                                            verbose=1, 
                                            mode='max', 
                                            restore_best_weights=True)
    # reduce learning rate callback
    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_auroc', 
                                                factor=0.2,
                                                patience=4, 
                                                min_lr=1e-7,
                                                mode='max',
                                                verbose=1) 
    # wandb_callbacks=[]
    history=model.fit(x=bigtrain_x,y=bigtrain_y,
                  batch_size=config.batch_size,epochs=config.epochs,verbose=1,
        validation_split=0.10,
                            callbacks=[es_callback, reduce_lr,
                                      wandb.keras.WandbMetricsLogger(),
                                      wandb.keras.WandbCallback(save_model=(False)),
                                      model_checkpoint_callback]
    )
    # model.save_weights(model_dir)



import wandb
wandb.login()
# 2: Define the search space
sweep_config = {
    'method': 'random', #grid, random
    'metric': {
      'name': 'val_loss',
      'goal': 'minimize'   
    },
    'parameters': {
        'epochs': {
            'values': [30, 50, 100]
        },
        'dense_number': {
            'values': [12, 16, 20, 24,28,32,36]
        },
        'regularization': {
            'values': [0.00001,0.0001, 0.001, 0.01]
        },
        'num_filters': {
            'values': [20, 25, 30,35,40,45,50]
        },
        'learning_rate': {
            'values': [0.001, 0.005,0.01]
        },
        'keylen': {
            'values': [8, 12, 16,24]
        },
        'num_heads': {
            'values': [2,4,8,12]
        },
        'filter_width': {
            'values': [5,7,9]
        }
    }
}
sweep_id = wandb.sweep(sweep_config, entity="myuserid", project="example-init")
wandb.agent(sweep_id, train)
```




### Additional Files

[debug_hung.log](https://github.com/wandb/wandb/files/11429965/debug_hung.log)
[debug-internal_hung.log](https://github.com/wandb/wandb/files/11429966/debug-internal_hung.log)
[debug_successful.log](https://github.com/wandb/wandb/files/11429968/debug_successful.log)
[debug-internal_successful.log](https://github.com/wandb/wandb/files/11429969/debug-internal_successful.log)
<img width="1299" alt="Screenshot 2023-05-09 at 11 55 34" src="https://github.com/wandb/wandb/assets/19845779/87016948-daab-4f1a-a98a-f588d5d79cdc">
<img width="1678" alt="Screenshot 2023-05-09 at 11 55 23" src="https://github.com/wandb/wandb/assets/19845779/1ad2956e-027f-4711-81ea-c16461266493">



### Environment

WandB version: 0.15.2

OS: Mac Monterrey 12.6

Python version:
Python 3.10.6
Versions of relevant libraries:
- wandb==0.15.2
- tensorflow-deps=2.10.0=0
- tensorboard==2.10.0
- tensorboard-data-server==0.6.1
- tensorboard-plugin-wit==1.8.1
- tensorflow-addons==0.20.0
- tensorflow-datasets==4.9.2
- tensorflow-estimator==2.10.0
- tensorflow-hub==0.13.0
- tensorflow-macos==2.10.0
- tensorflow-metadata==1.
- tensorflow-metal==0.6.0
- wheel=0.37.1=pyhd8ed1ab_0
- keras==2.10.0
- keras-nlp==0.4.
- keras-preprocessing==1.1.2

### Additional Context

I'm not sure if there's maybe an issue with the callbacks since this was causing issues in the past. Can I use the keras early-stopping callback, or do I need to switch to that of wandb? 

