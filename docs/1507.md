TITLE:
WandB-Logger drops all the logged values in training step for PyTorch Lightning

LABEL:
c:misc

STATE:
closed

BODY:
train-step:
```
def training_step(self, batch, batch_idx):
            x, y = batch
            logits, masks = self(x)
            loss = self.loss_func(logits, y)
            self.log(f"train-loss", loss, prog_bar=True)
            return {
                "loss": loss,
                "pred": torch.argmax(logits, -1),
                "labels": y,
            }
```
and on epoch end it logs the Precision and recall. 
after the validation step. I get a ` wandb: WARNING Step must only increase in log calls.  Step 379 < 380; dropping {train-loss: 0.494, 'train-precision': 0.9, 'train-recall': 1, 'epoch': 1}`

WandB version: 0.10.10
PyTorch-Lightning: 1.0.6

