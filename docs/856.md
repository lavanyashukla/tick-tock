TITLE:
Duplicate layer logging with torch.nn.DataParallel(model)

LABEL:
c:framework,stale

STATE:
closed

BODY:
* Weights and Biases version: 0.8.26
* Python version: 3.7.6
* Operating System: Linux

### Description

In the model tab, the table listing Name/Type/# Parameters/Output Shape shows multiple entries for each submodule (one for each GPU). So if I train on 8 GPUs, I will see 8 copies of each submodule, which makes it quite confusing and cluttered to read. This even happens when I call watch on the underlying single-GPU module via `wandb.watch(model.module)`. I believe the correct behavior would be to only display a single copy of the model submodules, even if there are multiple copies of that model across the GPUs.

### What I Did

```
wandb.watch(model)
model = torch.nn.DataParallel(model)
...
```


