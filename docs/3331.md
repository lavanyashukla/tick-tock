TITLE:
[App]: 

LABEL:
app

STATE:
closed

BODY:
### Current Behavior

Not all histograms are logged correctly.
I'm logging multiple histograms in my client code with the same calls. Some histograms appear correctly, some don't.

![Correct histogram](https://i.imgur.com/JHdUBxX.png)

![Incorrect histogram](https://i.imgur.com/67wEYqe.png)




### Expected Behavior

All histograms should be logged correctly:

### Steps To Reproduce

```julia
using PyCall
using Wandb

# Set up logging and pycall
wb_logger = WandbLogger(project="ecei_generative", entity="rkube")
np = pyimport("numpy")


D = get_cat_discriminator_3d(args) |> gpu;
G = get_generator_3d(args) |> gpu;

opt_D = ADAM(args["lr_D"]);
opt_G = ADAM(args["lr_G"]);

ps_D = Flux.params(D);
ps_G = Flux.params(G);

epoch_size = length(loader_train);

total_batch = 1 
for epoch ∈ 1:args["num_epochs"]
    num_batch = 1;
    @show epoch
    for (x, y) ∈ loader_train
        this_batch = size(x)[end]
        # Train the discriminator
        trainmode!(G);
        testmode!(G);
        loss_D, back_D = Zygote.pullback(ps_D) do
            # Sample noise and generate a batch of fake data
            y_real = D(x)
            z = randn(Float32, args["latent_dim"], this_batch) |> gpu;
            y_fake = D(G(z))
            loss_D = -H_of_p(y_real) + E_of_H_of_p(y_real) - E_of_H_of_p(y_fake) + args["lambda"] * Flux.Losses.binarycrossentropy(y_real, y)
        end


        grads_D = back_D(one(loss_D));
        Flux.update!(opt_D, ps_D, grads_D)

        # Train the generator
        testmode!(D);
        trainmode!(G);
        loss_G, back_G = Zygote.pullback(ps_G) do
            z = randn(Float32, args["latent_dim"], this_batch) |> gpu;
            y_fake = D(G(z));
            loss_G = -H_of_p(y_fake) + E_of_H_of_p(y_fake)
        end
        grads_G = back_G(one(loss_G));
        Flux.update!(opt_G, ps_G, grads_G)

        if num_batch % 10 == 0
            (x_test, y_test) = first(loader_test)
            testmode!(G);
            testmode!(D);
            y_real = D(x_test);
            z = randn(Float32, args["latent_dim"], this_batch) |> gpu;
            y_fake = D(G(z));

            xentropy = args["lambda"] * Flux.Losses.binarycrossentropy(y_real, y_test)

            y_real = y_real |> cpu;
            y_fake = y_fake |> cpu;
            grads_D1 = grads_D[ps_D[1]][:] |> cpu;
            grads_D4 = grads_D[ps_D[4]][:] |> cpu;
            grads_G1 = grads_G[ps_G[1]][:] |> cpu;
            grads_G4 = grads_G[ps_G[4]][:] |> cpu;

            # Use Numpy histograms for wandb
            # Fetch gradients for top and bottom layer of the generator and discriminator

            img = fake_image_3d(G, args, 16);
            img = convert(Array{Float32}, img);

            log(wb_logger, Dict("batch" => total_batch, 
                                "crossentropy" => xentropy,
                                "hist_gradD_1" => Wandb.Histogram(grads_D1),
                                "hist_gradD_4" => Wandb.Histogram(grads_D4),
                                "hist_gradG_1" => Wandb.Histogram(grads_G1),
                                "hist_gradG_4" => Wandb.Histogram(grads_D4),
                                "hist y_real" => Wandb.Histogram(y_real),
                                "hist y_fake" => Wandb.Histogram(y_fake),
                                "H_real" => -H_of_p(y_real),
                                "E_real" => E_of_H_of_p(y_real),
                                "E_fake" => E_of_H_of_p(y_fake),
                                "Generator" => Wandb.Image(img)))
        end
        num_batch += 1;
        global total_batch += 1;
    end
end```

### Screenshots

_No response_

### Environment

OS: Rockylinux 8

Browsers: Chrome

Version:


### Additional Context

_No response_

