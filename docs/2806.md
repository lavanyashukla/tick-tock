TITLE:
[CLI] max_epochs in Lightning no longer works during sweep? Model keeps training indefinitely.

LABEL:
cli

STATE:
closed

BODY:
**Description**
When I train a single model (without hyperparameter sweep), my model trains normally and stops at `max_epochs`. However, when I try to do a sweep, the model trains indefinitely. I thought maybe I wasn't passing the hyperparameters in properly, but I put a few print statements in my model and it seems that the `max_epochs` I defined in the config is loaded (the model doesn't use the defaults in the init, which has a longer `max_epochs`).

I would normally expect that this is a PyTorch Lightning issue, but I'm not sure since it works fine when I don't use the Sweeps.

**Code for the Sweeps**

```
sweep_id = wandb.sweep(sweep_config, project=project, entity=entity)

def sweep_iteration(model_scores=model_scores, model_paths=model_paths):

    with wandb.init(config=sweep_config) as run:

        # set up W&B logger
        wandb_logger = WandbLogger()

        print(wandb.config.get('max_epochs', 1000))

        # instantiate model - note how we refer to sweep parameters with wandb.config
        flood_model = FloodModel(hparams=wandb.config)

        # setup Trainer
        trainer = pl.Trainer(
            logger=wandb_logger,
            gpus=-1
        )

        # train
        trainer.fit(flood_model)

        wandb.watch(flood_model)

        print(wandb.config)

        model_scores.append(flood_model.trainer_params["callbacks"][0].best_model_score)
        model_paths.append(flood_model.trainer_params["callbacks"][0].best_model_path)

wandb.agent(sweep_id, function=sweep_iteration)
```

**How to reproduce**

[Link to the Colab Notebook](https://colab.research.google.com/drive/17K-gXacwxdgW6PAt5cpCpGi-dplg2IrH?usp=sharing)

You can simply run the notebook up until the Model Inference section (the Hyperparameter Sweep is just before that).

**Environment**
- Environment: Google Colab


