TITLE:
Wandb won't work in Multi-GPU training with PyTorch-Lightning

LABEL:
cli

STATE:
closed

BODY:
### Describe the bug

<!--- Description of the issue below  -->

I'm trying to use Kaggle's 2 GPU environment. Training can't start because impatiently wandb throws errors which I can't fix in any way. 

For CPU and single GPU wandb works perfectly, but for many GPUs not. I use the `ddp_notebook` strategy.
Now I use `self.logger.experiment.log` but I tried with `self.log` and it still didn't work.

No matter what I do I can't get it to work.

Here's the code:

<!--- A minimal code snippet between the quotes below  -->
```python

class WebpageClassifierV2(pl.LightningModule):
    def __init__(... ):
        super().__init__()
        self.lr = lr
        self.save_hyperparameters()
        
        self.model = nn.Sequential(... )
            
    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)
        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": torch.optim.lr_scheduler.ReduceLROnPlateau(
                    optimizer, patience=self.hparams.lr_scheduler_patience
                ),
                "monitor": "val_loss",
            },
        }

    def forward(self, x):
        return self.model(x)

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self.loss_function(y_hat, y)

        self.logger.experiment.log("train_loss", loss, sync_dist=True)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self.loss_function(y_hat, y)

        self.logger.experiment.log("val_loss", loss, sync_dist=True)

    def test_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)        
        loss = self.loss_function(y_hat, y)

        self.log("test_loss", loss, sync_dist=True)

wandb_logger = WandbLogger(project="...", group="...", name="...")

trainer = pl.Trainer(
    accelerator='gpu',
    devices=-1,
    strategy="ddp_notebook",
    precision=16,
    max_epochs=100,
    callbacks=[...],
    logger=wandb_logger,
)
trainer.fit(model, train_dataloader, val_dataloader)
```

<!--- A full traceback of the exception in the quotes below -->
```shell

Thread WriterThread:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/internal_util.py", line 49, in run
    self._run()
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/internal_util.py", line 89, in _run
    self._setup()
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/internal.py", line 377, in _setup
    context_keeper=self._context_keeper,
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/internal/writer.py", line 66, in __init__
    self._settings._flow_control_disabled or self._settings._offline
AttributeError: 'SettingsStatic' object has no attribute '_flow_control_disabled'
wandb: ERROR Internal wandb error: file data was not synced
Problem at: /opt/conda/lib/python3.7/site-packages/pytorch_lightning/loggers/wandb.py 406 experiment
Traceback (most recent call last):
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_init.py", line 1043, in init
    online.
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_init.py", line 689, in init
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/backend/backend.py", line 246, in cleanup
    """Connect to server."""
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/interface/interface_shared.py", line 475, in join
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/interface/interface.py", line 666, in join
    self._publish_pause(pause)
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/interface/interface_shared.py", line 472, in _communicate_shutdown
    return None
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/interface/interface_shared.py", line 226, in _communicate
    metric: Optional[pb.MetricRecord] = None,
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/interface/interface_shared.py", line 231, in _communicate_async
    preempting: Optional[pb.RunPreemptingRecord] = None,
Exception: The wandb backend process has shutdown
wandb: ERROR Abnormal program exit
---------------------------------------------------------------------------
ProcessRaisedException                    Traceback (most recent call last)
/tmp/ipykernel_23/3639995642.py in <module>
----> 1 trainer.fit(model, train_dataloader, val_dataloader)

/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)
    602         self.strategy._lightning_module = model
    603         call._call_and_handle_interrupt(
--> 604             self, self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path
    605         )
    606 

/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/call.py in _call_and_handle_interrupt(trainer, trainer_fn, *args, **kwargs)
     34     try:
     35         if trainer.strategy.launcher is not None:
---> 36             return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
     37         else:
     38             return trainer_fn(*args, **kwargs)

/opt/conda/lib/python3.7/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py in launch(self, function, trainer, *args, **kwargs)
    115             args=process_args,
    116             nprocs=self._strategy.num_processes,
--> 117             start_method=self._start_method,
    118         )
    119         worker_output = return_queue.get()

/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py in start_processes(fn, args, nprocs, join, daemon, start_method)
    196 
    197     # Loop on join until it returns True or raises an exception.
--> 198     while not context.join():
    199         pass
    200 

/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py in join(self, timeout)
    158         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
    159         msg += original_trace
--> 160         raise ProcessRaisedException(msg, error_index, failed_process.pid)
    161 
    162 

ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_init.py", line 1043, in init
    online.
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_init.py", line 689, in init
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/backend/backend.py", line 246, in cleanup
    """Connect to server."""
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/interface/interface_shared.py", line 475, in join
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/interface/interface.py", line 666, in join
    self._publish_pause(pause)
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/interface/interface_shared.py", line 472, in _communicate_shutdown
    return None
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/interface/interface_shared.py", line 226, in _communicate
    metric: Optional[pb.MetricRecord] = None,
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/interface/interface_shared.py", line 231, in _communicate_async
    preempting: Optional[pb.RunPreemptingRecord] = None,
Exception: The wandb backend process has shutdown

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py", line 139, in _wrapping_function
    results = function(*args, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 645, in _fit_impl
    self._run(model, ckpt_path=self.ckpt_path)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1086, in _run
    self._log_hyperparams()
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py", line 1154, in _log_hyperparams
    logger.log_hyperparams(hparams_initial)
  File "/opt/conda/lib/python3.7/site-packages/lightning_utilities/core/rank_zero.py", line 24, in wrapped_fn
    return fn(*args, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loggers/wandb.py", line 426, in log_hyperparams
    self.experiment.config.update(params, allow_val_change=True)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loggers/logger.py", line 53, in experiment
    return get_experiment() or DummyExperiment()
  File "/opt/conda/lib/python3.7/site-packages/lightning_utilities/core/rank_zero.py", line 24, in wrapped_fn
    return fn(*args, **kwargs)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loggers/logger.py", line 51, in get_experiment
    return fn(self)
  File "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loggers/wandb.py", line 406, in experiment
    self._experiment = wandb.init(**self._wandb_init)
  File "/opt/conda/lib/python3.7/site-packages/wandb/sdk/wandb_init.py", line 1081, in init
    Pass a dictionary-style object as the `config` keyword argument to add
Exception: problem

```


### Additional Files

_No response_

### Environment

WandB version: 0.13.9

OS: Linux-5.15.65+-x86_64-with-debian-bullseye-sid

Python version: 3.7.12

Versions of relevant libraries:

Pytorch-Lightning: 1.8.6

### Additional Context

_No response_

