TITLE:
Hyperband stopping criterion too permissive with noisy metrics

LABEL:
feature_request,c:sweeps

STATE:
open

BODY:
`wandb --version && python --version && uname`

* Weights and Biases version: 0.9.7
* Python version: 3.7.9
* Operating System: Ubuntu 18.04LTS

### Description

Running a sweep using Hyperband (HB) early stopping criterion.  The metric I use to evaluate model quality is somewhat noisy.  The current hyperband stopping criterion considers the run's minimum (best) [(code)](https://github.com/wandb/client/blob/63f3d7806669d2dd8d459134c2f88d6e341a797e/wandb/sweeps/hyperband_stopping.py#L135) metric value vs a single-sample snapshot of the other runs in that band. [(code)](https://github.com/wandb/client/blob/63f3d7806669d2dd8d459134c2f88d6e341a797e/wandb/sweeps/hyperband_stopping.py#L102).  For noisy metrics, the result is that runs are judged very favorably, and runs that a human would obviously judge as being worse than average are allowed to continue.  This problem becomes especially pronounced in later bands where the `min()` is taken over a larger number of samples.

### What I Did

Consider these charts of the objective metric vs time for a HB run.   

![smoothed objective metrics](https://user-images.githubusercontent.com/193183/92950266-3d66f380-f411-11ea-96cf-9105d884822b.png)

The run I'm concerned about is in hot pink, with the badly drawn label "This V".  It passed a band marker at about 3.7 hours where the two brownish runs at the bottom finished.  This smoothed plot makes it clear that the hot pink run is not in the better half of all runs to reach that point, and thus should be stopped.  But because it got one lucky sample below the median, it is allowed to continue, as shown in this unsmoothed plot:

![unsmoothed objective metrics](https://user-images.githubusercontent.com/193183/92949691-5ae78d80-f410-11ea-8c82-dce216f4191c.png)

### Discussion

It's not totally clear what to do about this.  But the current criterion is clearly biased in a way that leads to wasted compute because it stops a lot fewer runs than it should when the metrics are noisy.  Using single sample snapshots for both the current run and the thresholds I think would yield an unbiased criterion with high variance.  Alternately using the same aggregation for both could also eliminate the bias.  I'm not sure which would do better in general.


