TITLE:
[Q] GPU Metric duplication in distributed runs

LABEL:
c:misc

STATE:
closed

BODY:
Hi â€“ I'm running distributed training with https://github.com/mosaicml/composer, which starts a process-per-gpu, resulting in a wandb log/metric stream per-gpu.

This mostly works alright due to grouping, but because each process logs full system metrics, the grouped metric log view has duplicate logs a multiple of the gpus per worker.

Currently it seems to me the most sensible thing to do is maybe to disable system logs for non rank zero workers with `wandb.init(settings=wandb.Settings(_disable_stats=True))`, but I thought I'd see if there's a better way. 

![Screenshot_2023-03-03_08:43:31](https://user-images.githubusercontent.com/8343799/222781308-19b2ca03-8388-40e9-859a-38fd81d04ff4.png)


