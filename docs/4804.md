TITLE:
[Feature]: Give the possibility to add a seed to reproduce random and bayesian searches with sweeps

LABEL:
feature_request

STATE:
closed

BODY:
### Description

Hello everyone,
I am working on a big research project with my University and we need that everything is reproducible from the beginning to the end. We decided to rely on Weights and Biases for the hyper-parameter tuning part of the project since we love it.

However, the only hyper-parameter search that I am able to reproduce in wandb is the grid search since it simply iterates over the hyper-parameter values in the sweep config. So, by just putting a classic `set_seed()` function before the call to `wandb.agent()`, I am able to reproduce exactly the entire search. By exactly, I mean that the entire training for every single "sampled" configuration is reproducible.

Now, we have a much more complex model that we need to tune. For this model, it is unthinkable to use a grid search for scalability issues. For this reason, I tried to work with random and bayesian searches. However, even if I set a seed before the call to `wandb.agent()`, I am not able to reproduce the sampling of the hyper-parameters from the sweep config. Random search simply randomly samples a configuration for each run, instead, bayesian search samples only the first configuration and then tries to change it in order to maximize performance. 


### Suggested Solution

So, after this big preamble, my request is to simply add a `seed` or `random_state` parameter to the `wandb.agent()`, like `wandb.agent(sweep_id=..., function=..., count=..., random_state=...)`.

I tried to go through your code, and it seems you work with some `Queue` objects. Maybe you just need to add a seed on that part of the code. I am not able to find the logic of random and bayesian searches in the library.

Thank you in advance.

### Alternatives

_No response_

### Additional Context

_No response_

