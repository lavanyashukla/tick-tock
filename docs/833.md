TITLE:
Being not able to deepcopy wandb.config causes issue with Lambda layer in Keras when model saving

LABEL:
c:doc

STATE:
closed

BODY:
* Weights and Biases version: 0.8.24
* Python version: 3.6.8
* Operating System: Linux Ubuntu 18.04.3 LTS
* Keras version: 2.2.5
* Tensorflow version: 1.14.0

### Description

I know the version of Keras/Tensorflow is kinda out of date, but it is necessary for the project I have been working on.
I believe the problem is that putting wandb.config inside a Lambda layer will cause a wandb.config `copy.deepcopy` when `WandbCallback` tries to save the model.
However, wandb.config is not `copy.deepcopy`able thus leads to trouble.

Here's the code to reproduce the problem.
```python3
import numpy as np
import wandb
from wandb.keras import WandbCallback
from keras.models import Sequential
from keras.layers import Dense, Activation, Lambda

wandb.init()
wandb.config.meaning_of_life = 42

def def_model(config):
    model = Sequential()
    model.add(Dense(100, input_dim=784))
    model.add(Lambda(lambda x: x[:, :config.meaning_of_life]))
    model.add(Activation('relu'))
    return model

model = def_model(wandb.config)
model.compile(optimizer='adam', loss='mse')

X = np.random.random((64, 784))
y = np.random.random((64, 42))

model.fit(X, y, validation_data=(X, y), epochs=5, batch_size=64, callbacks=[WandbCallback()])
```

### What I Did

```
$ WANDB_MODE=dryrun python3 test.py
...(Tensorflow stuff)...
2020-02-05 01:02:07.214738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10311 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci
bus id: 0000:17:00.0, compute capability: 7.5)
2020-02-05 01:02:07.218142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10479 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci
bus id: 0000:65:00.0, compute capability: 6.1)
2020-02-05 01:02:08.266449: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
64/64 [==============================] - 2s 30ms/step - loss: 0.2863 - val_loss: 0.2392
Traceback (most recent call last):
  File "test.py", line 23, in <module>
    model.fit(X, y, validation_data=(X, y), epochs=5, batch_size=64, callbacks=[WandbCallback()])
  File "/home/toosyou/.local/lib/python3.6/site-packages/keras/engine/training.py", line 1178, in fit
    validation_freq=validation_freq)
  File "/home/toosyou/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py", line 224, in fit_loop
    callbacks.on_epoch_end(epoch, epoch_logs)
  File "/home/toosyou/.local/lib/python3.6/site-packages/keras/callbacks.py", line 152, in on_epoch_end
    callback.on_epoch_end(epoch, logs)
  File "/home/toosyou/.local/lib/python3.6/site-packages/wandb/keras/__init__.py", line 300, in on_epoch_end
    self._save_model(epoch)
  File "/home/toosyou/.local/lib/python3.6/site-packages/wandb/keras/__init__.py", line 576, in _save_model
    self.model.save(self.filepath, overwrite=True)
  File "/home/toosyou/.local/lib/python3.6/site-packages/keras/engine/network.py", line 1139, in save
    save_model(self, filepath, overwrite, include_optimizer)
  File "/home/toosyou/.local/lib/python3.6/site-packages/keras/engine/saving.py", line 415, in save_wrapper
    save_function(obj, filepath, overwrite, *args, **kwargs)
  File "/home/toosyou/.local/lib/python3.6/site-packages/keras/engine/saving.py", line 507, in save_model
    _serialize_model(model, h5dict, include_optimizer)
  File "/home/toosyou/.local/lib/python3.6/site-packages/keras/engine/saving.py", line 100, in _serialize_model
    model_config['config'] = model.get_config()
  File "/home/toosyou/.local/lib/python3.6/site-packages/keras/engine/sequential.py", line 283, in get_config
    'layers': copy.deepcopy(layer_configs)
  File "/usr/lib/python3.6/copy.py", line 150, in deepcopy
wandb: Waiting for W&B process to finish, PID 15904
    y = copier(x, memo)
  File "/usr/lib/python3.6/copy.py", line 215, in _deepcopy_list
    append(deepcopy(a, memo))
  File "/usr/lib/python3.6/copy.py", line 150, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.6/copy.py", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.6/copy.py", line 150, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.6/copy.py", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.6/copy.py", line 150, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.6/copy.py", line 220, in _deepcopy_tuple
    y = [deepcopy(a, memo) for a in x]
  File "/usr/lib/python3.6/copy.py", line 220, in <listcomp>
    y = [deepcopy(a, memo) for a in x]
  File "/usr/lib/python3.6/copy.py", line 150, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.6/copy.py", line 220, in _deepcopy_tuple
    y = [deepcopy(a, memo) for a in x]
  File "/usr/lib/python3.6/copy.py", line 220, in <listcomp>
    y = [deepcopy(a, memo) for a in x]
  File "/usr/lib/python3.6/copy.py", line 159, in deepcopy
    copier = getattr(x, "__deepcopy__", None)
  File "/home/toosyou/.local/lib/python3.6/site-packages/wandb/wandb_config.py", line 213, in __getattr__
    return self.__getitem__(key)
  File "/home/toosyou/.local/lib/python3.6/site-packages/wandb/wandb_config.py", line 203, in __getitem__
    return self._items[key]
KeyError: '__deepcopy__'
wandb: Program failed with code 1. Press ctrl-c to abort syncing.
wandb: You can sync this run to the cloud by running:
wandb: wandb sync wandb/dryrun-20200204_170205-1spsrry7
```


