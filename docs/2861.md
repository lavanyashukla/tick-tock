TITLE:
[CLI] Process hangs when trying to start a lightning DDP experiment with checkpointing

LABEL:
cli,stale

STATE:
closed

BODY:
I am trying to save checkpoints of my pytorch-lightning runs to wandb while running a DDP experiment. However, the script hangs when starting the run, and can't even be killed with ctrl-c. It's strange - the hang/deadlock only happens when I reference `wandb.run.dir` in the lightning ModelCheckpoint constructor. If I let lightning save checkpoints to the default dir, everything works fine (except the checkpoints don't get uploaded, as they're not placed in the wandb dir). And the script works fine when not enabling DDP.

I've attached a reproducible code sample below. 

My environment is:
- wandb 0.12.5 or 0.12.6 with [service] (tried both)
- lightning from pip install git+https://github.com/wandb/pytorch-lightning.git@wandb-service-attach from a few days ago
- pytorch 1.10.0
- python 3.9.2

Here's the script:
```
import os

import pytorch_lightning as pl
import wandb
from pl_examples.basic_examples.mnist_datamodule import MNISTDataModule
from pl_examples.basic_examples.simple_image_classifier import LitClassifier
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning.loggers import WandbLogger

if __name__ == "__main__":
    pl.seed_everything(1)
    dm = MNISTDataModule(batch_size=32, num_workers=4)
    model = LitClassifier()

    pl_logger = WandbLogger(
        project="zack_simone",
        name="test",
        log_model=True,  # upload checkpoints to wandb
    )
    pl_logger.watch(model, log="all", log_freq=100)

    callbacks = [
        ModelCheckpoint(
            dirpath=os.path.join(wandb.run.dir, "checkpoints"),
            every_n_train_steps=100,
        ),
    ]

    trainer = pl.Trainer(max_epochs=2, gpus=2, logger=pl_logger, strategy="ddp", callbacks=callbacks)
    wandb.require(experiment="service")
    trainer.fit(model, dm)
```

and the output
```
python -m quarantine.zack.simone.train2
Global seed set to 1
/home/user/.conda/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:178: LightningDeprecationWarning: DataModule property `dims` was deprecated in v1.5 and will be removed in v1.7.
  rank_zero_deprecation("DataModule property `dims` was deprecated in v1.5 and will be removed in v1.7.")
/home/user/.conda/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:162: LightningDeprecationWarning: DataModule property `test_transforms` was deprecated in v1.5 and will be removed in v1.7.
  rank_zero_deprecation(
wandb: Currently logged in as: sourceress (use `wandb login --relogin` to force relogin)
wandb: Tracking run with wandb version 0.12.6
wandb: Syncing run test
wandb:  View project at https://wandb.ai/sourceress/zack_simone
wandb:  View run at https://wandb.ai/sourceress/zack_simone/runs/2hollxc1
wandb: Run data is saved locally in /opt/projects/generally_intelligent/standalone/ballworld/wandb/run-20211101_180117-2hollxc1
wandb: Run `wandb offline` to turn off syncing.

wandb: logging graph, to disable use `wandb.watch(log_graph=False)`
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
Global seed set to 1
```
(hangs after that)



