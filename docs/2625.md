TITLE:
[Q]Wandb seems not logging the gradient for parameters that updated in optimizer during training

LABEL:
c:misc

STATE:
closed

BODY:
Hi, I recently work on transfer learning so I dynamically update the optimizer to unfreeze the model parameters.
What I found was that the parameters that I unfreeze compute gradients and were updated but it seems that wandb does not log those parameters' gradients. Is it a bug or can it be fixed by calling ` wandb.watch(model)` later during the training ?

I have some doubts about this situation so I remain this issue for Question.

Thanks

